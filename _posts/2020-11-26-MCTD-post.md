---
layout: post
title: Monte-Carlo & Temporal Difference
author: Jaeheon Kwon
categories: Ai
tags: [Reinforcement Learning]
---



### Monte-Carlo

 러프하게 말하면 실제 값을 통해 추정하는 방법이다.

$$E[x] = \int xp(x)dx \approx \frac1N\sum\limits_{i=1}^Nx_i$$

- MC는 Model-Free 방법론이다. MDP(transition, reward)에 대한 정보가 없음
- 에피소드를 끝까지 해봐야함.(no bootstrapping)
- 에피소드가 '끝'이 있어야 적용할 수 있음.



위 수식을 Q-function에 적용해보자.

$$Q(s,a) = \int G_tp(s_{t+1}:a_T\vert s,a)ds_{t+1}:a_T \approx \frac1N \sum\limits_{i=1}^NG_t^{(i)}$$

즉 샘플 $G_t$를 계속해서 뽑다보면 큰수의 법칙에 의해 Q값에 수렴한다는 것이다.

위에서 적은 것 처럼 $G_t$가 샘플 하나이기 때문에 에피소드가 끝이나야 업데이트가 가능하며, unbiased 이지만 variance가 크다는 문제점을 가지고 있다.



### First-Visit Monte-Carlo

- 한 에피소드에서 처음으로 방문한 상태에만 카운트 $N(s)\leftarrow N(s)+1$

- 리턴을 더해주고 $S(s)\leftarrow S(s)+G_t$

- 평균냄 $V(s) = S(s)/N(s)$

    

### Every-Visit Monte-Carlo

- 한 에피소드에서 방문한 모든 상태를 카운트 $N(s)\leftarrow N(s)+1$
- 에피소드가 끝나면 리턴을 더해주고 $S(s)\leftarrow S(s)+G_t$
- 평균냄 $V(s) = S(s)/N(s)$



딱히 뭘 쓰든 상관 없다고 강의에서 언급함. 어쨌든 $V(s)$에 수렴.

단, 모든 상태에 방문한다는 전제 조건이 필요함.



### Incremental Monte-Carlo Updates

$$V(s_t) \leftarrow V(s_t)+\alpha(G_t-V(s_t))$$

- $G_t-V(s_t)$는 에러텀
- $\alpha$를 fix하면 non-station problem(MDP가 계속 바뀜)에서는 유용함(과거는 잊자.)
- 매번 $G_t$를 향해 조금씩 움직인다고 생각해도 좋음





### Temporal-Difference

- MC와 동일하게 경험으로 부터 학습함.
- Model-Free
- Learn from **Incomplete** episodes, by bootstrapping (끝까지 안가봐도된다.)
- TD updates a guess towards a guess