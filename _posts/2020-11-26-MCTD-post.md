---
layout: post
title: Monte-Carlo & Temporal Difference
author: Jaeheon Kwon
categories: Ai
tags: [Reinforcement Learning]
---



### Summary

- MC has high variance, zero bias
    - Good convergence properties
    - Even with function approximation (대충 딥러닝 써도 수렴 잘된다는 뜻)
    - Not very sensitive to initial value
    - Very simple to understand and use



- TD has low variance, some bias

    - Usually more efficient than MC

    - TD(0) convergence to $v_\pi(s)$

    - But not always with function approximation
    - More sensitive to initial value (초기값을 통해 $v(s)$를 업데이트함)



> bias가 있어도 과연 잘 동작한다고 할 수 있느냐?
>
> 왜냐면 bias로 인해 잘못된 값으로 수렴할 수도 있으니까
>
> 다행히 잘 동작한다고 말할 수 있다고 함. 실버 교수님강의에서 증명하진 않음

### Monte-Carlo

 러프하게 말하면 실제 값을 통해 추정하는 방법이다.

$$E[x] = \int xp(x)dx \approx \frac1N\sum\limits_{i=1}^Nx_i$$

- MC는 Model-Free 방법론이다. MDP(transition, reward)에 대한 정보가 없음
- 에피소드를 끝까지 해봐야함.(no bootstrapping)
- 에피소드가 '끝'이 있어야 적용할 수 있음.



위 수식을 Q-function에 적용해보자.

$$Q(s,a) = \int G_tp(s_{t+1}:a_T\vert s,a)ds_{t+1}:a_T \approx \frac1N \sum\limits_{i=1}^NG_t^{(i)}$$

즉 샘플 $G_t$를 계속해서 뽑다보면 큰수의 법칙에 의해 Q값에 수렴한다는 것이다.

위에서 적은 것 처럼 $G_t$가 샘플 하나이기 때문에 에피소드가 끝이나야 업데이트가 가능하며, unbiased 이지만 variance가 크다는 문제점을 가지고 있다.



### First-Visit Monte-Carlo

- 한 에피소드에서 처음으로 방문한 상태에만 카운트 $N(s)\leftarrow N(s)+1$

- 리턴을 더해주고 $S(s)\leftarrow S(s)+G_t$

- 평균냄 $V(s) = S(s)/N(s)$

    

### Every-Visit Monte-Carlo

- 한 에피소드에서 방문한 모든 상태를 카운트 $N(s)\leftarrow N(s)+1$
- 에피소드가 끝나면 리턴을 더해주고 $S(s)\leftarrow S(s)+G_t$
- 평균냄 $V(s) = S(s)/N(s)$



딱히 뭘 쓰든 상관 없다고 강의에서 언급함. 어쨌든 $V(s)$에 수렴.

단, 모든 상태에 방문한다는 전제 조건이 필요함.



### Incremental Monte-Carlo Updates

$$V(s_t) \leftarrow V(s_t)+\alpha(G_t-V(s_t))$$

- $G_t-V(s_t)$는 에러텀
- $\alpha$를 fix하면 non-station problem(MDP가 계속 바뀜)에서는 유용함(과거는 잊자.)
- 매번 $G_t$를 향해 조금씩 움직인다고 생각해도 좋음





### Temporal-Difference

- MC와 동일하게 경험으로 부터 학습함.
- Model-Free
- Learn from **Incomplete** episodes, by bootstrapping (끝까지 안가봐도된다.)
- TD updates a guess towards a guess



$$V(s_t) \leftarrow V(s_t)+\alpha(R_{t+1}+\gamma V(s_{t+1})-V(s_t))$$

- $R_{t+1}+\gamma V(s_{t+1})$방향으로 업데이트함. 이를 TD-target이라고함.



> 직관적으로 생각해보자
>
> 우선 한 스텝을 가서 리워드를 받는다 (한 스텝을 갔으니 현재 스탭은 $t+1$)
>
> 그리고 한 스텝을 더 간 상태에서 $V$를 구해서 업데이트 한다.
>
>
> 또 다른 예시는 내가 중앙선을 침범했는데 반대편에서 오는 트럭과 충돌할 뻔 했다고 가정하자. MC의 경우 충돌한게 아니라서 리워드를 받지 못하지만, TD의 경우 중앙선을 침범하는 state가 좋지 않다는 것을 '**추측**'할 수 있고 이를 통해 현재 state가 나쁜 state라고 평가를 할 수 있다.



- guess($V(s_t)$) toward a guess($V(s_{t+1})$) 조금 더 정확하게 한 스텝 가보고 그걸로 업데이트 한다는게 핵심.

