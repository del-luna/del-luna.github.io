---
layout: post
title: Topic Modeling
author: Jaeheon Kwon
categories: Ai
tags: [LSA,LDA]
---



# Topic Modeling

[Github](https://github.com/py-tonic/Topic_Modeling)

## TF-IDF

---

1. tf(d,t) : 특정 문서 d에서의 특정 단어 t의 등장 횟수. DTM이 각 문서에서 각 단어의 등장 빈도를 나타내는 값이기 때문에 같다고 볼 수 있음
2. df(t) : 특정 단어 t가 등장한 문서의 수. 특정 단어가 각 문서에 '**몇 번**' 등장했는지는 중요치 않음, 각 문서별로 등장했으면 1로 카운팅함.
3. idf(d,t) : df(t)에 반비례 하는 수. $idf(d,t) = log(\frac{n}{1+df(t)})$ 단순히 역수를 취하면 n값에 따라 idf가 기하 급수적으로 커지기 때문에 log를 사용함.



idf에 대한 또 다른 직관적인 설명은 불용어 등과 같이 자주 쓰이는 단어들은 비교적 자주 쓰이지 않는 단어들 보다 최소 수십 배 자주 등장함.

그런데 비교적 자주 쓰이지 않는 단어들 조차 희귀 단어들과 비교하면 최소 수백 배는 더 자주 등장하는 편.

이 때문에 log를 씌워 주지 않으면, 희귀 단어들에 엄청난 가중치가 부여될 수 있기 때문에 격차를 줄여주기 위해서 씌운다.

또한 log안에 1을 더해주는 이유는 특정 단어가 전체 문서에서 등장하지 않을 경우 분모가 0이되는 상황을 방지해준다.

TF-IDF는 모든 문서에서 자주 등장하는 단어는 중요도가 낮다고 판단하며, 특정 문서에서만 자주 등장하는 단어는 중요도가 높다고 판단한다.

즉, TF-IDF값이 낮으면 중요도가 낮으며, 높으면 중요도가 높은 것이다.

| DTM   | 과일이 | 길고 | 노란 | 먹고 | 바나나 | 사과 | 싶은 | 저는 | 좋아요 |
| ----- | ------ | ---- | ---- | ---- | ------ | ---- | ---- | ---- | ------ |
| 문서1 | 0      | 0    | 0    | 1    | 0      | 1    | 1    | 0    | 0      |
| 문서2 | 0      | 0    | 0    | 1    | 1      | 0    | 1    | 0    | 0      |
| 문서3 | 0      | 1    | 1    | 0    | 2      | 0    | 0    | 0    | 0      |
| 문서4 | 1      | 0    | 0    | 0    | 0      | 0    | 0    | 1    | 1      |

우선 TF는 위의 DTM을 그대로 사용하면, 각 문서에서의 각 단어의 TF가 된다.

그렇다면 우선 구해야 할 것은 TF와 곱해야 할 값인 IDF.

로그는 자연 로그를 사용한다.

| Word   | IDF                 |
| ------ | ------------------- |
| 과일이 | ln(4/(1+1)) = 0.693 |
| 길고   | ln(4/(1+1)) = 0.693 |
| 노란   | ln(4/(1+1)) = 0.693 |
| 먹고   | ln(4/(2+1)) = 0.287 |
| 바나나 | ln(4/(2+1)) = 0.287 |
| 사과   | ln(4/(1+1)) = 0.693 |
| 싶은   | ln(4/(2+1)) = 0.287 |
| 저는   | ln(4/(1+1)) = 0.693 |
| 좋아요 | ln(4/(1+1)) = 0.693 |

문서의 총 수는 4이기 때문에 분자는 늘 4로 동일하다.

분모의 경우 각 단어가 등장한 문서의 수(df)를 의미하는데, 예를 들어 '**먹고**' 의 경우에는 총 2개의 문서(문서1, 문서2)에 등장하므로 2라는 값을 가진다.

각 단어에 대해서 IDF 값을 비교해보면 차이가 있는데 IDF는 앞서 말했듯 여러 문서에서 등장한 단어에 대해 가중치를 낮추는 역할을 한다.

그러면 이제 TF-IDF를 계산해보자.

TF는 DTM을 가져오면 각 문서에서 각 단어의 TF를 가져오게 되기 때문에, 앞서 사용한 DTM에서 각 단어 별로 위의 IDF값을 그대로 곱해주면 TF-IDF가 나오게 됩니다.

| TF-IDF | 과일이 | 길고  | 노란  | 먹고  | 바나나 | 사과  | 싶은  | 저는  | 좋아요 |
| ------ | ------ | ----- | ----- | ----- | ------ | ----- | ----- | ----- | ------ |
| 문서1  | 0      | 0     | 0     | 0.287 | 0      | 0.693 | 0.287 | 0     | 0      |
| 문서2  | 0      | 0     | 0     | 0.287 | 0.287  | 0     | 0.287 | 0     | 0      |
| 문서3  | 0      | 0.693 | 0.693 | 0     | 0.575  | 0     | 0     | 0     | 0      |
| 문서4  | 0.693  | 0     | 0     | 0     | 0      | 0     | 0     | 0.693 | 0.693  |

TF-IDF = DTM(TF) * IDF

즉, 특정 문서에서만 등장했고, 그 빈도수가 높다면 TF-IDF가 높은 값이 나오게 된다.

## Latent Semantic Analysis

---

LSA는 토픽 모델링을 위해 최적화 된 알고리즘은 아니지만, 토픽 모델링이라는 분야에 아이디어를 제공한 알고리즘이다.

LDA는 LSA의 단점을 개선하여 탄생한 알고리즘이라고 볼 수 있다.

BoW에 기반한 DTM이나 TF-IDF는 기본적으로 단어의 빈도 수를 이용한 수치화 방법이기 때문에 단어의 의미를 고려하지 못한다는 단점이 있다.(이를 토픽 모델링에서는 단어의 토픽을 고려하지 못한다고 함)

이를 위한 대안으로 DTM의 Latent한 의미를 이끌어내는 방법으로 잠재 의미 분석(Latent Semantic Analysis)라는 방법이 존재.

이 방법을 위해서는 SVD(Singular Value Decomposition)이 필요하다.

## Singular Value Decomposition

---

SVD란 A가 m x n행렬일 때, 다음과 같이 3개의 행렬 곱으로 분해 하는 것을 말함.

$A=UΣV^T$

여기서 각 3개의 행렬은 다음과 같은 조건을 만족합니다.

$U : m\times m$ 직교행렬($AA^T=U(ΣΣ^T)U^T$)

$V : n\times n$ 직교행렬($A^TA = V(Σ^TΣ)V^T$)

$Σ : m \times n$ 직사각 대각행렬

## Truncated SVD

---

<img src = "https://py-tonic.github.io/images/topic/svdtruncatedsvd.png">

절단된 SVD는 대각 행렬 Σ의 대각 원소 값 중에서 상위 값 t개만 남게 된다.

절단된 SVD를 수행하면 값의 손실이 일어나므로 기존의 행렬 A를 복구할 수 없게 된다.

또한 U행렬의 t열, V행렬의 t행 까지만 남긴다.

여기서 t는 우리가 찾고자 하는 토픽의 수를 반영한 하이퍼 파라미터.

t를 크게 잡으면 기존의 행렬 A로부터 다양한 의미를 가져갈 수 있지만, t를 작게 잡아야 노이즈를 제거할 수 있다.

일부 벡터들을 제거하는 것을 데이터 차원을 줄인다고도 말하는데, 데이터의 차원을 줄이게 되면 당연히 풀SVD를 하였을 때 보다 직관적으로 계산 비용이 낮아진다.

또한 상대적으로  중요하지 않은 정보를 삭제하는 효과를 갖게 되는데, 이는 영상 처리 분야에서 노이즈를 제거한다는 의미를 갖고, 자연어 처리에서는 설명력이 낮은 정보를 삭제하고 높은 정보를 남긴다는 의미를 갖는다.

다시말하면 기존의 행렬에서 드러나지 않았던 심층적인 의미를 확인할 수 있게 해준다.

## LSA

---

기존의 DTM이나 DTM의 단어 중요도에 따른 가중치를 주었던 TF-IDF는 단어의 의미를 전혀 고려하지 못한다는 단점이 존재한다.

LSA는 기본적으로 DTM이나 TF-IDF 행렬에 Truncated SVD를 사용하여 차원을 축소시키고 단어들의 잠재적인 의미를 끌어낸다는 아이디어를 갖는다.



| DTM   | 과일이 | 길고 | 노란 | 먹고 | 바나나 | 사과 | 싶은 | 저는 | 좋아요 |
| ----- | ------ | ---- | ---- | ---- | ------ | ---- | ---- | ---- | ------ |
| 문서1 | 0      | 0    | 0    | 1    | 0      | 1    | 1    | 0    | 0      |
| 문서2 | 0      | 0    | 0    | 1    | 1      | 0    | 1    | 0    | 0      |
| 문서3 | 0      | 1    | 1    | 0    | 2      | 0    | 0    | 0    | 0      |
| 문서4 | 1      | 0    | 0    | 0    | 0      | 0    | 0    | 1    | 1      |



```python
import numpy as np
A=np.array([[0,0,0,1,0,1,1,0,0],[0,0,0,1,1,0,1,0,0],[0,1,1,0,2,0,0,0,0],[1,0,0,0,0,0,0,1,1]])
np.shape(A)
```

```python
(4,9)
```

4 * 9의 크기를 가지는 DTM이 생성됨.

이에 대해서 Full SVD와 Truncated SVD를 수행하고 결과 값을 비교해보자.

- Full SVD

```python
U, s, VT = np.linalg.svd(A, full_matrices = True)

```

- Truncated SVD

```python
# hyperparameter t = 2
S = S[:2, :2]
U = U[:, :2]
VT = VT[:2, :]
A_prime = np.dot(np.dot(U, S), VT)
print(A) # origin
print(A_prime.round(2)) # Truncated
```

```python
[[0 0 0 1 0 1 1 0 0]
 [0 0 0 1 1 0 1 0 0]
 [0 1 1 0 2 0 0 0 0]
 [1 0 0 0 0 0 0 1 1]]
[[ 0.   -0.17 -0.17  1.08  0.12  0.62  1.08 -0.   -0.  ]
 [ 0.    0.2   0.2   0.91  0.86  0.45  0.91  0.    0.  ]
 [ 0.    0.93  0.93  0.03  2.05 -0.17  0.03  0.    0.  ]
 [ 0.    0.    0.    0.    0.    0.    0.    0.    0.  ]]
```

대체적으로 기존에 0인 값들은 0에 가까운 값이 나오고, 1인 값들은 1에 가까운 값들이 나옴.

또한 제대로 복구되지 않은 구간도 보인다.

이제 각각 차원이 축소된 U, S, VT의 의미를 살펴보자.

축소된 U는 4 x 2 의 크기를 가지는데 이는 문서의 크기 x 토픽의 수(t)의 크기이다.

단어의 개수인 9는 유지되지 않는데, 문서의 개수인 4는 유지되어서 4개의 문서를 각각 2개의 값으로 '표현' 하고 있다.

즉 U의 각 행은 잠재 의미를 표현하기 위한 수치화된 각각의 문서 벡터 라고 볼 수 있다.

축소된 VT는 2 x 9 의 크기를 가지는데, 이는 잘 생각해보면 토픽의 수(t) x 단어의 개수 이며,

VT의 각 열은 잠재 의미를 표현하기 위해 수치화된 각각의 단어 벡터라고 볼 수 있다.

이 문서 벡터들과 단어 벡터들을 통해 다른 문서의 유사도, 다른 단어의 유사도, 단어(쿼리)로 부터 문사의 유사도를 구하는 것들이 가능해진다.



## Latent Dirichlet Allochtion

---

토픽 모델링은 문서의 집합에서 토픽을 찾아내는 프로세스를 말한다.

이는 검색 엔진, 고객 민원 시스템 등과 같은 문서의 주제를 알아내는 일이 중요한 곳에서 사용된다.

LDA는 대표적인 토픽 모델링의 알고리즘이다.

DTM을 만들고 LDA를 수행한 결과를 보여준다.



## Introduction

---

우선 LDA를 블랙 박스로 보고 LDA에 문서 집합을 입력하면,  LDA에 문서 집합을 입력하면 어떤 결과를 보여주는지 간소화된 예시를 들어보자.

문서1 : 저는 사과랑 바나나를 먹어요

문서2 : 우리는 귀여운 강아지가 좋아요

문서3 : 저의 깜찍하고 귀여운 강아지가 바나나를 먹어요



LDA를 수행할 때 문서 집합에서 토픽이 몇 개가 존재할지 가정하는 것은 사용자가 해야 할 일이다.(**hyperparameter**)

여기서는 LDA에 2개의 토픽을 찾아달라고 요청해보자.

LDA에 입력으로 들어가는 DTM은 전처리를 거쳤다고 가정한다.

LDA는 각 문서의 토픽 분포와 각 토픽 내의 단어 분포를 추정한다.

<각 문서의 토픽 분포>

문서1 : 토픽 A 100%

문서2 : 토픽 B 100%

문서3 : 토픽 B 60%, 토픽 A 40%

<각 토픽의 단어 분포>

토픽 A : 사과 20%, 바나나 40%, 먹어요 40%, 귀여운 0%, 강아지 0%, 깜찍하고 0%, 좋아요 0%

토픽 B : 사과 0%, 바나나 0%, 먹어요 0%, 귀여운 33%, 강아지 33%, 깜찍하고 16%, 좋아요 16%



LDA는 토픽의 제목을 정해주진 않지만, 이 시점에서 알고리즘의 사용자는 위 결과로부터 두 토픽이 각각 과일에 대한 토픽과 강아지에 대한 토픽이라고 판단해볼 수 있다.



## Assumption

---

LDA는 문서의 집합으로부터 어떤 토픽이 존재하는지를 알아내기 위한 알고리즘이다.

LDA는 앞서 배운 빈도수 기반의 표현 방법인 BoW, DTM, TF-IDF를 입력으로 하는데, 여기서 알 수 있는 점은 LDA는 **단어의 순서**는 신경쓰지 않는다.

각 문서는 다음과 같은 과정을 거쳐서 작성되었다고 가정한다.

1. 문서에 사용할 단어의 개수 N을 정한다.
2. 문서에 사용할 토픽의 혼합을 확률 분포에 기반하여 결정한다. (Ex. 강아지 토픽 60%, 과일 토픽 40%)
3. 문서에 사용할 각 단어를 정한다.
    - 토픽 분포에서 토픽 T를 확률적으로 고른다.
    - 선택한 토픽에서 단어 출현 확률 분포에 기반해 문서에 사용할 단어를 고른다.



## Execute

---

수행 과정을 정리해보자.

1. 사용자는 알고리즘에게 토픽의 개수($k$)를 알려준다.
2. 모든 단어를 $k$개 중 하나의 토픽에 할당한다.
    - LDA가 모든 문서의 모든 단어에 대해서 $k$개 중 하나의 토픽을 랜덤으로 할당한다. 이 작업이 끝나면 각 문서는 토픽을 가지게 되며, 토픽은 단어 분포를 가지는 상태이다. 물론 랜덤으로 할당하였기 때문에 사실 이 결과는 대부분 틀린 상태이다. 만약 한 단어가 문서에 2회이상 등장 하였다면 서로다른 토픽에 할당되었을 수도 있다.
3. 이제 모든 문서의 모든 단어에 대해서 아래의 사항을 반복 진행한다.
    - 어떤 문서의 각 단어 w는 자신은 잘못된 토픽에 할당되어져 있지만, 다른 단어들은 전부 올바른 토픽에 할당되어진 상태라고 가정하자. 이에 따라 단어 w는 아래의 두 가지 기준에 따라서 토픽이 재할당 된다.
        - $p(topic(t)\vert document(d))$ : 문서 d의 단어들 중 토픽 t에 해당하는 단어들의 비율
        - $p(word(w)\vert topic(t))$ : 단어 w를 갖고 있는 모든 문서들 중 토픽 t가 할당된 비율

-> 재해석하면 우선 토픽이 문서 d의 단어 w에 토픽을 할당하려고한다.

1. 단어 w가 속한 문서에서 다른 단어들에 할당된 토픽을 본다.
2. 현재 문서($d$)에서 동일한 단어 w에 할당된 토픽과 다른 문서($d1,...,dn$)에서 동일한 단어 w에 할당된 토픽들을 본다.



## Conclusion

---

LSA : DTM을 차원 축소 하여 축소 차원에서 근접 단어들을 토픽으로 묶는다.

LDA : 단어가 특정 토픽에 존재할 확률과 문서에 특정 토픽이 존재할 확률을 결합 확률로 추정하여 토픽을 추출한다.

<img src = "https://py-tonic.github.io/images/topic/1.png">

| Terms      | Topic1 | Topic2 | Topic3 |
| ---------- | ------ | ------ | ------ |
| Baseball   | 0      | 0      | 0.2    |
| Basketball | 0      | 0      | 0.267  |
| Boxing     | 0      | 0      | 0.133  |
| Money      | 0.231  | 0.313  | 0.4    |
| Interest   | 0      | 0.312  | 0      |
| Rate       | 0      | 0.312  | 0      |
| Democrat   | 0.269  | 0      | 0      |
| Republican | 0.115  | 0      | 0      |
| Cocus      | 0.192  | 0      | 0      |
| President  | 0.192  | 0.063  | 0      |

LDA에서 각 토픽에 할당된 모든 단어들의 확률을 합하면 1이된다 (이래서 디리클레인가..?)

각 토픽$\Phi_k$은 토픽의 개수를 설정하는 하이퍼 파라미터($\beta$)에 영향을 받음.

$\theta_d$는 d번 째 문서가 가진 토픽 비중을 나타내는 벡터이다.(전체 토픽 개수 $K$만큼의 길이를 가진다.)

$\theta_1$은 아래의 표에서 첫 번째 행벡터가 된다.($\theta_d$또한 확률이므로 모든 요소의 합은 1이된다.)

$\theta$ 역시 하이퍼 파라미터 $\alpha$에 영향을 받는다.(이 또한 디리클레분포를 따른다는 가정)

| Docs | Topic1 | Topic2 | Topic3 |
| ---- | ------ | ------ | ------ |
| Doc1 | 0.4    | 0      | 0.6    |
| Doc2 | 0      | 0.6    | 0.4    |
| Doc3 | 0.375  | 0.625  | 0      |
| Doc4 | 0      | 0.375  | 0.625  |
| Doc5 | 0.5    | 0      | 0.5    |
| Doc6 | 0.5    | 0.5    | 0      |







## Reference

---

[Wikidocs](https://wikidocs.net/30708)