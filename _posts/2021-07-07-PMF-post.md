---
layout: post
title: Probabilistic Matrix Factorization
author: Jaeheon Kwon
categories: Papers
tags: [Recommend]
---



## Summary

- 기존의 Low Rank Approximation을 Norm based penalty로 치환한 방법은 적은 데이터셋에서만 사용 가능 

- Large Dataset에서 robust한 기법을 제안함.



본 논문을 읽기 위해서는 [MMMF](https://papers.nips.cc/paper/2004/file/e0688d13958a19e087e123148555e4b4-Paper.pdf)를 읽고 오거나 적어도 Low Rank Approximation, Matrix Completion에 대한 선수 지식이 조금 필요하다.

근데 솔직히 위 내용 다 알려면 빡빡한 선대 지식이랑 Convex Optimization도 알아야 하니까 우리는 이렇게 받아 들이자.

- MF는 기본적으로 Low Rank Approximation으로 문제를 풀려고 한다.(이게 Reconstruction에 유리하다.)
- 근데 이거 Non-Convex라서 좀 그렇다.
- 그러면 Convex로 치환하는 기법이 필요하다.
- 그게 Low Rank -> Norm based Penalty이다.

기본적으로 Norm은 Convex이고... Low Rank 하는 것이 Nuclear Norm Minimization과 동일하다.

더 궁금하다면 [Fazel 2001](https://web.stanford.edu/~boyd/papers/pdf/rank_min_heur_sys_approx.pdf)이나 위의 MMMF를 참고하자.

아무튼 위 방법이 효과적이지 않았는데, Large Dataset에선 잘 동작하지 않았다.

그래서 본 논문에서 제안하는 확률 기반의 Matrix Factorization은 다음과 같다.

$$p(R\vert U,V,\sigma^2) = \prod\limits_{i=1}^N \prod\limits_{j=1}^M [N(R_{ij}\vert U_i^TV_j, \sigma^2)]^{I_{ij}}$$

복잡해보이는데 사실 별거 없다. 노테이션은 다 설명하면 너무 길어져서 우선 위 수식의 의미만 알고 가자.

평균을 $U^TV$, 분산을 $\sigma^2$으로 가지는 가우시안 분포이다. 여기서 평균은 user-item의 latent matrix의 곱이다.

그러니까 우리의 데이터 셋에서 R이 나올 확률은 각각 user-item latent를 내적한 것을 평균으로 가지고, 분산을 $\sigma^2$으로 하는 분포를 따른다는 것이다.

또한 user-item latent matrix도 가우시안을 따른다고 가정하는데, 다음과 같다.

$$p(U\vert \sigma_U^2) = \prod\limits_{i=1}^N N(U_i\vert 0, \sigma^2_UI)$$

$$p(V\vert \sigma_V^2) = \prod\limits_{i=1}^N N(V_i\vert 0, \sigma^2_VI)$$

여기까지 나왔으면 그냥 수식 3개를 베이즈 룰로 엮을 수 있다.

$$p(R\vert U,V,\sigma^2) \propto p(R\vert U, V, \sigma^2)p(U\vert \sigma_U^2)p(V\vert \sigma_V^2)$$

이걸 이용해서 MAP를 사용하게 되는데..(사실 가우시안이니까 로그 붙여서 곱으로 표현된거 덧셈으로 편하게 해주는 트릭 쓴다.)

$$lnp(U,V\vert R,\sigma^2,\sigma_V^2,\sigma_U^2) \propto -\frac{1}{2\sigma^2}\sum\limits_{i=1}^n\sum\limits_{j=1}^p I_{ij}(X_{ij} - u_i^Tv_j)^2 - \frac1{2\sigma_u^2}\sum\limits_{i=1}^n u_i^Tu_i - \frac1{2\sigma_v^2}\sum\limits_{j=1}^p v_j^Tv_j$$

보면 알겠지만 뒤에 두 텀은 그냥 내적꼴이고 따라서 우리가 최소화 해야 하는 로스 함수는 다음과 같다.

$$E=\frac12 \sum\limits_{i=1}^N\sum\limits_{j=1}^MI_{ij}(R_{ij}-U_i^TV_j)^2 + \frac{\lambda_U}{2}\sum\limits_{i=1}^N \vert\vert U_i\vert\vert^2_{F} + \frac{\lambda_V}{2}\sum\limits_{j=1}^M \vert\vert V_j\vert\vert^2_{F}$$

우리에게 익숙한 qudratic(goodness of fit) + norm based penalty 형태의 로스 함수를 볼 수 있다.

결국 (prior)분산이 하이퍼 파라미터가 돼었는데 이를 조절해서 행렬의 크기를 조정하고 정규화 한다.

여기서 얻을 수 있는 사실은 U와 V에 대해 more complex prior distribution을 선택함으로서 sparse solution을 찾을 수 있다고 한다.(결국 이게 아까 서두에서 말한 low rank approximation형태를 prior를 통해 유도하겠다는 뜻인거 같다.)

이제 SGD를 통해 업데이트 하고 (Ridge solution을 사용할 수 있지만 SGD가 Large Dataset에서 확장하기 쉽다고 함.)





## Reference

- [paper](https://papers.nips.cc/paper/2007/file/d7322ed717dedf1eb4e6e52a37ea7bcd-Paper.pdf)
- [towards data science](https://towardsdatascience.com/probabilistic-matrix-factorization-b7852244a321)