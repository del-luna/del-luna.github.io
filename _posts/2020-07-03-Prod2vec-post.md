---
layout: post
title: Prod2Vec
author: Jaeheon Kwon
categories: Papers
tags: [recommendation]
---

# E-commerce in Your Inbox: Product Recommendations at Scale



## Abstract

<hr>

최근 몇 년 동안 온라인 광고는 점점 더 효과적이며 편재화 되었습니다.

방문자에게 표시되는 광고는 디지털 컨텐츠를 게시하고 SNS를 관리하며 메일 서비스를 운영하는 사이트 및 앱에 자금을 제공합니다.

이러한 다양한 인터넷 리소스가 제공되므로 주어진 플랫폼에 적합한 유형의 광고를 결정하는 것이 재정적 성공에 결정적인 요소가 되었습니다.

이러한 광고들로 인해 뉴스와 소셜피드는 큰 성공을 거두었으나, 현재까지 이메일 클라이언트 광고에 대한 공식은 아직 존재하지 않습니다.

논문에서는 전자 메일 영수증에서 결정된 사용자 구매 내역을 활용하여 맞춤화된 제품 광고를 야후 메일 사용자에게 제공하는 시스템에 대해 설명합니다.

효과적인 제품 추천을 제공하기 위해 특별히 고안된 새로운 신경 언어 기반 알고리즘을 사용합니다.

이 알고리즘은 동시 발생을 기반으로 예측된 제품 및 인기있는 제품을 포함하는 기준에 대해 평가되었습니다.

우리는 대규모 제품 구매 데이터 세트를 사용하여 172개의 전자 상거래 웹 사이트에서 2.9M이상의 사용자를 대상으로 엄격한 오프라인 테스트를 수행했습니다.

제품 추천 형식의 광고는 온라인 트래픽에서 성공적으로 테스트 되었으며, 메일 광고 형식보다 클릭률이 꾸준히 9% 증가하고 전환률도 비슷한 것으로 나타났습니다.



## Introduction

<hr>

전 세계적으로 많은 사람들은 이메일을 사용하고 이러한 엄청난 양의 트래픽으로 수익을 창출하기 위해 전자 메일 클라이언트는 일반적으로 전자 메일 콘텐츠와 함께 이미지 형식의 광고를 띄웁니다.

사용자가 광고를 클릭할 의사가 있도록 만드는 것은 어려운 일입니다.

각 개인 사용자에게 가장 적합한 광고를 게재하는 것이 목표인 효과적인 개인화 및 타겟팅(*paper)은 이 문제를 해결하는데 필수적입니다.

전자 메일만 사용하려는 사용자의 성향을 극복하려면 광고의 관련성이 높아야 합니다.

온라인 비즈니스를 위한 재정적 이익 외에도 개별 소비자 취향에 맞게 광고를 사전에 맞춤화 하면 사용자 충성도와 유지도를 높일 수 있습니다.

Inbound(수신..?)메일은 여전히 광고 타겟팅의 목적으로 충분히 탐색되고 이용되지는 않지만 수익 창출이 가능한 데이터를 발견할 수 있습니다.

연구에 따르면 Inbound의 10%만이 사람이 생성한 메일입니다.

또한 트래픽의 90%중 22%이상이 온라인 쇼핑과 관련된 전자 메일을 나타냅니다.

전체 트래픽의 상당 부분에 상업적인 의도가 있는 경우, 인기있는 타겟팅 광고 형식은 MRT(mail retargeting)이며, 특정 상업 웹 도메인에서 이전에 전자 메일을 받은 사용자를 대상으로 합니다. 

이 이메일들은 각 고객의 관심사와 상업 도메인과의 관계에 대한 광범위한 그림을 제공하므로, 타겟팅에 유용하고 강력한 신호를 제공합니다.

최근의 논문은 (Inbound data의)광범위한 잠재력을 활용하기 위해 MRT 규칙을 생성하기 위핸 클러스터링 방법을 제시하여 그러한 규칙이 전문가에 의해 생성된 것 보다 더 정확하다는 것을 보여 주었습니다.

그러나 사용자와 전자 상거래 사이트가 통신한다는 사실만을 활용한 MRT 규칙을 넘기 위해서 광고주는 구매한 제품 이름 및 전자 메일의 본문의 일부인 가격과 같이 보다 자세한 데이터가 필요합니다.

<img src = "https://py-tonic.github.io/images/e-commerce/1.png">



전자 메일 영수증 추출 기능은 개별 구매 내역을 기반으로 사용자에게 제품 광고를 통해 수익 창출의 기회를 제공합니다.

여러 상업적 전자 메일 도메인에서 구매 데이터를 얻을 수 있는 경우 훨씬 더 좋은 솔루션을 제공할 수 있습니다.

특히 "X를 구매한 고객은 Y도 구매했습니다."라는 유형의 추천을 하는 전자 상거래 사이트와 달리 이메일 제공 업체는 다음과 같은 훨씬 더 강력하고 효과적인 타겟팅 솔루션을 추천할 수 있습니다:

"V1이라는 곳에서 X를 구매한 고객도 V2에서 Y를 구매했습니다."

본 논문에서는 수백만명의 사용자와 제품으로 확장 가능한 item-level의 구매 예측 알고리즘을 개발했습니다.

이를 위해 시계열의 사용자 구매기록에 적용되는 신경 언어 모델을 사용하여 실제 값의 저차원 벡터 공간에 제품을 포함시키는 접근법을 제안합니다.

결과적으로 비슷한 맥락(즉, 주변 구매)을 가진 제품이 임베딩 공간에서 근처에 있는 벡터로 매핑됩니다.

다음에 구매할 제품에 대해 의미 있고 다양한 제한을 할 수 있도록 제품 벡터를 클러스터링하고 클러스터 간 전환 확률을 모델링합니다.

가장 가능성이 높은 클러스터의 임베딩 공간에서 가장 가까운 제품은 제품에 대한 최고 권장 사항을 형성하는 데 사용됩니다.

콜드 스타트 문제를 완화하기 위해 사용자 코호트의 인기있는 제품은 사전 구매기록 없이 사용자에게 백업 광고 권장 사항으로 사용되었습니다.



## Related Work

<hr>



### Leveraging e-mail data in advertising.

웹 환경은 컨텐츠 게시자에게 사용자의 등록 정보 및 사용자 클릭, 페이지 조회, 검색, 웹 사이트 방문, 소셜 활동 및 광고와의 상호 작용, 활동 로그 캡처를 포함하여 오프라인 설정보다 훨씬 자세하게 사용자 행동을 추적할 수 있는 수단을 제공합니다.

이를 통해 사용자의 행동에 따라 사용자를 타겟팅 할 수 있습니다.

빅 데이터 어플리케이션 및 플랫폼의 등장으로 머신 러닝 접근 방식은 광고 타겟팅 프로세스를 자동화 하는데 크게 활용됩니다.

머신 러닝의 경우 모델 학습에 사용되는 피쳐는 일반적으로 배포된 알고리즘의 성능에 중요한 영향을 미칩니다.

퍼블리셔가 수집한 사용자 이벤트에서 파생된 피쳐는 종종 클릭 또는 구매에 대한 사용자의 경향을 예측하는데 사용됩니다.

그러나 이러한 피쳐는 퍼블리셔와 광고주가 실제로 관심을 가지고 있는 것, 즉 사용자의 구매 의도에 대한 약한 피드백입니다.

반면 프로모션 및 구매 영수증 형태의 상업용 전자 메일은 강력하고 직접적인 구매 의도 신호를 전달하여 광고주가 고품질의 고객에게 도달할 수 있도록 합니다.

Direct Marketing Association의 전국 고객 이메일 보고서에 따르면 이메일 사용자는 일반 온라인 고객 보다 10% 이상의 가치가 있는 것으로 식별되었습니다.

최근의 연구는 상용 이메일 데이터의 가치를 조사하려는 첫 번째 시도 중 하나였습니다.

저자는 상용 도메인을 클러스터링 하기 위해 수신된 전자 메일 수에 SPCA(Sparse Principal Component Analysis)를 적용하여 전자 메일 데이터 소스에 대한 상당한 가능성을 보여 주었습니다.

전자 메일 도메인 외부에서 사용자의 구매 내역에 대한 정보는 전자 상거래 웹 사이트에서 광범위하게 사용되어 사용자에게 관련 제품을 추천합니다.

추천 시스템은 유사한 관심사를 가진 사용자의 구매 행동(CF)을 이용하거나 다른 제품과 사용자의 과거 상호 작용을 사용하여(cotext-based) 예측합니다.

이러한 연구와 달리 전자 메일에서 추출한 구매 데이터를 통해 더 나은 제품 예측을 배우기 위해 수 백개의 다른 사이트에서 정보를 수집할 수 있으므로 단일 사이트의 데이터에만 국한되지 않습니다.



### Neural language models.

여러 NLP 응용 프로그램에서 특정 목표는 일련의 단어에 확률 값을 할당하는 작업으로 일반화 될 수 있습니다.

이를 위해 언어 모델이 개발 되어 수학적 모델을 정의하여 단어의 통계적 속성과 단어간의 종속성을 포착합니다.

전통적으로 언어 모델은 단어를 원-핫 인코딩으로 표현했습니다.

그러나 이 방법은 종종 실제 작업에서 높은 차원과 심한 데이터 희소성으로 인해  최적의 성능을 얻지 못합니다.

이러한 문제를 해결하기 위해 신경 언어 모델이 제안되어 신경망을 통해 저차원의 분산된 단어 임베딩을 유도합니다.

이러한 접근법은 텍스트 문서에서 단어 순서를 이용하여 단어 순서에서 더 가까운 단어가 통계적으로 더 의존적이라는 가정을 명시적으로 모델링합니다.

역사적으로 신경망 기반 모델에 대한 비효율적인 훈련은 어휘 크기가 실제 작업에서 수백만으로 늘어날 수 있다는 점을 고려할 때 더 넓은 적용 가능성에 대해 장애가 되어 왔습니다.

그러나 이 문제는 단어 표현을 학습하기 위한 확장성이 뛰어난 continuous bag-of-words(CBOW) 및 SG(skip-gram)언어 모델의 개발과 함께 성공적으로 해결되었습니다.

이러한 강력하고 효율적인 모델은 대규모 텍스트 코포라에서 단어 사이의 구문적, 의미적 관계를 모두 포착하여 많은 NLP작업에서 최첨단의 결과를 얻음으로써 매우 유망한 결과를 보여주었습니다.

<img src = "https://py-tonic.github.io/images/e-commerce/2.png">



### Proposed Approach

<hr>

이 섹션 에서는 전자 메일 영수증으로 부터 결정된 이전 구매에 대한 정보를 활용하는 제품 추천에 대해 제안된 방법을 설명합니다.

이 작업을 해결하기 위해 신경 언어 모델을 사용하여 기록 로그에서 저 차원 공간의 제품 표현을 학습할 것을 제안합니다.

학습된 임베딩 공간에서 단순한 nearest neighbor search를 통해 제품 추천을 수행할 수 있습니다.

보다 구체적으로, $N$명의 사용자로부터 획득한 이메일 수신 로그 세트 $S$가 주어지면, 사용자 로그 $s = (e_1,...,e_M) \in S$는 $M$ 영수증의 통합되지 않은 순서로 정의됩니다.

각 이메일 영수증 $ e_m = (p_{m1},p_{m2},...,p_{mT_{m}}) $은 구매 제품으로 구성되며, 우리의 목표는 유사한 제품이 벡터 공간안에서 근처에 있도록 각 제품 $p$의 $D$-차원 실제 값 표현 $v_p \in \R^D$를 찾는 것입니다.

우리는 전자 메일 영수증의 추천을 구체적으로 다루는 제품 표현을 학습하기 위한 몇 가지 접근법을 제안합니다.

먼저 구매한 모든 제품을 독립적으로 고려하는 prod2vec 방법을 제안합니다. 그런 다음 일부 제품은 전자 메일 영수증에 함께 구매한 것으로 표시되어 보다 유용한 제품 표현을 제공하는 참신한 bagged-prod2vec 방법을 제안합니다.

마지막으로, 우리는 학습된 표현을 사용하는 제품 대 제품 및 사용자 대 제품 추천 모델을 제시합니다.



### Low-dimensional product embeddings.

**prod2vec.** prod2vec모델은 구매 시퀀스의 개념을 "문장"으로 사용하고 시퀀스 내의 제품을 "단어"로 사용하여 전자 메일 영수증 로그로 부터 제품의 벡터 표현을 학습하여 NLP도메인에서 용어를 빌리는 것을  포함합니다.

보다 구체적으로 prod2vec는 다음과 같이 정의된 전체 이메일 수신 로그 세트 $S$에 대해 목적 함수를 최대화 하여 스킵-그램 모델을 사용하여 제품 표현을 학습합니다.

$L = \sum\limits_{s\in S}\sum\limits_{p_i\in s}\sum\limits_{-c\leq j\leq c, j\neq 0} logP(p_{i+j}\vert p_i) \tag{3.1}$

동일한 이메일의 영수증으로 부터 제품이 온 경우 순서는 랜덤입니다.

현재 제품 $p_i$가 주어진 경우 주변 제품 $p_{i+j}$ 를 관찰할 확률 $P(p_{i+j}\vert p_i)$는 소프트 맥스 함수를 사용하여 정의합니다.

$P(p_{i+j}\vert p_i) = \frac{exp(v_{p_i}^Tv'_{p_{i+j}})}{\sum_{p=1}^P exp(v_p^Tv'_p)} \tag{3.2}$

$v_p$ 와 $v'_p$는 제품 $p$ 의 입력 및 출력 벡터 표현이며, $c$는제품 시퀀스의 컨텍스트 길이 이며 $P$는 고유한 제품의 수 입니다. 

방정식 (3.1)과 (3.2)에서 우리는 prod2vec모델이 제품 시퀀스의 컨텍스트를 보았는데, 여기에서 유사한 컨텍스트를 가진 제품(즉, 비슷한 이웃 구매가있는)은 유사한 벡터 표현을 갖습니다.

그러나 prod2vec는 전자 메일 영수증에 동시에 구매한 여러 제품이 포함될 수 있음을 명시적으로 고려하지 않습니다.

아래에서 설명하는 bagged 버전의 prod2vec를 도입하여 해결합니다.

> 위 수식 둘다 word2vec이랑 다를 게 없는 것을 볼 수 있습니다.
>
> 3.2의 분자는 context와 neighbor vector의 내적으로 볼 수 있고 
>
> 벡터의 내적은 코사인 유사도로 볼 수 있으므로 유사도를 높이는게 확률을 높이는 과정으로 볼 수 있다.



**bagged-prod2vec.** 여러 제품을 동시에 구매할 수 있다는 사실을 설명하기 위해 쇼핑백 개념을 소개하는 수정된 Skip-Gram 모델을 제안합니다.

그림에 표시된 것 처럼 모델은 제품 레벨이 아닌 전자 메일 영수증 레벨에서 작동합니다.

제품 벡터 표현은 다음과 같이 정의된 전자 우편 시퀀스에 대해 수정 된 목적함수를 최대화 함으로써 학습합니다. 

$L = \sum\limits_{s \in S}\sum\limits_{e_m\in s} \sum\limits _{-n \leq j \leq n, j\neq 0} \sum\limits_{k=1,...,T_m} logP(e_{m+j}\vert p_{mk}) \tag{3.3}$

m의 k번째 곱을 고려할 때 인접 이메일 영수증 $e_{m+j}, e_{m+j} = (p_{m+j,1}...p_{m+j,T_m})$ 에서 제품을 관찰 할 확률$P(e_{m+j}\vert p_{mk})$ 은 확률 곱 $P(e_{m+j}\vert p_{mk}) = P(p_{m+j}\vert p_{mk})\times ... \times P(p_{m+j,T_m}\vert p_{mk})$ 로 줄어듭니다.(각각 소프트맥스 3.2를 사용하여 정의됨.)



(3.3)의 세 번째 합계는 영수증을 초과하므로 동일한 전자 메일 영수증의 제품은 학습 중에 서로를 예측하지 않습니다.

또한 제품 구매의 일시적인 측면을 포착하기 위해 우리는 지시 언어 모델 사용을 제안합니다. 여기서 context로는 미래 제품만 사용합니다. 

수정을 통해 향후 구매를 예측할 수 있는 제품 임베딩을 학습할 수 있습니다.



**Learning.** 모델은 SGD를 사용하여 최적화 되었으며 대규모 문제에도 사용 가능합니다.

그러나 (3.1)과 (3.3)에서 gradients의 계산은 전체 어휘 크기$P$에 비례하여 실제 작업에서 계산 비용이 많이 듭니다.

> word2vec의 고질적인 문제점.
>
> 유사도 계산을 위해 모든 벡터들에 접근해야 하기 때문에 전체 코퍼스가 커질수록 계산 비용이 너무 커짐.
>
> 이를 해결하기 위한 방안으로 Subsampling과 Negative sampling등의 기법이 존재함.

그 대안으로 우리는 Negative sampling 접근법을 사용하여 계산 복잡성을 크게 줄입니다.



### Product-to-product predictive models

---

저 차원 제품 표현을 학습한 후 다음에 구매할 제품을 예측할 수 있는 몇 가지 가능성을 고려했습니다.

**prod2vec-topK.** 구매한 제품이 주어지면 이 방법은 다른 모든 제품과 코사인 유사도를 계산하여 가장 유사한 상위 K개의 제품을 추천합니다.

**prod2vec-cluster.** 보다 다양한 추천을 할 수 있도록 유사한 제품을 클러스로 그룹화 하고 이전에 구매한 제품의 클러스터와 관련된 클러스터의 제품을 추천하는 것을 고려했습니다.

하둡 분산 시스템 위에 구현된 K-means clustering 알고리즘을 적용하여 저차원 표현 간의 코사인 유사도를 기반으로 제품을 그룹화 했습니다.

클러스터 $c_i$에서 구매한 후 $C$ 클러스터 중 하나에서 제품을 구매하면 다항 분포 $Mu(\theta_{i1},...\theta_{iC})$를 따르는 것으로 가정합니다.

여기서 $\theta_{ij}$는 클러스터 $c_i$에서 구매한 다음 클러스터 $c_j$에서 구매할 확률입니다.

파라미터 $\theta_{ij}$를 추정하기 위해서 각각의 $i$, $j$에 대하여 Maximum likelihood를 적용합니다.

> 다항분포..
>
> $p(x\vert \theta) = \prod\limits_{c=1}^C \theta_{ic}^{x_i}$ 이런 느낌일 듯..?
>
> 솔직히 잘 모르겠다 각각의 $\theta$가 확률 변수로 사용되는 것 같음.

<img src = "https://py-tonic.github.io/images/e-commerce/3.png">



$\hat \theta_{i,j} = \frac{\#\:of\:times\:c_i\:purchase\:was\:followed\:by\:c_j}{count\:of\:c_i\:purchase} \tag{3.4}$

구매한 제품 $p$가 주어진 새로운 제품을 추천하기 위해 먼저 어떤 클러스터 $p$가 속하는지 식별합니다.(e.g. $p\in c_i$)

다음으로,$\theta_{i,j}$값에 의한 모든 클러스터 $c_j, j=1,...,C$를 평가합니다. 

마지막으로 최상위 클러스터의 제품은 $p$와의 코사인 유사도에 따라 정렬되며, 상위 K개의 제품을 추천으로 사용합니다.



### User-to-product predictive models

---

Product-to-product 예측 외에도 대부분의 추천 엔진은 user-to-product제품 예측도 가능합니다.

사용자에 대한 추천은 일반적으로 사용자의 온라인 행동, 소셜 연락처 등과 같은  다른 데이터 소스를 사용하여 유추된 과거 구매기록 혹은 관심사를 고려하여 이루어집니다.

이 섹션에서는 제품과 사용자의 벡터 표현을 동시에 배우는 새로운 접근 방식을 제안하여 공동 임베딩 공간에서 가장 가까운 K개의 제품을 찾아 추천할 수 있도록 합니다.

**user2vec.** user2vec모델은 paragraph2vec 모델에서 영감을 받은 "global context"로 유저를 고려하여 유저와 제품의 벡터 표현을 동시에 학습합니다.

모델의 아키텍처는 Fig.4에 설명되어 있습니다.

학습 데이터 세트는 유저 $u_n$과 구매 시점으로 정렬된 구매 기록으로 구성된 구매 시퀀스 $S$에서 파생됐습니다.

$u_n = (p_{n1},...,p_{nUn})$이며 $U_n$은 유저 $u_n$이 구매한 품목 수를 나타냅니다.

학습 과정에서 전자 메일 영수증에서 제품을 예측하도록 사용자 벡터가 업데이트 되는 반면 제품 벡터는 컨텍스트에서 다른 제품을 예측하는 방법을 학습합니다.

다음에서는 non-bagged 버전의 언어 모델을 제시하지만,제시된 방법을 확장하여 bagged 버전을 사용하는 것이 간단하다는 점에 유의하세요.

보다 구체적으로, user2vec의 목적은 모든 구매 시퀀스의 집합 $S$에 대한 log-likelihood를 최대화하는 것입니다.

user2vec 모델의 주요 장점 중 하나는 제품 추천이 구매 내역을 기반으로 해당 사용자에 맞게 특별히 조정되었다는 것입니다.

그러나 모델을 자주 업데이트해야 한다는 단점이 있습니다.

장기간 관련이 있을 수 있는 제품간의 추천과는 달리 가장 최근의 사용자 구매를 고려하여 사용자 간의 추천을 자주 변경해야 합니다.

> a 라는 제품을 구매한 경우 b라는 제품의 구매도 많았다.. 식의  기존에 쌓인 기록들을 바탕으로하는 item-based 추천의 경우와 다르게,
>
> 유저의 구매 시퀀스를 고려해야 하기 때문에 자주 업데이트해야 한다. 라고 해석함.

 

