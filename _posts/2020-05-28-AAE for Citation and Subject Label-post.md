---
layout: post
title: Multi-Modal Adversarial Autoencoders for Recommendations of Citation and Subject Labels
author: Jaeheon Kwon
categories: Papers
tags: [recommendation,Autoencoder]
---

# Multi-Modal Adversarial Autoencoders for Recommendations of Citations and Subject Labels



[Using Adversarial Autoencoders for Multi-Modal Automatic Playlist Continuation]( https://py-tonic.github.io/papers/2020/05/27/AAE-for-music-recommendation-post/ )의 선행 연구 논문입니다.

대략적인 내용만 정리되어있습니다.





## Abstract

<hr>

우리는 추천을 위한 Multi-Modal AAE를 제시하고 두 가지 다른 태스크(인용, 주제 레이블 추천)에서 평가합니다.

우리는 적대적 정규화, 희소성 및 다른 입력 양식의 효과에 대해 분석합니다.

408개의 실험을 수행함으로써, 적대적 정규화가 추천을 위해 AE의 기능을 지속적으로 개선함을 보여줍니다.

두 가지 태스크는 item co-occurrence가 인용의 경우 관련성(relatedness)과 유사하지만 주제 레이블의 경우 다양성을 의미한다는 점에서 item co-occurrence의 의미가 다릅니다.

우리의 결과는 item co-occurrence가 관련성과 유사할 때 입력으로 partial item set을 제공하는 것이 도움이 된다는 것을 보여줍니다.

따라서 새로운 추천 과제에 직면할 때 적절한 모델을 선택하기 위해 item co-occurrence의 의미를 고려하는 것이 중요합니다.

> Item co-occurrence?
>
> $Item \cdot Item^T$ 를 통해 얻어진 행렬, 이 행렬과 기존의 matrix(user-item)를 곱하면 간단한 recommendation이 가능함.



## Introduction

<hr>

이미지 분야에서 AE의 발전을 통해 적대적 정규화가 AE의 성능을 향상시킬 수 있음을 보여주었습니다.

AAE는 입력을 재구성할 뿐만 아니라 선택된 이전 분포와 코드를 일치시키도록 훈련시킵니다.

따라서 코드에 smoothness를 부여하면 AE가 추천을 위해 sparse item vectors를 재구성하는데 도움이 된다고 가정합니다.

> AE는 data가 어떤 point(latent vector)에 매핑된다.
>
> 또한 GAN은 discriminator를 통해  generator가 생성한  fake data가 real data의 분포를 따르도록 학습하게 된다.
>
> AAE는 이러한 GAN의 discriminator를 AE에 결합한 모양이고, encoder에서 출력되는 latent vector가 사용자가 정의한 분포를 따르도록 학습하기 때문에 smoothness라는 표현을 쓰지 않았을까..?

이론적 근거는 smoothness가explanatory factors of variation을 구분하는 좋은 표현의 기준중 하나이기 때문입니다.

> [Representation Learning: A Review and New Perspectives]( https://arxiv.org/pdf/1206.5538.pdf ) 여기에 나오는 개념중 하나인 듯.

본 논문에서는 AAE가 sparse recommendation task에 적용될 수 있는지를 분석합니다.

앞서 말했듯 인용과 주제 레이블 추천에 대하여 희소도 및 다른 입력 양식에 대한 적대적 정규화의 효과를 평가합니다.



**$Citation\ Recommendation$.** 

점점 더 많은 출판인이 인용 메타 데이터 공개적으로 제공하는 것을 목표로 Initiative for Open Citations에 기여하기로 결정합니다.

이를 통해 다음 시나리오를 추천 작업으로 고려하게 됩니다.

새로운 논문을 쓸 때, 저자는 각 연구 분야에서 핵심이거나 논문과 관련된 다른 출판물을 참조해야 합니다.

검토 과정에서 검토자가 부정적이라고 평가할 수 있습니다. 그러나 문헌의 양이 증가함에 따라 일부 중요한 논문조차 간과되는 경우가 있습니다.

따라서, 본 논문에서는 저자가 이미 다른 참고 문헌을 선택하고 논문이 거의 완성되었다고 가정할 때 인용 후보로 고려할 것을 권장하는 문제를 연구합니다. (논문의 제목 or 잠정 제목과 같은 정보를 사용할 수 있습니다.)

**$Subject\ Indexing$.**

인용 데이터 외에도 주제 레이블 또는 태그는 의학, 경제학과 같은 수많은 도메인에서 공개적으로 제공됩니다.

subject indexing은 과학 라이브러리에서 문서를 검색할 수 있게 하는 일반적인 작업입니다.

전문가가 새로운 문서에 일련의 주제로 주석을 달았습니다.

출판물의 메타 데이터만 사용하더라도 subject indexing에 대한 완전 자동화된 다중 레이블 분류 접근 방법은 유용합니다.

그러나 전문적인 subject indexer는 일반적으로 이러한 접근 방식의 결과를 추천사항으로만 사용하므로 인간 수준의 품질을 계속 보장할 수 있습니다.

이러한 상황은 이미 할당된 주제의 일부 목록을 명시적으로 고려하는 주제 레이블 추천 시스템을 구축하도록 동기를 부여합니다.





이 두 시나리오를 통합하기 위해 인용 또는 지정된 주제를 추천 태스크에 대한 암시적 피드백으로 사용합니다.

전자의 경우 credit assignment와 유사하다고 알려져있지만, 후자의 경우 주제 레이블은 전문가의 감독에 의해 논문과 관련성이 보장되도록 전문가가 선택합니다.

일반적으로 추천 문제는 유저 집합 $U$및 아이템 집합 $I$(matrix completion)을 가진 $U$x$I$행렬에서 missing ratings를 예측하는 것으로 모델링됩니다.

우리의 경우 연구 논문 자체를 저자나 신뢰할 수 있는 subject indexer를 통해 유저로 봅니다.

> user - paper
>
> item - citation, subject label, tag ..?

이론적 근거는 한 저자가 다른 도메인의 여러 논문에 참여할 수 있지만 특정 논문의 모든 저자는 동일한 추천 사항을 받아야 한다는 것입니다.

유사하게 주어진 논문은 주석을 달고있는 현재 subject indexer와는 별개로 후보 주제에 대한 동일한 추천을 받아야 합니다.

AAE에 대한 새로운 해석을 개발하여 추천 태스크에 적용하는 방법과 여러 입력 양식을 통합하는 방법을 보여줍니다.

우리는 평가 뿐만 아니라 추가 메타 데이터, 즉 문서 제목을 컨텐츠 기반 기능으로 고려하여 이 실험에서 이 기능을 사용합니다.

또한 이 모델이 데이터 세트의 희소성에 어느 정도까지 영향을 미치는지 평가합니다.

인용 또는 연구 논문 추천을 수행할 때 이미 인용된 논문만 추가하고 인용 빈도가 낮은 논문은 무시하는 것이 바람직하지 않습니다.

일반적인 pruning 전략은 거의 인용되지 않은 문서 및 다른 저작물이 너무 적은 인용 문서를 제거하는 것으로 구성됩니다.

이 pruning 스텝은 고려되는 아이템 수와 희소성 정도에 영향을 줍니다.

pruning 임계 값이 모델의 성능에 어떤 영향을 미치는지 더 잘 이해하기 위해 pruning 임계 값이 제어 변수인 실험을 수행합니다.

우리의 결과는 아이템의 일부 목록이 주제 추천 작업보다 인용 추천 작업에 중요하다는 것을 보여줍니다.

이것은 아이템 공동 발생의 의미론을 검사하면 연구자 또는 실무자가 새로운 추천 작업을 처리하는 데 도움이되고, 특히 아이템의 일부 목록을 입력으로 제공할지 여부를 결정하는데 도움이 될 수 있기 때문에 흥미롭습니다.

인용 추천을 위해, item co-occurrence와 내재된 관련성 즉, 지금까지 다른 저작물이 인용한 것은 관련성이 높습니다.

반대로 주제 레이블의 경우 co-occurrence는 다양성을 의미합니다.(단일 문서의 주석을 위해 유사한 주제를 함께 사용하는 경우는 거의 없습니다.)

그러므로 타이틀은 이미 할당 된 주제보다 관련성이 높습니다.

파라미터 수의 차이에도 불구하고 모든 평가된 방법은 데이터 희소성에 민감한 반응을 보였습니다.

타이틀의 사용으로 인해, AAE는 기준에 대해 경쟁적인 성능을 제공합니다.

subject label에서는 기준을 능가하는 성능을 보여줍니다.

AAE의 개별 구성 요소를 자세히 살펴보면 유일한 MLP 디코더가 subject labeling 작업에서 전체 모델보다 우수한 성능을 달성한 반면, 인용 추천 태스크에서 전체 모델보다 성능이 떨어집니다.

 

## Related Work

<hr>

인용 추천 사항에 대해서는 구체적으로 참고 문헌의 일부 목록을 기반으로 한 추천 사항과, 내용을 기반으로한 추천 사항으로 구분 가능합니다.

전자는 글을 쓰는 동안 주어진 statement에 대해 일치하는 인용을 찾는데 적합하지만 후자는 더 넓은 문서 레벨에서 누락된 인용을 식별하려고 노력합니다.

우리는 item co-occurrence에 기초할 뿐만 아니라 이러한 부분적인 목록을 완성 문제에 대한 보충 메타 데이터를 고려한 새로운 방법의 필요성을 인식했습니다.

subject label 추천 사항은 태그 추천 사항과 유사합니다.

두 경우 모두 목표는 일부 컨텐츠에 대한 설명적 레이블을 제안하는 것입니다.

대부분의 선행 연구들은 소셜 미디어 태그에 중점을 두며 우리는 과학 문서의 표준화된 동의어 사전에서 주제 레이블을 고려합니다.



## Problem Statement

<hr>

문서는 일반적인 추천 시나리오에서 유저로 간주될 수 있지만 아이템은 각각 인용된 문서 또는 제목 레이블 입니다.

$m$개의 문서 집합 $D$와 $n$개의 아이템 집합 $I$가 주어질 때 일반적으로 추천 태스크는 spanned space $D$x$I$를 모델링 하는 것입니다.

우리는 레이팅을 sparse matrix $X \in \{0,1\}^{m \times n}$으로 모델링합니다.

$X_{jk}$는 문서 $j$ 에서 아이템 $k$의 암시적 피드백을 나타냅니다.

실제 시나리오를 시뮬레이션 하기위해 문서 $D$를 학습및 평가를 위한 $D_{train}$과 $D_{test}$로 나눕니다. 

보다 정확하게는 출판 연도를 기준으로 학습 및 테스트 문서로 분할 합니다.

특정 연도 전에 출판된 문서는 학습용으로 사용되고 나머지 문서는 테스트 데이터로 사용됩니다.

이것은 실제 응용 프로그램에 가까운 실험 설정으로 이어집니다.

모든 모델에는 훈련을 위한 추가 정보 $S_{train} = D_{train} ▷◁  S$와 함께 $X_{train} = D_{train} ▷◁  X$의 전체 레이팅이 제공됩니다.

본 연구에서는 문서 제목을 추가 정보로 사용합니다.

테스트 세트인 $X_{test},S_{test}$ 또한 유사하게 얻어집니다.

평가를 위해 각 행에서 0이 아닌 entry 하나를 0으로 설정하여 $X_{test}$에서 랜덤하게선택된 아이템을 제거합니다.

우리는 생성된 테스트 집합을 $\tilde X_{test}$로 표기합니다.

$\tilde X_{test}$와 $S_{test}$가 주어질 때 예측 값은 $X_{pred} \in [0,1]^{m_{test} \times n}$ 입니다.

마지막으로 ranking metric을 통해 예상 점수 $X_{pred}$와 실제 점수 $X_{test}$를 비교합니다.

목표는 $\tilde X_{test}$에서 생략된 아이템이 $X_{pred}$에서 높은 순위를 얻는 것입니다.

두 가지 시나리오에서 (i.e. 인용 추천 사항 및 제목 레이블 추천 사항은) 문서와 아이템을 bipartite 그래프로 간주합니다.

인용을 고려하면 과학 문서는 일반적으로 인용 논문과 인용된 논문 모두이므로 이 관점은 반 직관적일 수 있습니다.

그럼에도 불구하고 일반적인 인용 네트워크의 데이터 세트가 너무 커서 인용된 모든 논문에 대한 메타 데이터를 가질 수는 없습니다.

예를 들어, 실험에 사용하는 PubMed 인용 데이터 세트는 2,896,764개의 다른 문서를 인용하는 224,092개 문서 메타 데이터를 제공합니다.

따라서 인용 문헌 자체의 메타 데이터에만 추천 사항을 기반으로 하는 것이 합리적입니다.



## Reference

<hr>

[paper]( https://arxiv.org/pdf/1907.12366.pdf )

