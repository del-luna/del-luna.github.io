---
layout: post
title: Gaussian Process in Machine Learning
author: Jaeheon Kwon
categories: Papers
tags: [bayesian]
---



# Gaussian Processes in Machine Learning



### Abstract

- stochastic process를 이해하고, 어떻게 사용되는지에 초점을 맞춤.

- 공분산 함수에서 하이퍼 파라미터의 역할, marginal likelihood에 대해 설명함.

- GP의 실질적인 이점을 설명하고 GP로 작업하는 트렌드에 대해 살펴봄.



parametric model은 해석 가능하다는 장점이 있지만 복잡한 데이터의 경우 간단한 모델은 표현력이 부족할 수 있으며 더 복잡한 모델(Neural Net)은 실제로 작업하기가 쉽지 않다는 문제가 존재함.

> *parameteric model : 학습 중에 학습 데이터의 정보를 parameter로 'absorbs' 하는 모델, 학습 후 데이터를 버릴 수 있음(k-means 같은 건 학습 데이터가 계속 쓰임.)

하지만, SVM 및 GP와 같은 커널 시스템은 실용적이면서 유연한 모델임을 강조함.



### Gaussian Process

$Definition\ 1.$ 

A Gaussian Process is a collection of Random Variable, any finite number of which have (consistent) joint Gaussian distribution

GP(Gaussian Process)는 mean function $m(x)$와 covariance function $k(x,x')$로 정의된다.

가우시안 분포 는 mean vector와 covariance matrix로 정의되지만, GP는 함수로 정의된다. 우리는 GP를 다음과 같이 나타냄.

$$f \sim GP(m, k)$$

$k = acf= positive\ semi\ definites$

$k$가 covariance term 이기 때문..



GP는 Random Process의 한 종류인데,

RP는 랜덤 변수에 시간을 추가한 것이라고 생각하면 직관적이다.

기존의 랜덤 변수가 RP에서는 시간에 따라 변하니까(time = index set) 아래처럼 시계열 모양의 그림이라고 생각해도 좋다.

기존의 랜덤 변수가 sample space에서 Real line으로의 매핑이라면,

랜덤 프로세스는 sample space에서 함수공간으로의 매핑이다.



결국 우리가 얘기하고 싶은 것은, $f\sim N(\mu,\Sigma)$를 통해서 랜덤 벡터를 생성 가능하고, 위 그림에서 회색으로 색칠된 밴드가 $\mu$에다가 $\Sigma$를 통해 계산된 노이즈를 더함으로서 구할 수 있다는 것이다.



그래서 이 GP를 prior로 놓고 Bayesian Inference를 진행할 것이다.

> 여기서 의문.
>
> GP를 prior로 둔다는 것의 의미? 장점?
>
> 장점은 개인적으로 생각했을 때... Likelihood가 gaussian이면 conjugate prior라서 posterior 계산 할 때 편해진다..? 그리고 mean, var만 알면 되는 간편한 가우시안의 특성 그 자체가 장점이다..?
>
> 그런데 GP를 prior로 둔다는 의미를 잘 모르겠다. 보통 이런 태스크에서 Prior라고 하면 떠오르는 것이 mean 주변의 굉장히 뾰족한 분포를 둬서 가중치를 0주변 값으로 제한하는, 어떻게 보면 규제 항의 추가처럼 느낄 수 있는데 GP를 prior로 두면 어떤 의미로 해석이 가능할까..?



### Posterior Gaussian Process

$$\begin{bmatrix}f \\ f^*\end{bmatrix} \sim N(\begin{bmatrix}\mu \\ \mu_* \end{bmatrix},\begin{bmatrix} \Sigma \ \Sigma_* \\\Sigma_*^T \Sigma_{**} \end{bmatrix})$$



$f : train\ function$

$f^* : test\ function$



joint Gaussian distribution

$$f^* \vert f \sim N(\mu_* + \Sigma^T_*\Sigma^{-1}(f-\mu), \Sigma_{**} - \Sigma_*^T\Sigma^{-1}\Sigma_*) $$



$f\vert D \sim GP(m_D, k_D),$

$m_D(x) = m(x) + \Sigma(X,x)^T\Sigma^{-1}(f-m)\\ k_D(x,x') = k(x,x') - \Sigma(X,x)^T\Sigma^{-1}\Sigma(X,x')$

> $\Sigma(X,x) :$ 모든 트레이닝 케이스 사이의 공분산 벡터



Posterior variance $k_D(x,x')$는 기존의 covariance(prior variance) - 양수 이므로 항상 prior variance보다 작은 것을 확인할 수 있다.(데이터가 몇 가지 정보를 줬기 때문에..?)



이제 마지막으로 noise term을 고려해서 식을 적으면 다음과 같다.(noise도 가우시안으로 설정)

$y(x) = f(x) + \epsilon , \epsilon\sim N(0, \sigma_n^2),\\ f\sim GP(m,k), y\sim GP(m,k+\sigma_n^2\delta_{ii'})$



이렇게 되면 Posterior에서 샘플링을 진행할 수 있다. Mean function과 convariance function을 정의했으니 학습 데이터가 주어질 때 prior를 posterior로 업데이트 할 수 있다.

이제 다음과 같은 문제를 해결해야한다.

mean, covariance funcion을 어떻게 설정하는지?

noise level은 어떻게 추정하는지?



### Training a Gaussian Process

일반적으로 ML에서는 prior에 대해 정보가 너무 없다.

즉, 평균, 공분산 함수를 정의하기에는 정보가 부족해서 어렵다는 뜻이다.

가진건 데이터밖에 없으니 데이터를 통해 둘에 대해 추론해보자.

Hierarchical Prior를 사용해서 추론한다.(하이퍼 파라미터에 의해 파라미터화 되는 평균, 공분산 함수)

$f\sim GP(m,k)$

$m(x) = ax^2 + bx + c$

$k(x,x') = \sigma_y^2 exp(-\frac{(x-x')^2}{2l^2}) + \sigma_n^2\delta_{ii'}$

$\theta = [a,b,c,\sigma_y,\sigma_n,l] $ 6가지 파라미터 셋을 설정했는데, 이를 통해 prior 정보를 구체화 할 수 있다고 한다.



Log Marginal Likelihood를 구해보자.

$$L=log\ p(\textbf{y}\vert \textbf{x},\theta) = -\frac12 log\vert\Sigma\vert -\frac12(\textbf y - \mu)^T\Sigma^{-1}(\textbf y -\mu)-\frac n2log(2\pi)$$

각각의 항은 Complexity Penalty(복잡도 측정 및 패널티), Negative Quadratic(데이터 피팅), Log-Normalization(별로 안중요함) 로 구성된다.

이제 위 함수를 $\theta_m, \theta_k$에 대해 편미분 해서 최적화 해보자.

$$\frac {\partial L}{ \partial\theta_m} = -(\textbf y - \mu)^T\Sigma^{-1}\frac{\partial m}{\partial \theta_m}$$

$$\frac{\partial L}{\partial \theta_k} = \frac12 trace(\Sigma^{-1}\frac{\partial\Sigma}{\partial\theta_k} + \frac12(\textbf y-\mu)^T \frac{\partial\Sigma}{\partial \theta_k}\Sigma^{-1}\frac{\partial\Sigma}{\partial\theta_k}(\textbf y - \mu))$$

 



[paper link](https://www.cs.ubc.ca/~hutter/EARG.shtml/earg/papers05/rasmussen_gps_in_ml.pdf)



