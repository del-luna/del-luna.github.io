---

---



## Contribution

- 제안한 방법이 Spectral graph convolution의 1차 근사임을 보임.
- 제안한 방법이 빠르고 확장 가능한 Semi-supervised에서 어떻게 사용될 수 있는지를 보임.



선행 연구에서는 $\mathcal L=\mathcal{L}_0 + \mathcal{L}_{reg}$ 과 같은 식의 로스를 사용하는데 뒤에 있는 term인 $\mathcal{L}_{reg}$같은 경우 Graph Laplacian Regulatizaion이다. 이는 연결된 노드가 비슷한 representation을 가지도록 하는 정규화 텀으로 연결된 노드가 도일한 레이블을 공유할 가능성이 있다는 가정에 의존한다. 
이로 인해 모델의 표현력이 단순한 유사도 이외에는 제한되기 때문에 본 논문에서는 인접행렬을 인풋으로 하여 노드 연결에 대한 정보의 제한 문제를 해결함.



$$H^{(l+1)} = \sigma(\tilde D^{-\frac12}\tilde A \tilde D^{-\frac12}H^{(l)}W{(l)})$$

- $H^{(l)}$ : $l$번째 레이어의 히든 스테이트, $H^0 = X$ 는 Graph node init feature
- $\tilde A$ : $A+I_N$ 
- $\tilde D$ : $\tilde A$의 degree를 대각 성분으로 가지는 대각행렬



$\tilde D^{-\frac12}\tilde A \tilde D^{-\frac12}$ 는 각 노드에 연결된 edge수에 따른 인접행렬의 정규화라고 볼 수 있다.(엣지에 관계없이 모든 노드를 학습 잘 하기위한 과정이라고 보자.)

결국 이 normalized 행렬을 가중치로하고 feature vector(init인 경우) 혹은 이전 레이어의 hidden state인 $H$ 를 곱해주면

feature들의 weighted sum 형태의 행렬이 되고 여기다가 학습 가능한 파라미터 $W$를 곱해서 학습을 진행하게 된다.

결국 노드 representation으로 노드에 연결된 노드들을 가중합하는 방법이라고 볼 수 있다.

이러한 과정이 레이어가  하나일 때는 각 노드들의 인접 노드만 이용하여 H를 계산하지만, 이 레이어를 K개 쌓게 되면 K개 떨어져 있는 (K개의 edge를 가지는) 노드까지 이용한 H를 계산할 수 있다고 한다.

> 이 부분의 디테일한 과정은 [여기](https://baekyeongmin.github.io/paper-review/gcn-review/)를 참조하자.



## Spectral Graph Convolutions

spectral graph convolution은 푸리에 도메인에서 다음과 같이 정의된다.

$$g_\theta \star x = Ug_\theta U^Tx$$

- $x\in R^N$ : node
- $g_\theta$ : filter, $diag(\theta)$ parametrized by $\theta \in R^N$
- $U$ : normalized laplacian L의 eigen decomposition 후 고유 벡터로 이루어진 행렬 즉, $L = U\Lambda U^T$

$g_\theta$를 L에대한 고윳값의 함수로 이해할 수 있다.

> 솔직히 이 부분에 대한 이해가 부족한데, [여기](https://tootouch.github.io/research/spectral_gcn/#graph-convolution%EC%97%90%EC%84%9C-gcn%EC%9C%BC%EB%A1%9C%EC%9D%98-%EA%B3%BC%EC%A0%95)를 참고해서 조금 적어보자면
>
> 우선 그래프에 적용한 푸리에 변환은 $\mathcal F (x) = U^Tx = \hat x$ 로 정의된다고 한다.
>
> 반대로 역변환은 $\mathcal F^{-1}(\hat x) = U\hat x$ 이다.
>
> 이제 컨볼루션의 정의에 의해 식을 다음과 같이 적을 수 있다.
>
> $x\star g = \mathcal F^{-1}(\mathcal F(x)\bigodot \mathcal F(g)) = U(U^Tx \bigodot U^Tg)$
>
> 위 식에서 g는 고윳값에 대한 함수로 정의했으니 다음과 같이 작성할 수 있다.
>
> $g_\theta \star x = Ug_\theta U^Tx$

Spectral based의 장점은 eigen decomposition을 통해 많은 정보를 가질 수 있다는 점이지만 반대로 단점은 비싼 연산이라는 것이다. $O(N^2)$



필터에 파라미터를 사용하는데, 이를 통해 여러 hop의 노드 정보들도 가져올 수 있다.

즉 기존의 이웃 노드들만 고려하는 것이 아니라 2hop이 되면 이웃의 이웃 노드들 까지 보겠다는 의미가 된다.

따라서 필터를 다음과 같이 작성할 수 있다.

$$g_\theta(\Lambda) = \sum\limits_{k=0}^K \theta_k\Lambda^k$$

따라서 위 식에 대입하면 다음과 같다.

$$x\star g_\theta =  U\sum\limits_{k=0}^K \theta_k \Lambda^k U^T x \\ \ \ \ \ \ \ \ \ \ \ \ = \sum\limits_{k=0}^K \theta_kL^kx$$

> k=1인 경우 기존의 Lx 수식이 된다.
>
> Lx의 의미는 자신과 이웃 노드간의 차이의 summation이다.



연산이 비싸다는 문제를 해결하기위해 $g_\theta$ Chbyshev polynomials로 근사할 수 있다.

$$g_{\theta'}(\Lambda) \approx \sum\limits_{k=0}^K \theta'_kT_k(\tilde \Lambda)$$

이를 통해 $O(\epsilon)$ 으로 계산 복잡도를 낮췄다.