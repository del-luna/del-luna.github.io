---
layout: post
title: What is Ensemble?
date: 2019-09-24 00:00:00
author: Jaeheon Kwon
categories: Ai
tags: [ensemble]
---



# What is Ensemble & Stacking

무작위로 선택된 수 많은 사람들에게 복잡한 질문을 하고 대답을 모은다고 가정합시다. 많은 경우 이렇게 모은 답이 전문가의 답보다 낫습니다. 이를 대중의 지혜(wisdom of crowd) 라고 합니다. 이와 비슷하게 일련의 예측기(분류 or 회귀 모델)로 부터 예측을 수집하면 가장 좋은 모델 하나보다 더 나은 결과를 얻을 수 있을 것입니다.

## 투표기반 분류기

정확도가 80%인 분류기 여러개를 훈련시켰다고 가정합시다. 더 좋은 분류기를 만드는 간단한 방법은 각 분류기의 예측을 모아서 가장 많이 선택된 클래스를 예측하는 것입니다. 이렇게 다수결 투표로 정해지는 분류기를 Hard voting 라고 합니다  

만약 모든 분류기가 클래스의 확률을 예측할 수 있으면 개별 분류기의 예측을 평균 내어 확률이 가장 높은 클래스를 예측할 수 있습니다. 이를 Soft voting 라고 합니다.  

투표기반 분류기에서 중요한 가정은 분류기가 모두 독립이고 오차에 상관관계가 없어야 한다는 점입니다.  

같은 데이터로 훈련시키게 되면 분류기가 같은 종류의 오차를 만들기 쉽기 때문에 잘못된 클래스가 다수인 경우가 많고 앙상블의 정확도가 낮아집니다.  

## Bagging & Pasting

앞서 말햇듯이 다양한 분류기를 만드는 한 가지 방법은 각기다른 훈련 알고리즘을 사용하는 것입니다.  

또 다른 방법은 같은 알고리즘을 사용하지만 training set의 서브셋을 무작위로 구성하여 분류기를 각기 다르게 학습 시키는 것입니다. (보통 이 서브 셋을 Bootstrap Sample 이라고 합니다.)



몇 가지 가정 하에서, 이 표본들은 꽤 좋은 통계적 특성을 가집니다.

첫 번째 근사에서는 실제 기초(그리고 종종 알려지지 않은) 데이터 분포에서 직접 그리고 서로 독립적으로 도출되는 것으로 볼 수 있습니다.

따라서 실제 데이터 분포의 대표적이고 독립적인 샘플(거의 iid)로 간주될 수 있습니다.

문제는 이러려면 데이터 세트의 크기가 기본 분포의 복잡성을 대부분 포착할 수 있을 정도로 매우 커야하고, 부트 스트랩 샘플 크기에 비해도 충분히 커야 서로 독립이 보장됩니다.



training set에서 중복을 허용하여 샘플링하는 방식을 bagging이라 하며, 중복을 허용하지 않고 샘플링하는 방식을 pasting이라고 부릅니다.  

bagging과 pasting 모두 같은 training sample을 여러개의 예측기에 걸쳐 사용할 수 있지만 bagging만이 한 예측기를 위해 같은 training sample을 여러 번 샘플링할 수 있습니다.  

동작 방식을 보면 모든 예측기가 훈련을 마치면 앙상블은 모든 예측기의 예측을 모아서 새로운 샘플에 대한 예측을 만듭니다.  

수집 함수는 분류일 때는 통계적 최빈값, 회귀에 대해서는 평균을 계산합니다.  

개별 예측기는 원본으로 훈련시킨 것 보다 훨씬 크게 편향되어 있지만 수집 함수를 통과하면 편향과 분산이 모두 감소합니다.  

일반적으로 앙상블은 원본 데이터셋으로 하나의 예측기를 훈련시킬 때와 비교해 편향은 비슷하지만 분산은 줄어듭니다.  

배깅은 각 예측기가 학습하는 서브셋에 다양성을 증가시키므로 배깅이 페이스팅보다 편향이 조금 더 높습니다. 하지만 이는 예측기들의 상관관계를 줄이므로 앙상블의 분산을 감소시킵니다.  

## Boosting

부스팅은 약한 학습기를 여러개 연결하여 강한 학습기를 만드는 앙상블 방법을 뜻합니다.  

부스팅 방법의 아이디어는 앞의 모델을 보완해나가면서 일련의 예측기를 학습시키는 것입니다.  

### 1. AdaBoost

이전의 모델을 보완하는 새로운 모델을 만드는 방법은 이전 모델이 과소적합했던 training sample의 가중치를 더 높이는 것입니다. 이렇게 하면 새로운 예측기는 학습하기 어려운 샘플에 점점 더 맞춰지게 됩니다.  

### 2. Gradient Boosting

그래디언트 부스팅은 아다부스트처럼 앙상블에 이전까지의 오차를 보정하도록 예측기를 순차적으로 추가합니다. 하지만 아다부스트 처럼 반복마다 샘플의 가중치를 수정하는 대신 이전 예측기가 만든 잔여 오차에 새로운 예측기를 학습시킵니다.  

그리고는 모든 모델의 예측을 더해서(앙상블)새로운 샘플에 대한 예측을 합니다.  

## Stacking

스태킹은 앙상블에 속한 모든 예측기의 예측을 취합하는 간단한 함수(ex: hard voting)를 사용하는 대신 취합하는 모델을 훈련시킬 수 없을까? 라는 기본 아이디어에서 출발합니다.  

각각의 예측기로 예측한 값을 마지막 예측기(blender)가 이 예측을 입력으로 받아 최종예측을 만듭니다.  

블렌더를 학습시키는 일반적인 방법은 hold-out set을 사용하는 것입니다. 어떻게 작동하는지 살펴보겠습니다.  

1. Training set을 두개의 sub set으로 분리한다.
2. 첫 번째 subset을 가지고 예측기를 학습시킨다
3. 두 번째 subset을 가지고 예측기를 test 한다.
4.  3번을 input으로하고 정확한 값을 output으로 해서 더 높은 layer의 예측기로 학습시킨다.

스태킹은 일반적으로 조합하는 first layer의 모델들이 서로 매우 다를 때 가장 큰 효과를 발휘합니다.  

first layer의 모델의 결과물을 고루 섞어 특정 모델의 강점을 부각하고 특정 모델의 약점을 보완하는 Stacking 모델(second layer)은 대부분 단일 레이어의 모델 보다 더 좋은 성능을 보입니다.

