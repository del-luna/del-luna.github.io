---
layout: post
title: RL introduction
author: Jaeheon Kwon
categories: Ai
tags: [Reinforcement Learning]
---



## What is Reinforcement Learning?

---

- Supervisorê°€ ì—†ë‹¤. ë‹¨ìˆœíˆ rewardë¥¼ maximizeí•˜ëŠ”ê²Œ ëª©ì . ì–´ë–¤ í–‰ë™ì„ í•´ì•¼ rewardê°€ ë†’ì•„ì§€ëŠ”ì§€ ì•Œë ¤ì£¼ì§€ ì•Šê¸° ë•Œë¬¸ì— ëª¨ë¸ì´ ê³„ì† ì‹œë„í•˜ë©´ì„œ í•™ìŠµí•¨. ì´ë¡œ ì¸í•´ ì‚¬ëŒì´ ì°¾ì§€ ëª»í•œ Global Optimalì— ë„ë‹¬í•  ìˆ˜ ìˆìŒ
- Feed backì´ ì§€ì—°ë  ìˆ˜ ìˆìŒ. ì¦‰, ì–´ë–¤ ì•¡ì…˜ì´ ë¦¬ì›Œë“œë¥¼ ë†’ì´ê²Œ ì•¼ê¸°í•œê±´ì§€ ì•Œê¸° ì–´ë ¤ì›€. ì˜ˆë¥¼ ë“¤ì–´ ì•„ì´ìŠ¤í¬ë¦¼ì„ 10ê°œë‚˜ ë¨¹ì–´ë„ ê´œì°®ë‹¤ê°€ 1ì‹œê°„ í›„ì— ë°°ê°€ ì•„í”ˆ ê²ƒ? ì²˜ëŸ¼ ë¦¬ì›Œë“œê°€ ìˆê¸° ì „ê¹Œì§€ ëª¨ë“  í–‰ë™ë“¤ì´ ê³ ë ¤ ëŒ€ìƒì´ê¸° ë•Œë¬¸ì—(1ì‹œê°„ ë™ì•ˆ í–‰ë™í•œ ê²ƒì´ ë°°ê°€ ì•„í”ˆ ì›ì¸ì¼ ìˆ˜ë„, ì•„ì´ìŠ¤í¬ë¦¼ì„ ë¨¹ì€ ê²ƒì´ ì›ì¸ì¼ì§€  í™•ì‹¤í•˜ì§€ ì•ŠìŒ) ì›ì¸ì„ ëª…í™•íˆ ì°¾ê¸° ì–´ë ¤ì›€
- Time really matters(Sequential, non iid) ìˆœì„œê°€ ì¤‘ìš”í•¨, ì• ì´ˆì— 'ìˆœì°¨ì  ì„ íƒì„ ì˜ í•˜ë„ë¡ í•™ìŠµí•˜ëŠ” ê²ƒ' ì´ ëª©í‘œì´ê¸°ë„ í•˜ê³ , ê¸°ì¡´ì˜ ML/DLì—ì„œ ë°ì´í„°ê°€ iidë¼ê³  ê°€ì •í•˜ì§€ë§Œ ê°•í™”í•™ìŠµì€ ì•„ë‹˜.
- Agentì˜ actionì´ ì´í›„ ë°ì´í„°ì— ì˜í–¥ì„ ì¤€ë‹¤.



## What is Reward?

**Reward**: Scalar feedback signal

ë¦¬ì›Œë“œë¥¼ ë²¡í„°ë¡œ ë†“ê³ ì‹¶ê±°ë‚˜ ë²¡í„°ì¸ ê²½ìš° ê°•í™”í•™ìŠµì„ ì ìš©í•˜ê¸° ì–´ë µë‹¤ê³ í•©ë‹ˆë‹¤. ê° ë²¡í„°ì˜ ë””ë©˜ì…˜ ë³„ë¡œ ê°€ì¤‘ì¹˜ë¥¼ ì ìš©í•´ í•©ìœ¼ë¡œ ë†“ìœ¼ë©´ ìŠ¤ì¹¼ë¼ë‹ˆê¹Œ ì´ë ‡ê²Œ ì ‘ê·¼ì€ ê°€ëŠ¥í•˜ë‹¤ê³  í•©ë‹ˆë‹¤.

ì—ì´ì „íŠ¸ì˜ ëª©í‘œëŠ” comulative rewardë¥¼ ìµœëŒ€í™” í•˜ëŠ” ê²ƒ!

ë¦¬ì›Œë“œë¥¼ ê³„ì† greedyí•˜ê²Œ íƒìƒ‰í•˜ëŠ” ê²ƒì´ ì•„ë‹ˆë¼ long-termì„ ì˜ ë´ì•¼í•©ë‹ˆë‹¤.

> e.g financial investment, ì²´ìŠ¤ì—ì„œ í° ëª‡ë§ˆë¦¬ ì£¼ê³  í€¸ ì¡ê¸°



## Terminology

- Agent : í•™ìŠµì‹œí‚¬ ëŒ€ìƒ(e.g. ëª¨ë¸, ë¡œë´‡)
- Action : Agentê°€ ì·¨í•˜ëŠ” í–‰ë™ (time stepë§ˆë‹¤ ìˆ˜í–‰)
- Environment : ì™¸ë¶€ í™˜ê²½
- Observation : Action ë•Œë¬¸ì— ë°”ë€ ìƒí™©ì— ëŒ€í•œ ì •ë³´
- Reward : Actionë§ˆë‹¤ ì–»ëŠ” ë³´ìƒ
- History : Observation, Action, rewardsì˜ Sequence $h_t=\{a_1,o_1,r_1,...,a_t,o_t,r_t\}$
- State : ë‹¤ìŒ ì•¡ì…˜ì„ ê²°ì •í•˜ê¸° ìœ„í•´ ì“°ì´ëŠ” ì •ë³´ (State is function of History $S_t=f(H_t)$)



> ì›¹ ê´‘ê³ ë¥¼ ìœ„í•´ ê°•í™”í•™ìŠµì„ ì‚¬ìš©í–ˆë‹¤ê³  í•©ì‹œë‹¤.
>
> AgentëŠ” ì›¹ì— íŠ¹ì • ì˜ì—­ì— ê´‘ê³ ë¥¼ ë„ìš°ê³ (Action) ê´‘ê³  í´ë¦­ ìˆ˜(Observation)ë¥¼ ë³´ê³  Rewardë¥¼ ì–»ìŠµë‹ˆë‹¤. ê·¸ë¦¬ê³  Rewardë¥¼ ë†’ì´ëŠ” ë°©í–¥ìœ¼ë¡œ ë‹¤ì‹œ ê´‘ê³ ë¥¼ ë„ìš°ê² ì£ ?



<img src = "https://py-tonic.github.io/images/rl/1.png">

Stateì— ëŒ€í•´ ì¢€ ìì„¸íˆ ì–˜ê¸°í•´ë´…ì‹œë‹¤.

Environment state($S_t^e$): Observation, rewardë¥¼ ë±‰ì–´ë‚´ê¸° ìœ„í•´ ì‚¬ìš©í•˜ëŠ” ì •ë³´ë“¤, ë³´í†µ Agentí•œí…Œ ë³´ì´ì§€ëŠ” ì•ŠìŠµë‹ˆë‹¤.

> ê²Œì„ê¸°ë¥¼ ì˜ˆë¡œ ë“¤ì–´ë´…ì‹œë‹¤.
>
> Environment : ê²Œì„ê¸°
>
> Action : ì¡°ì´ìŠ¤í‹±
>
> Observation : ê²Œì„í™”ë©´
>
> Agent : ìš°ë¦¬
>
> Reward : ì ìˆ˜
>
> ê·¸ë ‡ë‹¤ë©´ ì—¬ê¸°ì„œ $S_t^e$ëŠ” ê²Œì„ê¸°ê°€ ë¦¬ì›Œë“œì™€ ê²Œì„ í™”ë©´ì„ ê³„ì† ë³€ê²½í•´ì£¼ê¸° ìœ„í•´ ì‚¬ìš©ë˜ëŠ” ì •ë³´ë“¤ì„ ëœ»í•©ë‹ˆë‹¤.(ë‚´ ìœ„ì¹˜, ì—ì´ì „íŠ¸ì˜ ì•¡ì…˜, ëª¬ìŠ¤í„°ì˜ ì¡´ì¬ ìœ ë¬´ ë“±ë“± ê³„ì‚°ì„ ìœ„í•´ í•„ìš”í•œ ëª¨ë“  ì •ë³´ë“¤) ê·¸ë ‡ê¸° ë•Œë¬¸ì— ì—ì´ì „íŠ¸í•œí…Œ ë³´ì—¬ì§ˆ í•„ìš”ê°€ ì—†ìŠµë‹ˆë‹¤.



Agent state($S_t^a$): ë‚´ê°€ ë‹¤ìŒ ì•¡ì…˜ì„ ìœ„í•´ í•„ìš”í•œ ì •ë³´

> ì—ì´ì „íŠ¸ì— ë”°ë¼ ì´ ì •ë³´ëŠ” ë§ì„ ìˆ˜ë„, ì ì„ ìˆ˜ë„ ìˆìŠµë‹ˆë‹¤.
>
> íˆ¬ìì1ì€ ë°”ë¡œ ì–´ì œì˜ ì£¼ì‹ì„ ë³´ê³  ì˜¤ëŠ˜ ì£¼ì‹ì„ ì‚´ì§€ ë§ì§€ íŒë‹¨í•˜ì§€ë§Œ, íˆ¬ìì2ëŠ” ë‹¤ë¥¸ ë¹„ìŠ·í•œ ìƒí™©ì˜ ëª¨ë“  ì£¼ì‹ì‹œì¥ì„ ê³ ë ¤í•´ì„œ íŒë‹¨í•  ê²½ìš° íˆ¬ìì2ì˜ $S_t^a$ëŠ” ì—„ì²­ ë§ê² ì£ ? ê·¸ë¦¬ê³  ì´ëŠ” íˆìŠ¤í† ë¦¬ì˜ í•¨ìˆ˜ì¸ ê²ƒì€ ìëª…í•©ë‹ˆë‹¤. $S_t^a = f(H_t)$



### Example

| â™£ï¸    | â™£ï¸    | â™ ï¸    | â™¥ï¸    | ğŸ”¨    |
| ---- | ---- | ---- | ---- | ---- |
| â™¥ï¸    | â™£ï¸    | â™ ï¸    | â™ ï¸    | ğŸ«    |
| â™ ï¸    | â™£ï¸    | â™ ï¸    | â™¥ï¸    | â“    |

ìœ„ì˜ ì˜ˆì‹œë“¤ì€ ëª¨ë‘ íˆìŠ¤í† ë¦¬ë¡œ ë³¼ ìˆ˜ ìˆìŠµë‹ˆë‹¤.

StateëŠ” ìš°ë¦¬ê°€ ì •ì˜í•˜ê¸° ë‚˜ë¦„ì…ë‹ˆë‹¤. $H=\{h1,h2,h3\}$ê°€ ìˆì„ ë•Œ, ê° íˆìŠ¤í† ë¦¬ì˜ $h[1:]$ì„ Stateë¡œ ì •ì˜í•´ë´…ì‹œë‹¤.

- $s1$: {â™£ï¸, â™ ï¸, â™¥ï¸}
- $s2$: {â™£ï¸, â™ ï¸, â™ ï¸}
- $s3$: {â™£ï¸, â™ ï¸, â™¥ï¸} 

ì´ë ‡ê²Œ ë˜ë©´ $s1,s3$ê°€ ê°™ìœ¼ë¯€ë¡œ s3ì—ì„œ ìš°ë¦¬ëŠ” ê½ì„ì„ ì•Œ ìˆ˜ ìˆìŠµë‹ˆë‹¤.

í•˜ì§€ë§Œ ê°ê°ì˜ ë¬¸ì–‘ì´ ë‚˜ì˜¨ ê°œìˆ˜ë¡œ Stateë¥¼ ì •ì˜í•˜ë©´ ì–´ë–»ê²Œ ë ê¹Œìš”?

- $s1$:{â™£ï¸:2, â™ ï¸:1, â™¥ï¸:1}
- $s2$:{â™£ï¸:1, â™ ï¸:2, â™¥ï¸:1}
- $s3$:{â™£ï¸:1, â™ ï¸:2, â™¥ï¸:1}

ì´ë²ˆì—ëŠ” $s2,s3$ê°€ ê°™ìœ¼ë¯€ë¡œ ì´ˆì½œë¦¿ì„ ì–»ì„ ìˆ˜ ìˆìŠµë‹ˆë‹¤. ì´ì²˜ëŸ¼ Stateë¥¼ ì •ì˜í•˜ê¸°ì— ë”°ë¼ ì˜ˆì¸¡ ê°’ì€ ë‹¬ë¼ì§‘ë‹ˆë‹¤.



## Major Components of an RL Agent

- Policy : ì—ì´ì „íŠ¸ì˜ í–‰ë™ì„ ë‚˜íƒ€ëƒ…ë‹ˆë‹¤. state -> actionìœ¼ë¡œì˜ ë§¤í•‘
    - Derterministic policy: $ a = \pi(s)$
    - Stochastic policy: $\pi(a\vert s) = P[A_t=s\vert S_t=s]$ 
- Value : ë¯¸ë˜ì— ì–»ì„ ìˆ˜ ìˆëŠ” ì´ ë¦¬ì›Œë“œì˜ í•©ì„ ì˜ˆì¸¡í•´ì£¼ëŠ” í•¨ìˆ˜. stateê°€ ì¢‹ì€ì§€ ë‚˜ìœì§€ í‰ê°€í•´ì¤Œ.
    - $v_{\pi}=E_{\pi}[R_{t+1}+\gamma R_{t+2}+...\vert S_t=s]$
    - ì–´ë–¤ í˜„ì¬ Stateë¡œ ë¶€í„° policy $\pi$ë¥¼ ë”°ë¼ ê°”ì„ ë•Œ ì´ ì–»ì„ rewardì˜ ê¸°ëŒ“ê°’ ì¦‰, policyê°€ ì—†ìœ¼ë©´ Valuefunctionì´ ì •ì˜ë˜ì§€ ì•ŠìŒ.
    - ê¸°ëŒ“ê°’ì„ ì“°ëŠ”ì´ìœ ? Environmentì—ë„ í™•ë¥ ì ì¸ ìš”ì†Œê°€ ìˆê³ , Stochastic policyì¸ ê²½ìš°ì—” ë‹¹ì—°íˆ ì¨ì•¼ë˜ê³ !
- Model : í™˜ê²½ì´ ì–´ë–»ê²Œ ë ì§€ ì˜ˆì¸¡.
    - $P$ : Predict state
    - $R$ : Predict reward



## Categorizing RL Agents

- Value Based
    - Value Functionë§Œ ìˆëŠ” ê²ƒ.(ì œì¼ Rewardì˜ ê¸°ëŒ“ê°’ì´ ë†’ì€ í•¨ìˆ˜ìª½ìœ¼ë¡œ ê°€ë©´ë¨)
- Policy Based
    - Policy : Policyë§Œ ìˆì–´ë„ Agentê°€ í™œë™ ê°€ëŠ¥
- Actor Critic
    - Policy
    - Value Function

- Model Free
    - Policy and/or Value Function
- Model based
    - Policy and/or Value Function
    - Model

<img src = "https://py-tonic.github.io/images/rl/2.png">

## Reference

[íŒ¡ìš”ë©](https://www.youtube.com/channel/UCwkGvF7xKz2E0Lv-fZ9wv2g)

