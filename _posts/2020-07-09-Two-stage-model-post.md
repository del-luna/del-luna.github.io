---
layout: post
title: Two-stage Model for Automatic Playlist Continuation at Scale
author: Jaeheon Kwon
categories: Papers
tags: [recommendation]
---



# Two-stage Model for Automatic Playlist Continuation at Scale



## Abstract

<hr>

이 논문에서 우리는 Recsys2018 챌린지에 대한 접근법을 제시합니다.

첫 번째 단계는 빠른 검색을 위해 최적화 된 two-stage 모델을 사용하고, 두 번째 단계는 검색된 후보의 순위를 다시 매겨 추천 시스템의 최상위 목록의 정확도를 최대화합니다.

> 이 논문 저자들의 팀이 fisrt prize



## Introduction

<hr>

노래의 하위 집합이 보류되는 별도의 10K의 테스트 재생 목록 집합을 사용하여 모델을 평가합니다.

특히 테스트 재생 목록의 길이는 0곡(cold-start)에서 100곡까지 다양합니다.

이는 재생 목록 생성의 모든 단계에서 추천 모델이 제대로 작동해야하는 생산 시나리오를 시뮬레이션합니다.

우리의 접근 방식은 2단계 아키텍처를 기반으로 합니다.

첫 번째 단계는 큰 2.2M노래 검색 공간을 각 재생 목록에 대해 훨씬 더 작은 후보 집합으로 줄이는데 중점을 둡니다.

CF와 딥러닝 모델의 조합을 사용하여 90%가 넘는 리콜을 통해 각 재생목록에 대해 20K 후보를 검색할 수 있습니다. 또한 이 후보 집합의 상위 1K곡은 이미 60%에 가까운 리콜을 포함합니다.

리콜이 높으면 대부분 관련 곡이 검색된 집합에서 캡처되므로 후속 모델이 정확도를 크게 떨어 뜨리지 않고 이 집합에만 집중할 수 있습니다.

따라서 런타임에 미치는 영향을 최소화하면서 2단계에서 보다 정교한 모델을 적용할 수 있습니다.

두 번째 단계에서는 각(재생목록,노래) 쌍을 관련성 점수에 직접 매핑하는 pairwise 모델을 개발합니다.

재생 목록과 곡 기능을 입력에 통합하여 이 모델은 기존의 CF방법으로는 표현하기 어려운 pair-wise 관계를 캡처할 수 있습니다.

두 번째 단계의 목표는 추천 곡의 상단에서 정확도를 최대화 하는 후보 곡의 순위를 다시 매기는 것입니다.

다음 섹션에서는 데이터 분할, 교육 및 추론 절차 뿐만 아니라 두 단계 모두에 대해 자세히 설명합니다.



## Approach

<hr>

이 접근법의 모델 아키텍처는 Fig.1에 나와 있습니다.

첫 번째 단계에서 잠재 CF모델 WRMF(Weighted Regularized Matrix Factorization)를 사용하여 각 재생목록에 대해 20K후보 곡을 빠르게 검색합니다.

검색된 곡 마다 CNN, User-User, Item-Item 인접 모델의 임베딩을 사용하여 추가 모델 점수를 계산합니다.

> WRMF를 사용해서 플레이 리스트별 20K 후보곡 생성 -> 각 곡에대해 여러 모델을 사용하여 추가 모델 점수를 계산한다는데... 추가 모델 점수를 어떻게 계산..?
>
> For each retrieved song we compute additional model scores using embedding convolutional neural network (CNN) as well as User-User and Item-Item [13] neighborbased models. 

선형 가중 조합과 함께 모든 모델 점수는 추출된 재생 목록 노래 피처와 연결되어 두 번째 단계에 대한 입력으로 사용됩니다.

<img src = "https://py-tonic.github.io/images/2stage/1.png">

GB 모델은 모든 후보 곡의 순위를 다시 매기고 최종 순위를 출력합니다.

> 이 부분이 핵심인데 어떻게 동작..? 
>
> 우리 태스크에선 후보 곡들을 N개 생성하고 순위를 매겨서 100개만 추출하면 될듯

cold-start 외에도 길이에 관계없이 모든 재생 목록에 단일 2-단계 모델이 사용됩니다.

복잡성을 줄이고 훈련 시간을 단축하기 위해 의도적으로 수행됩니다.

이 섹션에서는 각 단계에 대해 자세히 설명하며 다음과 같은 표기법이 사용됩니다.

- **R** : 재생 목록-노래 matrix, $R_{ij}=1$이면 노래 $i$가 재생 목록 $j$에 들어있고 0이면 존재하지 않음. 
- **U, V** : 재생 목록, 노래 잠재 표현. $U_i,V_j$는 각각 재생 목록 $i$및 노래 $j$에 대한 잠재 표현을 나타냄.
- **S** : 재생 목록 $i$및 노래 $j$에 대한 예측된 관련성 점수를 $S_{ij}$로 나타냄.



### First Stage

<hr>

첫 번째 단계의 주요 목표는 충분히 높은 리콜로 후보 곡 집합을 신속하게 검색하는 것입니다.

잠재 CF모델은 효율적인 추론과 높은 정확도로 잘 수행되는 것으로 나타났습니다.

경험적으로, 우리는 WRMF가 최상의 성능을 보여줌을 발견하고 초기 검색을 위해 이를 사용합니다.

그러나, WRMF는 재생 목록 연속에 중요한 것으로 표시된 재생 목록 내의 노래 순서를 무시합니다.

시간 정보를 통합하기 위해 노래 임베딩 보다 CNN을 사용하여 순서에 따른 재생 목록 임베딩을 생성하는 CNN 기반 잠재 모델을 개발합니다.

또한 잠재 모델은 전체적인 패턴에 초점을 맞추는 것으로 나타 났으며, 소량의 아이템들 사이에서 강한 국소화된 상관 관계를 탐지하는데 열악한 경향이 있습니다.

따라서 잠재 모델이 놓친 패턴을 포착하기 위해 첫 번째 단계에서 인접 기반 CF 모델을 추가로 통합합니다.

전체 검색을 여러번 반복하지 않으려면 WRMF에서 검색한 후보곡에만 이 모델을 적용하여 계산 오버헤드를 크게 줄입니다.

아래에서는 각 모델에 대해 자세히 설명합니다.

**WRMF** 는 binary/implicit CF에서 가장 많이 사용되는 잠재 모델 중 하나입니다.

10년 전에 출판 된 사실에도 불구하고, 우리는 충분한 튜닝으로 여전히 높은 경쟁력을 달성할 수 있음을 발견했습니다.

WRMF는 다음 목표를 최적화 하기 위해 반복적으로 최소-제곱을 적용합니다.

$argmin \sum\limits_{i,j}c_{ij}(R_{i,j}-U_iV_j)^2 + \lambda_U \vert\vert U_i\vert\vert^2 + \lambda_V\vert\vert V_j\vert\vert^2 \tag{1}$

$c_{ij} = 1+\alpha R_{i,j}$입니다. $c_{ij}$의 $\alpha$는 관측 된 재생 목록-노래 쌍에 얼마나 많은 가중치가 부여 되는지를 제어합니다. 실험 결과 $\alpha = 100, \lambda_U = \lambda_V = 0.001$을 사용하면 성능이 우수하다는 것을 알았습니다.

> implicit feedback에 대한 CF논문에서 사용하는 계수 c와 같이 confidence를 나타내는 것 같은데 ..?

**CNN** CF에서 시각적 패턴을 모델링하는 기존의 연구는 주로 RNN에 중점을 두었습니다.

그러나 RNN본질적으로 시퀀스이며, 병렬화가 어렵기 때문에 훈련과 추론이 느려집니다.

언어 모델링 및 기타 도메인의 최근 연구에 따르면 CNN은 RNN과 비교할 수 있거나 더 나은 성능을 가진 시퀀스 작업에 쉽게 적용될 수 있습니다.

RNN과 달리 CNN은 완전히 병렬화 가능하므로 GPU와 같은 최신 아키텍처에서 속도가 크게 향상됩니다. 이러한 결과에서 영감을 받아 재생 목록 내에서 노래 순서를 고려하여 생성하는 temporal CNN을 개발합니다.

모델 아키텍처는 Fig.2에 나와 있습니다. 이 모델에서 연결된 노래 임베딩은 게이트 선형 유닛(GLU) 컨볼 루션 블록의 여러 레이어를 통과합니다.

그런 다음 마지막 GLU 블록의 활성화는 Max-Pool을 사용하여 함께 집계되고 완전 연결 레이어로 전달되어 재생 목록 임베딩을 출력합니다.

공식적으로, 각 재생 목록 $i$에 대한 입력은 $i$에 존재하는 노래에 대한 연결된 임베딩으로 구성됩니다.

$\Phi_i^{1:k} = [V_1^{CNN},...,V_k^{CNN}]\tag{2}$

<img src = "https://py-tonic.github.io/images/2stage/2.png">

재생 목록 내의 노래 하위 시퀀스도 유효한 재생 목록을 형성하므로 for $k:1 \leq k \leq \vert V(i)\vert$의 범위를 갖습니다.

따라서 $\Phi$는 $p\times k$ 행렬이며, $p$는 입력 임베딩의 크기입니다.

GLU블록의 컨볼루션은 왼쪽에서 오른쪽으로 $\Phi_i^{1:k}$로 적용되며 각 연속 레이어는 노래 시퀀스 내에서 점점 더 긴 범위 구조를 캡처합니다.

다양한 길이의 입력을 다루기 위해서 마지막 컨볼 루션 레이어 다음에 max-pool을 사용했습니다.

Max-Pool은 각 컨볼루션 커널에 대해 가장 큰 활성화를 유지합니다. 예를 들어 마지막 컨볼루션 레이어에 500개의 커널이 있는 경우 max-pool의 출력은 길이가 500인 벡터가 됩니다.

따라서 출력의 길이는 입력 수가 아닌 커널 수에만 의존합니다. CNN을 통해 입력 시퀀스를 전달하면 재생 목록 임베딩이 생성됩니다:

$U_i^{cnn} = f(\Phi_i^{1:k},\theta) \tag{3}$

우리의 목표는 검색이므로, $U_i^{cnn}$은 $V_k^{cnn}$을 따르는 노래의 정확한 예측자가 되어야 합니다.

이것은 우리의 목적 함수의 기반을 형성합니다.

완전히 연결된 레이어의 크기를 적절하게 조정하여 재생 목록 임베딩 $U^{cnn}$을 입력 노래 임베딩 $V^{cnn}$과 동일한 크기로 만듭니다.

주어진 노래 $j$가 재생 목록 $i$에 포함될 확률은 다음과 같이 정의됩니다:

$P(V_j^{cnn}\vert U_j^{cnn}) = \frac1{1+e^{-U_i^{cnn}V_j^{cnn}}} \tag{4}$

훈련 중에 예측 포인트 $k$가 주어지면 $V_k^{cnn}$를 따르는 노래의 확률을 높이고 다른 모든 노래에 대해 낮추는 것을 목표로 합니다.

stochastic 최적화 접근법을 채택하여 각 미니 배치에서 재생 목록에 대한 예측 포인트 $k\in [1,\vert V(i)\vert]$를 반복적으로 샘플링합니다.

$k$가 주어질 때 우리는 위치 $k$ 다음에 나타나고, $j \notin V(i)$인 노래 $j \in V(i)$를 샘플링합니다. 

그런 다음 $L$의 그래디언트를 사용하여 모델을 업데이트 합니다.

다음 단어만 예측되는 언어 모델링과 달리, 미래를 향한 임의의 예측을 모두 훈련시킵니다.

결과적으로 다음 노래만 예측하면 모델이 입력의 마지막 노래에 크게 집중하여 성능이 크게 저하됩니다.

> 음 이게 이렇게된다고..? 플레이 리스트 예측이 굳이 글로벌한 정보를 가져야 되려나..?



