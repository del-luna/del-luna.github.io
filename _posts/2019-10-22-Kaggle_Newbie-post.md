---
layout: post
title: for kaggle newbie's
author: Jaeheon Kwon
categories: Kaggle
tags: [model]
---

# Kaggle 뉴비를 위한 Stacking에서 사용되는 모델들

안녕하세요 Titanic을 이용해서 AI스터디 도중 Kaggle은 아무래도 대회이다보니 스태킹, 앙상블을 사용하는 경우가 대부분이라 이 때 자주 사용되는 모델에대해 간략한 정리를 하면 좋을 것 같아서 정리해봤습니다.  

뉴비들을 위한 자료이기때문에 상세히 다루지는 않습니다.  

스태킹이나 앙상블을 하면서 자기가 사용하는 모델들이 어떤 원리로 동작하는지도 모르고 쓰는 것 보단 간략히 알고 쓰는 게 좋을 것 같아서 만든 자료 입니다.

## 1. Logistic Regression

Logistic Regression는 샘플이 특정 클래스에 속할 확률을 추정하는 데 널리 사용됩니다.(EX : 이 이메일이 스팸일 확률은 얼마인가?)
추정 확률이 50%가 넘으면 모델은 그 샘플이 해당 클래스에 속한다고 예측합니다.

그렇다면 실제로 어떻게 동작할까요?
선형 회귀 모델과 같이 로지스틱 회귀 모델은 입력 특성의 가중치 합을 계산하고 편향을 더 합니다.(Wx+b)
대신 선형 회귀처럼 바로 결과를 출력하지 않고 Logistic을 출력합니다.
Logistic(σ)은 0과 1사이의 값을 출력하는 Sigmoid function입니다.
$σ(t) = 1/{1+exp(-t)}$



<img src = "https://py-tonic.github.io/images/Stacking/0.png">

[source : ratsgo's blog]([https://ratsgo.github.io/machine%20learning/2017/04/02/logistic/](https://ratsgo.github.io/machine learning/2017/04/02/logistic/))

로지스틱 회귀 모델이 샘플 x가 양성 클래스에 속할 확률 p = h(x)를 추정하면 이에대한 예측을 쉽게 구할 수있습니다.
t < 0 이면 σ(t) <0.5 이고, t ≥ 0.5이면 σ(t) ≥ 0.5 이므로 로지스틱 회귀 모델은 Wx가 양수일 때 1(양성 클래스)이라고 예측하고, 음수일 때 0(음성 클래스)이라고 예측합니다.

Cost function = -1/m ∑[y log(p) + (l-y)log(1-p)]로 간단히 나타낼 수 있습니다.

! 이 식은 이진분류라서 2개지만 항을 여러개로 늘린다면 다중 분류가 가능한 Softmax 함수가 됩니다.

## 2. SVM

SVM을 설명하기 위해서 예시를 들어 보겠습니다.

<img src = "https://py-tonic.github.io/images/Stacking/1.png">

위의 그림을 보면 왼쪽 그래프에서는 세 개의 선형 분류기에서 만들어진 결정 경계가 보입니다. 점선으로 나타난 결정 경계를 만든 모델은 잘 분류하지 못하고 있는 것 같습니다.
다른 두 모델은 완벽하게 분류 했지만 그래프가 샘플에 너무 가까워서 다른 샘플에서는 제대로 작동하지 않을 것 같습니다.

오른쪽의 그래프에 있는 실선은 SVM 분류기의 결정 경계입니다. 이 직선은 두 클래스를 잘 분류했을 뿐만 아니라 제일 가까운 훈련 샘플로부터 가능한 멀리 떨어져 있습니다.
SVM 분류기는 클래스 사이의 가장 폭이 넓은 도로를 찾는 것으로 생각할 수 있습니다.

그래서 Large margin classification라고 합니다.

도로 바깥쪽에 훈련 샘플을 더 추가해도 결정 경계에는 영향을 미치지 않습니다.
도로 경계에 위치한 샘플에 의해  전적으로 결정(또는 의지)됩니다.
이런 샘플을 <u>**Support Vector**</u> 라고 합니다.

모든 샘플이 도로 바깥쪽에 분류되어 있다면 이를 Hard margin classification이라고 합니다.
그러나 하드 마진 분류는 데이터가 선형적으로 구분되어야 하며 이상치에 민감하다는 단점이 있습니다.

<img src = "https://py-tonic.github.io/images/Stacking/2.png">

이런 문제를 피하려면 좀 더 유연한 모델이 필요합니다.
도로의 폭을 가능한 넓게 유지하면서 margin error(샘플이 도로 중앙이나 반대쪽에 있는 경우)사이에 적절한 균형을 잡아야 합니다.
이를 Soft margin classification이라고 합니다.
C 라는 hyperparameter를 이용하여 균형을 조절할 수 있습니다.

C값을 줄이면 도로의 폭이 넓어지지만 margin error도 커집니다.

위의 설명들은 모두 선형 SVM이다 그렇다면 선형 SVM으로 데이터를 분류할 수 없을 때 어떻게 해야 할까?

RBF-Kernel-SVM이라는 방법이 있습니다.


그중에서도 가우시안 RBF 커널은 성능이 좋아 자주 사용됩니다.
RBF의 경우 γ(gamma)라는 매개변수를 이용하여 성능을 조정합니다.
γ의 역할은 하나의 데이터 샘플이 영향력을 행사하는 거리를 결정합니다.
γ는 가우시안 함수의 표준편차와 관련되어 있는데, 클수록 작은 표준편차를 갖습니다.

이름에서도 볼 수 있듯 가우시안 함수와 관련이 있는데 γ를 증가 시키면 종 모양 그래프가 좁아지므로 각 샘플의 영향 범위가 작아집니다. 
결정 경계가 조금 더 불규칙해지고 각 샘플을 따라 구불구불하게 휘어집니다.(곡률증가)
<img src = "https://py-tonic.github.io/images/Stacking/3.png">

## 3. Decision Tree & Rancom Forest

결정 트리를 이해하기 위해 아래의 그림을 살펴봅시다.

<img src = "https://py-tonic.github.io/images/Stacking/4.png">

위 예시는 운동경기가 열렸다면 PLAY=1, 그렇지 않으면 PLAY=0으로 하는 이진분류(binary classification) 문제입니다.
노드를 해석해보면 비가오면서 바람이 불면 경기가 열리지 않았고
날씨가 맑고 습도가 70이하이면 경기가 열렸습니다.

가장 위의 노드를 Root node라고 부릅니다.

아래의 노드들은 Child node이며 만약 Child node를 갖지 않는다면 그 노드는 leaf node라고 부릅니다.

결정트리는 굉장히 단순하고 특히 데이터의 전처리가 거의 필요하지 않는다는 장점이 있습니다.

결정트리의 앙상블을 Random Forest라고 합니다.
일반적으로 배깅 방법을 적용한 결정트리의 앙상블이며 랜덤포레스트 알고리즘은 트리의 노드를 분할할 때 전체 특성 중에서 최선의 특성을 찾는 대신 무작위로 선택한 특성 후보 중에서 최적의 특성을 찾는 식으로 무작위성을 더 주입합니다.

이는 결국 트리를 더욱 다양하게 만들고 편향을 손해보는 대신 분산을 낮추어 전체적으로 더 훌륭한 모델을 만들어 냅니다.

학습 원리는 아래와 같습니다.

1. 주어진 훈련 샘플에서 배깅을 이용하여 n개를 샘플링합니다.
2. 샘플링한 n개의 데이터셋에서 데이터 특성값을 중복 허용 없이 d개 선택합니다.
3. 이를 이용해 결정트리를 학습하고 생성합니다.
4. 1~3단계를 K번 반복합니다.
5. 1~4단계를 통해 생성된 K개의 결정트리를 이용해 예측하고 예측된 결과의 평균(회귀 문제)이나 빈도 수(분류 문제)가 높은 예측 결과를 선택하여 최종 예측값으로 결정합니다.

[source : 잡동사니 탐구](https://m.blog.naver.com/PostView.nhn?blogId=samsjang&logNo=220979751089&proxyReferer=https%3A%2F%2Fwww.google.com%2F)

## 4. K-NN

KNN도 굉장히 단순한 알고리즘입니다.

한마디로 정의하면 '유유상종' 입니다.
현재 데이터의 위치를 기준으로 주변의 다른 데이터들과의 거리를 측정한 뒤

거리가 가장 가까운 K개의 주변 데이터들을 참고하여 현재의 데이터를 분류하는 알고리즘 입니다.

<img src = "https://py-tonic.github.io/images/Stacking/5.png">

제 블로그에 [KNN 구현해보기](https://jaeheondev.github.io/KNN-Algorithm-post/)가 있으니 참고해보세요!

## 5. Naive Bayes

나이브 베이즈 분류기를 이해하기 위해서는 우선 Bayes' theorem를 이해할 필요가 있습니다.

P(B│A)가 A가 일어나고나서 B가 일어날 확률이라고 합시다.
이 때 P(B│A)를 쉽게 구할 수 있다면 아래의 식을 이용하여 P(AlB)를 구할 수 있습니다.

P(A│B) = P(B│A) x P(A) / P(B)
나이브 베이즈 분류기는 이러한 베이즈 정리를 이용하여 분류를 수행합니다.
예를들어 스팸메일 필터를 만들 때 입력 텍스트가 주어진다면 입력 텍스트가 정상메일인지 스팸메일인지 구분하기 위한 확률을 이와 같이 표현할 수 있습니다.

P(정상 메일 │ 입력 텍스트) = 입력 텍스트가 있을 때 정상 메일일 확률
P(스팸 메일 │ 입력 텍스트) = 입력 텍스트가 있을 때 스팸 메일일 확률

베이즈 정리를 이용하여 식을 변환해 봅시다.

P(정상 메일 │ 입력 텍스트) = P(입력 텍스트 │ 정상 메일) x P(정상 메일) / P(입력텍스트)
P(스팸 메일 │ 입력 텍스트) = P(입력 텍스트 │ 스팸 메일) x P(스팸 메일) / P(입력텍스트)

입력 텍스트가 주어졌을 때, P(정상 메일 │ 입력 텍스트)가 P(스팸 메일 │ 입력 텍스트)보다 크다면 정상 메일이라고 볼 수 있으며, 그 반대라면 스팸 메일이라고 볼 수 있습니다.
그런데 두 확률 모두 분모가 같으므로 간략화 할 수 있습니다.

P(정상 메일 │ 입력 텍스트) = P(입력 텍스트 │ 정상 메일) × P(정상 메일)
P(스팸 메일 │ 입력 텍스트) = P(입력 텍스트 │ 스팸 메일) × P(스팸 메일)

[source](https://wikidocs.net/22892)

