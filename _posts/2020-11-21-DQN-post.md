---
layout: post
title: Deep Q Network
author: Jaeheon Kwon
categories: Papers
tags: [rl]
---



### Abstract

강화학습을 사용하여 고차원 sensory input에서 직접 제어 정책을 성공적으로 학습하는 최초의 딥러닝 모델을 제시함.

모델은 큐러닝의 변형으로 훈련된 CNN으로, 입력은 raw 픽셀이고 아웃풋은 미래 보상을 추정하는 가치함수이다.

아키텍처 혹은 학습 알고리즘을 조정하지 않고 7가지 아타리 게임에 적용함.

6개의 게임에서 이전의 모든 접근 방식을 능가하고, 3개 게임해서 사람(전문가)을 능가했음.



### Introduction

비전 및 언어와 같은 고차원 인풋에서 직접 에이전트를 컨트롤 하는 방법을 배우는 것은 RL의 오랜 과제 중 하나였음.

이런 도메인에서 작동하는 대부분의 성공적인 RL 어플리케이션은 선형 가치함수 또는 정책 표현과 결합된 핸드 크래프트 피처에 의존했다. 명백하게도 이런 시스템의 성능은 피처 표현에 크게 의존한다.

> 여기서 알 수 있는 사실은 강화학습의 역사가 꽤 깊다는 사실.
>
> DNN의 유행 전에 저런 ML의 접근 방식을 사용해서 활용해오고 있었으나 DNN이 뜨면서 DNN에 접목시킨 것이 이 논문의 핵심 포인트중 하나.

강화학습은 딥러닝 관점에서 몇 가지 챌린지를 제시함.

1. 지금까지(2014년 기준임..) 성공적인 딥러닝 방식은 많은 양의 수작업으로 처리도니 레이블이 지정된 훈련 데이터가 필요함.
    - 반면에 강화학습 알고리즘은 희소하고, 노이즈가 끼며 지연되는 스칼라 보상 신호에서 학습 할 수 있어야 함. 행동과 결과 보상 사이의 지연은 지도 학습에서 발견되는 인풋과 목표 사이의 직접적인 연관성과 비교할 때 특히 많은 차이점이 보임.(이게 더 어려움)
2. 또 다른 문제는 대부분의 딥러닝 알고리즘이 데이터 샘플을 iid라고 가정함.
    - 반면에 강화학습에서는 일반적으로 상관 관계가 높은 상태의 시퀀스를 만남.
    - 새로운 동작을 학습함에 따라 데이터 분포가 변경되며 이는 고정된 기본 분포를 가정하는 딥러닝 방법에 문제가 될 수 있음



> 2번의 경우 강화학습은 iid를 사용할 수 없습니다.
>
> 기본적으로 MDP환경 위에서 동작하게 되는데 이전 상태에 따라 다음 상태가 정해지는 의존적인 관계(Markov)를 가정하기 때문에 Correlation이 높은 시퀀스를 마주하게 됩니다.
>
> 정책의 경우에도 업데이트를 통해 분포가 변경되기 때문에 데이터 분포가 변경된다고 언급한 것 같음.



논문에서는 복잡한 강화학습 환경에서 raw video 데이터를 바탕으로 성공적인 정책을 학습하기 위해 CNN이 이러한 문제를 극복할 수 있음을 보여줌.

상관 관계를 갖는 데이터와 non-stationary distribution 문제를 완화하기 위해 이전의 트랜지션을 랜덤하게 샘플링하는 experience reply 메커니즘을 사용하여 많은 과거 행동에 대한 훈련 분포를 원할하게 만들어 줌