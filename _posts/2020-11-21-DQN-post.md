---
layout: post
title: Deep Q Network
author: Jaeheon Kwon
categories: Papers
tags: [rl]
---



### Abstract

강화학습을 사용하여 고차원 sensory input에서 직접 제어 정책을 성공적으로 학습하는 최초의 딥러닝 모델을 제시함.

모델은 큐러닝의 변형으로 훈련된 CNN으로, 입력은 raw 픽셀이고 아웃풋은 미래 보상을 추정하는 가치함수이다.

아키텍처 혹은 학습 알고리즘을 조정하지 않고 7가지 아타리 게임에 적용함.

6개의 게임에서 이전의 모든 접근 방식을 능가하고, 3개 게임해서 사람(전문가)을 능가했음.



### Introduction

비전 및 언어와 같은 고차원 인풋에서 직접 에이전트를 컨트롤 하는 방법을 배우는 것은 RL의 오랜 과제 중 하나였음.

이런 도메인에서 작동하는 대부분의 성공적인 RL 어플리케이션은 선형 가치함수 또는 정책 표현과 결합된 핸드 크래프트 피처에 의존했다. 명백하게도 이런 시스템의 성능은 피처 표현에 크게 의존한다.

> 여기서 알 수 있는 사실은 강화학습의 역사가 꽤 깊다는 사실.
>
> DNN의 유행 전에 저런 ML의 접근 방식을 사용해서 활용해오고 있었으나 DNN이 뜨면서 DNN에 접목시킨 것이 이 논문의 핵심 포인트중 하나.

강화학습은 딥러닝 관점에서 몇 가지 챌린지를 제시함.

1. 지금까지(2014년 기준임..) 성공적인 딥러닝 방식은 많은 양의 수작업으로 처리도니 레이블이 지정된 훈련 데이터가 필요함.
    - 반면에 강화학습 알고리즘은 희소하고, 노이즈가 끼며 지연되는 스칼라 보상 신호에서 학습 할 수 있어야 함. 행동과 결과 보상 사이의 지연은 지도 학습에서 발견되는 인풋과 목표 사이의 직접적인 연관성과 비교할 때 특히 많은 차이점이 보임.(이게 더 어려움)
2. 또 다른 문제는 대부분의 딥러닝 알고리즘이 데이터 샘플을 iid라고 가정함.
    - 반면에 강화학습에서는 일반적으로 상관 관계가 높은 상태의 시퀀스를 만남.
    - 새로운 동작을 학습함에 따라 데이터 분포가 변경되며 이는 고정된 기본 분포를 가정하는 딥러닝 방법에 문제가 될 수 있음



> 2번의 경우 강화학습은 iid를 사용할 수 없습니다.
>
> 기본적으로 MDP환경 위에서 동작하게 되는데 이전 상태에 따라 다음 상태가 정해지는 의존적인 관계(Markov)를 가정하기 때문에 Correlation이 높은 시퀀스를 마주하게 됩니다.
>
> 정책의 경우에도 업데이트를 통해 분포가 변경되기 때문에 데이터 분포가 변경된다고 언급한 것 같음.



논문에서는 복잡한 강화학습 환경에서 raw video 데이터를 바탕으로 성공적인 정책을 학습하기 위해 CNN이 이러한 문제를 극복할 수 있음을 보여줌.

상관 관계를 갖는 데이터와 non-stationary distribution 문제를 완화하기 위해 이전의 트랜지션을 랜덤하게 샘플링하는 experience reply 메커니즘을 사용하여 많은 과거 행동에 대한 훈련 분포를 원할하게 만들어 줌

논문의 목표는 가능한 많은 게임을 플레이하는 방법을 성공적으로 배울 수 있는 **단일 신경망 에이전트**를 만드는 것.



### Background

에이전트가 액션의 시퀀스, 관찰 및 리워드에서 환경 E(아타리)와 상호작용 하는 태스크를 고려함.

각 타임스텝에서 에이전트는 액션 세트에서 액션을 선택함. 액션은 E로 전달되고 내부 스테이트와 게임 점수를 수정함. 일반적으로 환경은 stochastic임.

에이전트는 환경의  내부 상태를 관찰하지는 않음. 대신 환경으로부터 이미지 $x_t\in R^d$를 관찰함. 이미지는 현재 화면을 나타내는 원시 픽셀 값의 벡터.

Value iteration 알고리즘은 i->무한 일때 액션 밸류 함수 Q_i->Q^*로 수렴한다.

실제로 이런 접근 방식은 비실용적임. 액션 밸류 함수는 일반화 없이 각 시퀀스에 대해 별도로 추정됨.

> Such value iteration algorithms converge to the optimal actionvalue function, Qi → Q∗ as i → ∞ [23]. In practice, this basic approach is totally impractical, because the action-value function is estimated separately for each sequence, without any generalisation. 

대신 function approximator를 사용하여 action-value function을 추정하는 것이 일반적임.

일반적으로 linear function을 사용하지만 신경망 같은 비선형 함수 근사값도 사용할 수 있음.

가중치를 $\theta$로 놓은 신경망 function approximator를 Q-Network라고 함. Q-Network는 각 이터레이션 i에서 변겨오디는 손실 함수 $L_i(\theta_i)$를 최소화 하여 학습함.

$y_i$는 이터레이션 i의 대상이고, p(s,a)는 시퀀스 s 및 액션에 대한 확률 분포입니다.

이 알고리즘은 model- free임 E의 추정치를 명시적으로 구성하지 않고 에뮬레이터의 샘플을 사용하여 직접 강화학습 작업을 해결함. 또한 off-policy 방식임, greedy하게 a=max Q(s,a)에 대해 학습하면서 상태 공간의 적절한 탐색을 보장하는 행동 액션 분포를 따름.

실제로 행동 분포는 확률이 1-e 인 greedy 전략을 따르고 확률로 임의의 행동을 선택하는 greedy 전략에 의해 종종 선택됨



### Related Work

가장 잘 알려진 강화학습 알고리즘 중 하나인 TD-gammon은 Q-learning과 유사한 model-free 알고리즘을 사용하고 히든 레이어가 1개인 다층 퍼셉트론을 사용하여 가치함수를 근사시킴.

그러나 이 알고리즘 또한 체스 및 바둑과 같은 태스크에는 성공적이지 못했음

또한 Q-learningr과 같은 model-free 강화학습 알고리즘을 비선형 함수 근사 혹은 off-policy와 결합하면 Q-Network가 발산될 수 있음이 밝혀짐

위와 같은 이유로 대부분의 강화학습 태스크는 더 나은 수렴 보장을 가진 선형 함수 근사에 초점이 맞춰졌음

그러나 딥러닝이 다시 인기를 끌고 환경 E를 추정하기 위해 딥러닝이 사용되었음. 제한된 볼츠만 머신은 가치함수 혹은 정책을 추정하는데 사용됨. 또한 Q-learning의 발산 문제는 Temporal diffrence 알고리즘을 통해 부분적으로 해결되었음.

이러한 방법은 비선형 함수 근사로 고정된 정책을 평가할 때 혹은 제한된 Q-learning의 변형을 사용하여 선형 함수 근사를 사용하여 control 정책을 학습할 때 수렴하는 것을 입증함. 그러나 여전히 비선형 제어로 확장되지는 못함.

아마도 논문과 가장 유사한 이전 접근 방식은 neural fitted Q-learning(NFQ) 임.

NFQ는 RPROP알고리즘을 사용하여 Q-Network의 매개변수를 업데이트하여 손실함수 시퀀스를 최적화함.

대조적으로 논문의 접근 방식은 시각적 입력에서 직접 강화학습을 end-to-end로 적용함.

결과적으로 action-value를 구별하는 것과 직접적으로 관련된 피처를 학습할 수 있음.

Q-learning은 이전 experience replay 및 단순한 신경망과 결합되었지만 원시 시각적 입력이 아닌 저차원 state에서 재시작됨.



### Deep Reinforcement Learning

