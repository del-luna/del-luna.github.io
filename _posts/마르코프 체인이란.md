---

---



## 마르코프 체인이란?

---

마르코프 체인은 마르코프 성질을 지닌 이산 확률 과정(Discreate Stochastic Process)을 의미합니다.

시간에 따른 계의 상태 변화를 나타내며, 매 시간마다 계는 상태를 바꾸거나 같은 상태를 유지합니다. **마르코프 성질**은 과거와 현재 상태가 주어졌을 때의 미래 상태의 조건부 확률 분포가 과거 상태와는 독립적으로 현재 상태에 의해서만 결정된다는 것을 뜻합니다.

1차 마르코프 체인

$$P(o_t\vert o_{t-1},...,o_1) = P(o_t\vert o_{t-1})$$

2차 마르코프 체인

$$P(o_t\vert o_{t-1},...,o_1)=P(o_t\vert o_{t-1},o_{t-2})$$

마르코프 체인 앞의 n이 늘어날수록 이전 상태의 영향을 더 많이 받습니다.



## 마르코프 모델

---

마르코프 모델은 위의 가정 하에 확률 모델을 생성한 것으로, 가장 먼저 각 상태를 정의합니다.

상태는 $V={v_1,...,v_m}$으로 정의하고, 상태 전이 확률을 정의합니다. $a_{ij}$는 $v_i$에서 $v_j$로 이동할 확률을 뜻합니다. 식으로는 다음과 같이 나타냅니다.

$$a_{ij}=P(o_t=v_j\vert o_{t-1}=v_i) \\ a_{ij}>0\: and\: \sum\limits_{j=1}^m a_{ij}=1$$

상태와 상태 전이 확률을 정리하여 diagram으로 아래와 같이 표현 할 수 있습니다.

![1](/Users/devcat/git/blog/images/markov/1.png)

## Markov decision process

---

강화학습은 주로 MDP라는 확률 모델로 표현됩니다. 의사 결정 과정을 확률과 그래프를 이용해 모델링 한 것으로써, 시간 '$t$'에서 상태는 '$t-1$'에서만 영향을 받는다 라는 위에서 설명한 1차 마르코프 체인을 기반으로 만들어졌습니다. 

MDP를 설명하기 앞서 MDP의 기본 모델인 Markov reward process를 알아봅시다.

### Markov reward process

MRP는 마르코프 프로세스의 각 상태에 리워드를 추가하여 확장한 것입니다.

아래와 같이 정의된 <$S,P,R,\gamma$>로 표현 가능합니다.

- $S$: state의 집합입니다. 바둑판의 현재 상태에서 돌들의 위치, 미로 탈출 문제에서 현재의 위치 등을 나타냅니다.
- $P$: 각 요소가 $p(s'\vert s)=Pr(S_{t+1}=s'\vert S_t=s)$인 집합입니다. $p(s'\vert s)$는 현재 상태 $s$에서 $s'$로 이동할 확률을 의미하며 transition probability라고 합니다.
- $R$: 각 요소가 $r(s) = E[R_{t+1}\vert S_t=s]$인 집합입니다. $r(s)$는 상태 $s$에서 얻는 리워드를 뜻합니다.
- $\gamma$: 즉각적으로 얻는 리워드와 미래에 얻을 수 있는 리워드 간의 중요도를 조절하는 변수입니다. 주로[0,1]사이의 값을 가지며, discount factor라고 불립니다.

![2](/Users/devcat/git/blog/images/markov/2.png)

return $G_t$는 $t$시간 이후 얻을 수 있는 리워드의 합을 의미하며 $\gamma$를 통해 다음과 같이 정의됩니다.

$$G_t = R_{t+1} + \gamma R_{t+2} +... = \sum\limits_{k=0}^{\infty} \gamma^kR_{t+k+1}$$



### State-value function

state value function $v(s)$는 state $s$에서 시작했을 때 얻을 수 있는 return의 기댓값을 의미하며 다음과 같이 정의됩니다.

$$v(s)=E[G_t\vert S_t=s]$$

$v(s)$는 궁극적인 목표를 달성하는데 있어서 state $s$가 얼마나 좋은 상태인지를 나타낸다고 볼 수 있다.

$v(s)$는 다음과 같이 재귀적인 형태로 표현될 수 있으며 이를 Bellman equation이라 합니다.

$$v(s)=E[R_{t+1} + \gamma R_{t+2} + \gamma^2 R_{t+3} + ... \vert S_t=s] = E[R_{t+1} + \gamma v(S_{t+1})\vert S_t =s]$$

$$v(s)=R_{t+1} + \gamma\sum\limits_{s' \in S}p(s'\vert s)v(s')$$





어떤 State가 마르코프 성질을 가지는지 가지지 않는지도 생각해 볼 수 있습니다.

$$P(S_{t+1}\vert S_t) = P(S_{t+1}\vert S_1,...,S_t)$$

현재 상태는 과거와는 독립적일 때 마르코프



> 아무튼 이전에 어떤 행동, 보상(이건 결국 State에 포함되는 개념)을 받았던 현재 상태만 고려하는 것으로 어떻게 보면 현재 State는 과거의 모든 History 정보를 포함하고 있기 때문에 현재만 고려해도 된다고 해석했습니다.

이렇게되면 $S_t^e$ 또한 마르코프 이며, $H_t$또한 마르코프입니다.



> Environment는 다음 틱을 결정하기 위해 쓰이는 정보들입니다. 그 정보만 있으면 그 환경이 어떻게 오게 된건지 10분전의 정보(이전 정보)는 필요가 없으므로 $S_t^e$는 마르코프 History 또한 마찬가지입니다.

