---
layout: post
title: 파이썬 웹 크롤러 만들기
author: Jaeheon Kwon
categories: Python
tags: [crawling]
---


# **파이썬 웹 크롤러** **만들기**

방학을 맞아 다른학교 친구들과 머신러닝 프로젝트를 하나 하기로 했습니다. 

주제는 중고차 가격분석 앱으로 결정했고 데이터를 수집 - 정제 - 모델 생성 - 학습 - 앱으로 배포까지 하는게 목표입니다. 책이나 캐글에서 주어진 가공된 데이터가 아니기 때문에 바닥부터 하나하나 해가기 위해서 크롤링 부터 공부하면서 배운 것들을 기록해보기로 했습니다.



## BeautifulSoup

```python
pip install bs4
```

BeautifulSoup는 html코드를 파이썬이 이해하는 객체구조로 변화하는 파싱을 맡습니다.

## Selenium

```python
pip install selenium
```

셀레니움은 주로 웹테스트 목적으로 이용하는 프레임워크입니다. webdriver라는 API를 이용하는데 이를 이용해 브라우저를 직접 동작시키고 요즘 대부분의 웹에서 사용하는 JS를 이용한 비동기적 컨텐츠를 가져올 수 있습니다.

## Chrome WebDriver

[https://sites.google.com/a/chromium.org/chromedriver/downloads](https://sites.google.com/a/chromium.org/chromedriver/downloads)

프로젝트 진행중에 알게됬는데 v74 위로 최신버전을 다운받았더니 알수없는 오류가 생겨서 v74버전으로 다운받고 진행했습니다.



## 사용하기

나는 중고차 분석 앱을 만들기로 했으니 중고차 데이터가 필요하겠죠?

[http://auto.danawa.com/usedcar/?Work=list&Tab=list](http://auto.danawa.com/usedcar/?Work=list&Tab=list)

이 사이트를 이용하기로 했다.

데이터를 크롤링 하기에 앞서서 어떤 데이터가 필요한지 생각해 봤습니다.

토이 프로젝트이다 보니 복잡한 앱은 만들 생각이 없었고 가격을 결정하는 가장 큰 피쳐가 무엇인지 생각해보니 <u>차종, 연식, 주행거리, 사고유무</u> 등이 떠올랐습니다.

위 사이트에서 사고유무는 보이지 않아서  결국 <u>차종, 연식, 주행거리, 가격</u> 데이터를 크롤링 하기로 했습니다.

위의 링크를 타고 들어가 F12(개발자 도구)를 눌러봅시다.

우리가 원하는 데이터인 <u>차종, 연식, 주행거리 , 가격</u>이

```python
<li class = "row"></li>
```

사이에 있으며 각각

```python
<div class="col info">
<div class="col year">
<div class="col km">
<div class="col price">
```

태그안에 들어있는 것을 알 수 있습니다.

또한 한페이지에 총 30가지 항목(차종)이 있으므로

```python
<li class = "row"></li>
```

가 30번 나와야 합니다.



위 사이트에는 거의 270페이지 가량 총 8200대의 차량정보가 있는데, 150페이지 까지의 데이터를 모아 보기로 했습니다.

```python
from selenium import webdriver
from bs4 import BeautifulSoup
import pandas as pd

path  = r'C:\Users\illus\Downloads\chromedriver_win32\chromedriver.exe'
driver = webdriver.Chrome(path)
page_list = list(range(1,151))
result_list = []
for page in page_list:
    driver.get("http://auto.danawa.com/usedcar/?Work=list&Tab=list&Page={}&Brand=&Series=&Model=".format(page))

    html = driver.page_source
    soup = BeautifulSoup(html,"lxml")
    data_list = soup.select("li[class=row]")

    for data in data_list :
        name = data.find('a',{'class':'name'}).text
        year = data.find('div',{'class':'col year'}).text.strip()
        km = data.find('div',{'class':'col km'}).text.strip()
        price = data.find('div', {'class': 'col price'}).text.strip()
        result_list.append([name, year, km, price])

data = pd.DataFrame(result_list)
data.head()
data.columns = ['car_name','year','mileage','price']
data.head()

data.to_csv('danawa_car_data.csv', encoding = 'euc-kr')
```

위의 코드는 전체코드이고 하나하나 분석해봅시다.

```python
path  = r'C:\Users\illus\Downloads\chromedriver_win32\chromedriver.exe'
driver = webdriver.Chrome(path)
```

크롬 드라이버를 다운받은 경로를 설정해줍니다.

```python
page_list = list(range(1,151))
result_list = []
for page in page_list:
    driver.get("http://auto.danawa.com/usedcar/?Work=list&Tab=list&Page={}&Brand=&Series=&Model=".format(page))
```

페이지 리스트들을 만들어 for문을 이용해 하나하나 접근합니다.

```python
data_list = soup.select("li[class=row]")
```

우리는 필요한 데이터들이 모두 이 안에 담겨있는것을 알고 있으므로 이 데이터들의 리스트를 생성해서 for문을 이용해 뽑아 쓰기만 하면 됩니다.

```python
 for data in data_list :
        name = data.find('a',{'class':'name'}).text
        year = data.find('div',{'class':'col year'}).text.strip()
        km = data.find('div',{'class':'col km'}).text.strip()
        price = data.find('div', {'class': 'col price'}).text.strip()
        result_list.append([name, year, km, price])

```

find()함수를 이용해 각각 정해진 데이터가 있는 태그를 찾고 text는 태그를 제거한 텍스트만 얻는 함수이고 strip()은 공백을 제거해줍니다.

각각의 정보를 name, year, km, price를 reslut_list에 넣어줍니다.

```python
data = pd.DataFrame(result_list)
data.head()
data.columns = ['car_name','year','mileage','price']
data.head()

data.to_csv('danawa_car_data.csv', encoding = 'euc-kr')
```

Pandas를 이용해서 각 데이터들의 column을 달아주고 csv로 저장하면 됩니다!
