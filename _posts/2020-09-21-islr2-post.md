---

---



## 2.1 What Is Statiscal Learning?

통계적 학습에 대해 알아보기 위해 한가지 예시를 들어봅시다.

우리는 특정 상품의 판매량을 향상시키는 방법에 대해 조언해주기 위해 클라이언트에게 고용된 통계 컨설턴트입니다.

광고 데이터셋은 세 가지 플랫폼에 대한 각 시장의 제품 광고 예산과 함께 200개의 서로 다른 제품 판매량으로 구성됩니다.



![0](/Users/devcat/Desktop/islr/2.1.png)



클라이언트가 직접 제품의 판매량을 증가시킬 순는 없지만 각각의 플랫폼에 대한 광고 비용의 지출을 컨트롤 할 수는 있습니다.

따라서,광고와 판매량 사이의 연관성을 판단하여 클라이언트에게 광고 예산을 컨트롤해서 간접적으로 매출을 증가시키도록 해야합니다.

즉, 우리의 목표는 세 가지 플랫폼에 대한 예산을 기반으로 매출을 예측하는 데 사용할 수 있는 정확한 모델을 개발하는 것입니다.



$X$축은 각각의 플랫폼에 대한 광고비용, $Y$축은 판매량입니다.

이런 관계식에서 $X$는 $input\:variable,\:predictors,\:independent\:variables,\:features$ 등으로 불리고, $Y$는 $ouput\: variable,\: response$ or $dependent\: variable$ 이라고 불립니다.

각각의 그래프는 판매량에 대해 simple least squares를 적용한 선을 하나 가집니다.(이건 3장에서 다룹니다.)



위 관계를 일반화하여 식으로 나타내봅시다.

$p$ 개의 입력 변수 $X_1,X_2,...,X_p$와 종속 변수 $Y$의 관계는 다음과 같이 나타냅니다.

$$Y=f(X)+\epsilon \tag{2.1}$$

$f$는 입력 변수에 대한 알 수 없는 함수이고 $X$가 $Y$에 대해 제공하는 체계적인 정보를 나타내며, $\epsilon$은 $X$와 독립이며 평균이 0인 랜덤 에러 입니다.



또 다른 예시를 고려해봅시다.

아래의 왼쪽 그래프는 소득 데이터 셋에서 30명의 개인에 대한 소득과 교육 연수를 나타낸 그래프입니다.![2.2](/Users/devcat/Desktop/islr/2.2.png)



그래프는 교육 연수를 입력 변수로하여 소득을 예측할 수 있음을 보여줍니다.

그러나 $f$는 여기에서도 여전히 알 수 없습니다. 이 상황에서 관찰된 점을 기반으로 $f$를 추정해야 합니다.

수입은 시뮬레이션된 데이터 셋이므로 $f$가 알려져 있으며 위 그림의 오른쪽에서 파란색 곡선으로 표시됩니다. 여기서 수직 선은 오차 $\epsilon$을 나타냅니다.

오른쪽 그래프를 보면 30개의 관측치 중 일부는 파란색 곡선 위에 있고, 일부는 그 아래에 있습니다. 전반적으로 오차의 평균은 약 0입니다.



일반적으로 함수 $f$는 하나 이상의 입력 변수를 갖습니다. 아래의 그림은 교육 연수와 나이에 대한 수입의 함수를 나타냅니다. 여기서 $f$는 관측된 데이터를 바탕으로 추정한 2차원 평면입니다.



![2.3](/Users/devcat/Desktop/islr/2.3.png)



본질적으로, 통계적 학습은 $f$를 추정하기 위한 접근 방법을 의미합니다. 이번 챕터에서는 주요 이론적 개념과 획득한 추정치를 평가하기 위한 도구를 설명합니다.



### 2.1.1 Why Estimate f?

$f$를 추정하는 두 가지 이유가 있습니다.

- Prediction
- Inference



**Prediction**

대부분의 상황에서 입력 $X$는 쉽게 사용 가능하지만, 출력 $Y$는 쉽게 얻을 수 없습니다.

이러한 설정에서 오차항은 평균이 0이므로 다음 식을 사용하여 $Y$를 예측할 수 있습니다.

$$\hat Y = \hat f(X) \tag{2.2}$$

$\hat f$는 $f$에 대한 추정치를 나타내고, $\hat Y$는 $Y$에 대한 결과 예측을 나타냅니다.

일반적으로 $Y$에 대한 정확한 예측을 제공하는 경우 $\hat f$의 형태에 관심이 없다는 의미에서 종종 블랙박스로 취급됩니다.

예를 들어, $X_1,...,X_p$는 실험실에서 쉽게 측정 가능한 환자 혈액 샘플의 특성이고, $Y$는 특정 약물의 심각한 부작용에 대한 환자의 위험을 인코딩한 변수입니다.

$X$를 사용하여 $Y$를 예측하는 것은 당연합니다. 이를 통해서 부작용의 위험이 높은 환자 즉, $Y$의 추정치가 높은 환자에게 해당 약물을 투여하는 것을 피할 수 있기 때문입니다.

$Y$에 대한 예측으로서 $\hat Y$의 정확도는 두 가지 양에 따라 달라지며, 이를 $reducible\: error$와 $irreducible \:error$라고 합니다.

일반적으로 $\hat f$는 $f$를 위한 완벽한 추정이 아니기 때문에 약간의 오류를 유발합니다. 

이 때의 오류를 $reducible\:error$ 라고 하며 $f$를 추정하기 위해 가장 적절한 통계적 학습 기법을 사용하여 $\hat f$의 정확도를 잠재적으로 향상시킬 수 있습니다.

그러나, $f$에 대해 완벽한 추정치를 형성할 수 있어도, 추정된 응답이 $\hat Y=f(X)$ 형식을 취했더라도 예측에는 여전히 약간의 오류가 있을 것입니다.

이는 $Y$또한 $\epsilon$의 함수이기 때문이며 정의에 따라 $X$를 사용하여 예측할 수 없습니다.

따라서 $\epsilon$과 관련된 변동성은 예측의 정확성에도 영향을 미칩니다.

이것은 우리가 $f$를 아무리 잘 추정하더라도 $\epsilon$에 의해 발생하는 오류를 줄일 수 없기 때문에 $irreducible\:error$라고 알려져 있습니다.



irreducible error가 0보다 큰 이유는 $\epsilon$에는 Y를 예측하는데 유용하지만 측정되지 않는 변수가 포함될 수 있기 때문입니다.

$\epsilon$은 측정할 수 없는 분산도 포함될 수 있습니다. 예를 들어, 부작용의 위험은 주어진 날에 약물 자체의 제조 변형 혹은 그날 환자의 건강 상태 등, 주어진 환자에 따라 다를 수 있습니다.



예측 $\hat Y = \hat f(X)$를 산출하는 주어진 추정치 $\hat f$와 입력 변수 $X$를 고려해봅시다.

$\hat f$와 $X$가 고정되어 있다고 가정하고, 다음과 같이 생각할 수 있습니다.

$$E(Y-\hat Y) = E[f(X)+\epsilon - \hat f(X)]^2 \\ \quad\quad\quad\quad\:=[f(X)-\hat f(X)]^2 +Var(\epsilon)   \tag{2.3}$$

$E(Y-\hat Y)^2$은 예측 값과 실제 값 사이 차이의 제곱에 대한 평균 혹은 기댓값이며, $Var(\epsilon) $은 오차항 $\epsilon$에 대한 분산을 나타냅니다.

이 책은 reducible error를 최소화 하기위해 $f$를 추정하는 기술에 초점을 맞춥니다.

irreducible error는 항상 $Y$에 대한 예측 정확도의 상한을 제공한다는 점을 명심해야합니다. 이 경계는 실제로 거의 알려지지 않았습니다.



**Inference**

우리는 종종 $Y$가 $X_1,...,X_p$의 변화에 영향을 받는 방식을 이해하는 데 관심이 있습니다.

이 상황에서 우리는 $f$를 추정하고 싶지만 우리의 목표는 반드시 $Y$를 예측하는 것은 아닙니다.

대신 우리는 $X$와 $Y$의 관계를 이해하거나 보다 구체적으로 $Y$가 $X_1,...,X_p$의 함수로 어떻게 변하는지 이해하려고 합니다. 



이제 $\hat f$는 정확한 형태를 알아야 하므로 블랙 박스로 취급할 수 없습니다.

이 설정에서는 다음 질문에 답하는 데 관심이 있을 수 있습니다.

- Which predictors are associated with the response?

    사용 가능한 입력 변수 중 극히 일부만이 $Y$와 실질적으로 연관되는 경우가 많습니다.

    많은 가능한 변수 집합 중에서 몇 가지 중요한 예측 변수를 식별하는 것은 경우에 따라 매우 유용할 수 있습니다.

- What is the relationship between the response and each predictor?

    일부 입력 변수는 입력 변수를 증가시키면 $Y$값이 증가하는 것과 관련이 있다는 점에서 양의 상관관계를 가질 수 있습니다. 다른 변수들은 반대의 관계를 가질 수 있습니다.

    $f$의 복잡도에 따라 반응과 주어진 입력 변수간의 관계는 다른 입력 변수의 값에 따라 달라질 수 있습니다.

- Can the relationship between Y and each predictor be adequately summarized using  a linear equation or is the relationship more complicated?

    역사적으로 $f$를 추정하는 대부분의 방법은 선형을 사용했습니다. 특정 상황에서는 그러한 가정이 합리적이거나 심지어 바람직합니다. 하지만 종종 실제 관계는 보다 복잡해서 선형 모델로는 입력 변수와 출력 변수사이의 관계를 정확하게 제공하는 것이 불가능합니다.



이 책에서는 예측 설정, 추론 설정 혹은 이 둘의 조합에 해당하는 여러 예시를 볼 수 있습니다.

예를 들어, 직접 마케팅 캠페인을 수행하는데 관심이 있는 회사를 생각해봅시다.

목표는 개별로 측정된 인구 통계학적 변수의 관찰을 기반으로 메일에 대하여 긍정적으로 응답할 개인을 식별하는 것입니다.

이 경우 인구 통계학적 변수는 입력 변수 역할을 하고, 마케팅 캠페인에 대한 반응(긍정 or 부정)은 결과에 해당합니다.

회사는 각 개별 입력 변수와 응답 간의 관계를 깊이 이해하는 것에 관심이 없습니다.

대신 회사는 입력 변수를 사용하여 반응을 예측하는 정확한 모델을 원합니다. 이 것이 바로 예측 모델링의 예시입니다.



이와 반대로 가장 처음 고려했던 광고 데이터셋에 대해 생각해봅시다. 

다음과 같은 질문에 대해 관심이 있을 수 있습니다.

- Which media contribute to sales?
- Which media generate the biggest boost in sales?
- How much increase in sales is associated with a given increase in TV advertising?

이런 상황은 추론 영역에 속합니다.



또 다른 예시는 가격, 매장 위치, 할인 수준, 경쟁 가격 등과 같은 변수를 기반으로 고객이 구매할 수 있는 제품의 브랜드를 모델링 하는 것입니다.

이런 상황에서 가장 관심있는 부분은 ''각각의 변수가 구매 확률에 얼마나 영향을 미치는지'' 입니다.

예를 들어, 제품 가격 변경이 판매에 어떤 영향을 미칠까요? 이것은 추론을 위한 모델의 예시입니다.



몇몇 모델은 예측과 추론 두 가지에 관심이 있을 수 있습니다.

예를들어 부동산 환경에서 주택 가치를 범죄율, 구역 설정, 강으로부터의 거리, 대기 질, 학교, 지역 사회 소득 수준, 주택 크기 등과 같은 입력과 연관시키려고 할 수 있습니다. 

이 경우 개별 입력 변수가 가격에 어떤 영향을 미치는지 관심이 있을 수 있습니다. 즉, 강이 보이는 경우 주택 가치가 얼마나 될까요? 또는 주택 특성을 감안할 때 주택 가치를 예측하는 데 관심이 있을 수 있습니다.

궁극적인 목표가 예측인지, 추론인지 또는 두 가지의 조합인지에 따라 $f$를 추정하는 다른 방법이 적절할 수 있습니다.

예를 들어, 선형 모델은 비교적 단순하고 예측 가능한 추론인 반면에, 다른 접근 방식 만큼 정확한 예측을 산출하지 못할 수 있습니다.



### How Do we Estimate f?

이 책 전체에서 $f$를 추정하기 위한 많은 선형 및 비선형 접근법을 탐구합니다.

이러한 방법은 일반적으로 특정 특성을 공유합니다. 이 섹션에서는 이러한 공유 특성에 대한 개요를 제공합니다.

우리는 항상 서로 다른 n개의 데이터 포인트를 관찰했다고 가정합니다.

이런 관측 값을 훈련 데이터 라고 합니다. $f$를 추정하는 방법을 훈련하거나 가르치기 위해 이러한 훈련 데이터 관측 값을 사용하기 때문.

$x_{ij}$가 관측 치 $i = 1,2,...,n$에 대한 $j = 1,2,...,p$번 째 예측 변수 또는 입력 값을 나타내도록 합니다.

따라서 $y_i$는 $i$번 째 관측치에 대한 반응 변수를 나타냅니다.

그러므로 우리의 훈련 데이터는 다음과 같이 구성됩니다.

$$(x_1,y_1),(x_2,y_2),...,(x_n,y_n)$$

$$where\:\: x_i= (x_{i1},x_{i2},...,x_{ip})^T$$

우리 목적은 $f$를 추정하기 위해서 통계적 학습 방법을 훈련 데이터에 적용하는 것입니다.

다시말해서 우리는 모든 관측치 $(X,Y)$에 대해 $ Y\approx \hat f(X)$ 를 만족하는 함수 $\hat f$를 찾고 싶은 것입니다.

일반적으로 이 작업에 대한 대부분의 통계적 학습 방법은 parametric or non-parametric으로 특징지을 수 있습니다.



**Parametric Method**

파라미터릭 방법은 두 가지 단계의 모델 기반 접근방법을 포함합니다.

1. 함수의 $f$의 형태나 모양에 대해 가정합니다. 예를들어 아주 간단한 $f$에 대한 가정은 X에 대해 선형으로 나타내는 것입니다.

    $$f(X) = \beta_0 + \beta_1X_1 + \beta_2X_2+\cdot\cdot\cdot + \beta_pX_p \tag{2.4}$$

    이 것이 3장에서 광범위하게 다룰 선형 모델 입니다. $f$가 선형이라고 가정하면 $f$ 추정 문제는 크게 단순화됩니다. 임의의 $p$차원 함수 $f(X)$를 추정하는 대신 $p$+1개의 계수 $\beta_0,...,\beta_p$를 추정합니다.

2. 모델이 선택 되면, 훈련 데이터를 이용하여 모델을 학습합니다. (2.4)와 같이 선형 모델인 경우 파라미터를 추정합니다. 이러한 파라미터의 값을 찾으려면 다음과 같은 식을 따릅니다.

    $$Y\approx \beta_0 + \beta_1X_1 + \beta_2X_2 + \cdot\cdot\cdot + \beta_pX_p$$

    모델 (2.4)를 피팅하는 가장 일반적인 접근 방식은 최소 제곱법이라 불리며 3장에서 다룹니다. 최소 제곱법 또한 선형 모델을 피팅하는 많은 방법 중 하나입니다. 6장에서는 파라미터를 추정하는 다른 접근 방식에 대해 다룹니다.

위에서 설명한 모델 기반 접근 방식을 파라미터릭이라고 합니다. $f$를 추정하는 문제를 파라미터 셋 추정 중 하나로 줄입니다.

일반적으로 파라미터 집합을 추정하는 것이 훨씬 더 쉽습니다.

이 방식의 잠재적인 단점은 우리가 선택한 모델이 일반적으로 $f$의 알 수 없는 실제 형식과 일치하지 않는다는 것입니다. 선택한 모델이 실제 $f$와 너무 멀면 추정치가 좋지 않습니다. $f$에 대해 다양한 기능적 현태에 맞는 유연한 모델을 선택하여 문제를 해결할 수 있습니다.

하지만 일반적으로 유연한 모델을 피팅하려면 더 많은 수의 파라미터를 추정해야 합니다. 

이러한 더 복잡한 모델은 데이터를 오버피팅 시킬 수 잇으며 이는 본질적으로 오버피팅이 오류 또는 노이즈를 너무 가깝게 따라 간다는 것을 의미합니다.

아래의 그림은 앞서 앞선 소득 데이터에 파라미터릭 방식을 적용한 예시입니다.



![2.4](/Users/devcat/Desktop/islr/2.4.png)



우리는 선형 모델을 다음 형태를 통해 피팅합니다.

$$income = \beta_0+\beta_1\times education+\beta_1\times seniority$$

