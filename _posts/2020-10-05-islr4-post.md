---
layout: post
title: ISLR chapter.4
author: Jaeheon Kwon
categories: Ai
tags: [islr]
---



 3장에서 다룬 내용들은 반응 변수 $Y$가 정량적인 경우 였습니다. 하지만 반응 변수가 정성적인 경우도 많이 존재합니다. 예를 들면 눈의 색은 파란색, 갈색, 초록색 등 정성적으로 표현할 수 있습니다. 종종 이런 정성적인 변수를 $categorical$이라고 부릅니다.

이번 챕터에서는 $classification$으로 알려진 정성적인 반응을 예측하는 방법에 대해 알아봅니다.



## 4.1 An Overview of Classification

회귀에서와 마찬가지로 분류에서도 분류기를 만드는데 사용할 수 있는 일련의 관측치가 존재합니다. 우리는 분류기가 트레이닝 데이터 뿐만 아니라 테스트에서도 잘 동작하기를 원합니다.

이번 챕터에서는 시뮬레이션된 체납 데이터 셋을 사용한 분류 개념을 설명합니다.

우리는 연간 수입과 월별 신용카드 잔액을 기준으로 개인이 신용카드 비용을 체납할 것인지 여부를 예측하는데 관심이 있습니다.

<img src = "https://py-tonic.github.io/images/islr/4.1.png">

위 그림은 연간 수입과 카드 잔액에 대한 데이터의 일부를 나타낸 것입니다.

왼쪽 패널에는 주어진 달에 체납한 사람은 주황색, 그렇지 않은 사람은 파란색으로 나타냈습니다. 전체 체납률은 약 3% 이므로 체납을 하지 않은 사람중 극히 일부만 표시했습니다. 체납한 사람은 그렇지 않은 사람보다 신용카드 잔액이 더 많은 경향이 있습니다.

위 그림은 예측 변수 balance와 체납 사이에 매우 뚜렷한 관계를 나타낸다는 점에 주목해야 합니다. 실제 데이터에서는 이런 뚜렷한 관계가 잘 나타나지 않습니다. 그러나 이 챕터에서는 분류를 설명하기 위해 예측 변수와 반응 변수의 관계가 다소 과장된 예시를 사용합니다.



## 4.2 Why Not Linear Regression?

우리는 선형 회귀가 정성적 반응의 경우 적절한 케이스가 아니라고 얘기했습니다. 왜 그럴까요?

응급실에 있는 환자의 증상에 근거하여 환자의 상태를 예측한다고 가정합시다. 이 단순화된 예시에서는 뇌줄중, 약물 과다복용, 간질 발작의 세 가지 가능한 진단이 존재합니다. 이 세 가지 케이스를 정량적 반응 변수 $Y$로 인코딩 하면 다음과 같습니다.

$$Y = \begin{cases} 1\quad if\ stroke\\              2 \quad if\ drug\ overdose \\ 3\quad if\ epileptic\ seizure\end{cases}$$

이 코딩을 사용하면 예측 변수에 기반하여 $Y$를 예측하는 선형 회귀 모델을 피팅시키는데 최소 제곱법을 사용할 수 있습니다.

불행히도, 이 코딩은 뇌졸중과 발작 사이에 약물 과다 복용이 있고, 뇌졸중과 약물 과다 복용의 차이가 약물 과다 복용과 간질 발작의 차이와 같다고 주장합니다.

실제로 위와 같은 순서는 특별한 의미가 없습니다. 

반응 변수의 값이 경도, 중간, 심각과 같은 자연적인 순서를 따르고 변수 사이의 차이가 유사하다면 1, 2, 3으로 인코딩 하는 것이 합리적입니다.

일반적으로 세 수준 이상의 정성적 반응 변수를 선형 회귀에 대비한 정량적 반응으로 변환할 수 있는 자연스로운 방법은 없습니다.

바이너리의 경우는 더 낫습니다. 예를 들어, 환자의 상태가 두 가지만 존재한다면 (뇌졸중, 과다복용) 우리는 회귀 때 처럼 더미 변수 방식으로 접근할 수 있습니다.

$$Y = \begin{cases} 0\quad if\ stroke\\              1 \quad if\ drug\ overdose \end{cases}$$

그런 다음 선형 회귀에 적합 시키고 $\hat Y>0.5$이면 과다복용, 그렇지 않으면 뇌졸중이라고 예측할 수 있습니다. 위와 같이 0/1 코딩이 있는 바이너리의 경우 최소 제곱에 의한 회귀는 의미가 있습니다. 선형 회귀 분석을 사용하여 얻은 $X\hat \beta$가 $Pr(drug\ overdose\vert X)$의 추정치임을 알 수 있습니다.

그러나 선형 회귀 분석을 사용할 경우 추정치의 일부가 [0,1] 구간을 벗어날 수 있으므로 확률로 해석하기 어렵습니다. 그럼에도 불구하고 예측은 순서를 제공하며 대략적인 확률 추정치로 해석될 수 있습니다.

그러나 더미 변수 접근 방식은 세 수준 이상의 정성적 반응을 수용하도록 쉽게 확장할 수 없습니다. 이러한 이유로 다음에 제시되는 것과 같이 정성적 반응 값에 적합한 분류 방법을 사용하는 것이 바람직합니다.



## 4.3 Logistic Regression

다시 체납 데이터셋으로 돌아가봅시다. 반응 변수는 두 가지 카테고리를 갖습니다.(Yes or no) 이 반응 $Y$를 직접 모델화 하는 대신, 로지스틱 회귀에서는 $Y$가 특정 범주에 속할 확률을 모델화합니다.

예를 들어, balance가 주어졌을 때 체납의 확률은 다음과 같습니다.

$$Pr(default = Yes\vert balance)$$



### 4.3.1 The Logistic Model

$p(X) = Pr(Y=1\vert X)$와 $X$의 관계를 어떻게 모델링해야 할까요?

섹션 4.2에서 우리는 선형 회귀 모델을 사용하여 확률을 표현하는 방법에 대해 얘기했습니다.

$$p(X) = \beta_0 +\beta_1X\tag{4.1}$$

<img src = "https://py-tonic.github.io/images/4.2.png">



만약 우리가 이 접근법을 사용하여 체납 예측을 한다면 우리는 위 그림의 왼쪽 패널에 표시된 모델을 얻습니다. 여기서 우리는 문제점을 발견할 수 있습니다. balance가 0에 가까우면 우리는 음의 확률을 예측합니다. 또한 우리가 매우 큰 balance에 대해 예측한다면 1보다 큰 값을 얻게 됩니다.

물론 신용카드 잔액과 관계없이 체납의 진정한 확률은 [0,1]에 있어야 하기 때문에 이러한 예측은 타당하지 않습니다. 이 문제는 현재 데이터셋에서만 발생하는 문제가 아닙니다. 바이너리 반응 변수에 대한 직선은 언제나 $p(X)<0,\ p(X)>1$을 가질 수 있습니다.

이런 문제를 피하기 위해 우리는 모델 $p(X)$의 주어진 아웃풋을 [0,1]값으로 만들어야 합니다. 로지스틱 회귀에서 우리는 로지스틱 함수를 만듭니다.

$$p(X) = \frac{e^{\beta_0+\beta_1X}}{1+e^{\beta_0+\beta_1X}} \tag{4.2}$$



> 위 표현보다는 $\sigma = \frac{1}{1+e^{-y}}$형태가 더 익숙하죠? 
>
> 위 분수를 분자로 나눠주면 시그모이드 함수가 됩니다.

모델 4.2를 피팅시키기 위해 MLE를 사용하며 다음 섹션에서 이를 설명합니다.

오른쪽 패널이 로지스틱 회귀 모델을 데이터에 피팅시킨 것을 보여줍니다. 이제 balance가 낮을 경우 확률은 0에 가깝지만 0아래로 내려가진 않습니다. 높은 balance의 경우 1에 가깝지만 1을 넘지는 않습니다. 

로지스틱 함수는 S자 모양을 가집니다. 위 식을 약간 조정해서 다음과 같이 쓸 수 있습니다.

$$\frac{p(X)}{1-p(X)} = e^{\beta_0+\beta_1X}\tag{4.3}$$

$p(X)/[1-p(X)]$를 $odds$ 라고 부르고 값은 $[0,\infty]$ 의 구간을 갖습니다. 이는 매우 낮고, 높은 체납 확률을 나타냅니다. 승산은 일반적으로 경마에서 확률 대신 사용됩니다. 경마에서 승산은 올바른 베팅 전략과 더 자연스럽게 연관됩니다.

> 위 식의 직관적 의미는 사건 X가 발생하지 않을 확률 대비 발생할 확률입니다.
>
> 사건이 발생할 확률이 1에 가까울 수록 승산은 무한대로 올라갑니다.

식 4.3을 다음과 같이 조정할 수 있습니다.

$$log(\frac{p(X)}{1-p(X)}) = \beta_0+\beta_1X \tag{4.4}$$ 

식의 왼쪽 부분을 $log-odds\ or\ logit$이라고 부릅니다. 로지스틱 회귀 모델 4.2는 X에 선형인 logit이 있음을 알 수 있습니다.

3장으로 돌아가봅시다. 선형 회귀 모델에서 $\beta_1$은 $X$의 단일 단위 증가와 관련된 $Y$의 평균 변화를 나타냅니다. 대조적으로 로지스틱 회귀 모형에서 X의 단위 유닛 증가는 로그 승산이 $\beta_1$으로 변경되거나, 동등하게 $e^{\beta_1}$로 승산이 곱해집니다.

그러나, 4.2에서 $p(X)$와 $X$의 관계는 직선이 아니기 때문에 $\beta_1$이 $X$의 단일 유닛과 관련된 $p(X)$의 변화에 해당하지는 않습니다. 선형적인 관계도 없고, $X$의 단위 변경당 $p(X)$의 변화율이 현재 값에 따라 달라집니다. (위 그림의 오른쪽 패널 참조)



### 4.3.2 Estimating the Regression Coefficients

식 4.2의 계수 $\beta_0,\beta_1$은 알려지지 않았으므로 훈련 데이터를 통해 추정해야 합니다.

3장에서는 최소 제곱법을 통해 선형 회귀 계수를 추정했습니다. (비선형) 최소 제곱법을 사용하여 모델 4.4를 피팅시킬 수 있지만, 더 나은 통계 특성을 가지는 MLE가 더 선호됩니다.

MLE를 사용하여 로지스틱 회귀 모델을 피팅시키는 직관은 다음과 같습니다.

$\beta_0,\beta_1$에 대한 추정치를 구하고 식 4.2를 사용하여 각 개인에 대한 체납의 예측 확률 $\hat p(x_i)$ 관측된 체납 상태에 가능한 가깝게 일치하도록 해야 합니다.

즉, 이러한 추정치를 4.2에 주어진 $p(X)$모델에 연결하면 체납한 모든 사람에 대해 1에 가까운 숫자를 산출하고, 그렇지 않은 사람에게는 0에 가까운 숫자를 산출할 수 있는 $\hat \beta_0,\hat\beta_1$을 찾으려고 합니다.

이 직관은 가능도 함수라고 불리는 수학 방정식을 사용하여 공식화 할 수 있습니다.

$$\mathcal l(\beta_0,\beta_1) = \prod\limits_{i:y_i=1}p(x_i)\prod\limits_{i':y_{i'}=0}(1-p(x_{i'}))\tag{4.5}$$

$\beta_0,\beta_1$은 위 가능도 함수를 최대화 하는 추정치를 선택하면 됩니다. MLE는 이 책에서 다룰 많은 비선형 모델을 피팅시키는 일반적인 접근 방식입니다. 선형회귀에서 최소제곱법 또한 MLE의 특별한 케이스입니다.

> 로지스틱 회귀는 반응 변수 Y가 이항 분포를 따른다고 가정함 $Y\sim Bn(n,p)$
>
> 원래는 이항분포라면 nCr텀이 있어야 하는데 실제 데이터의 시행 횟수 n=1이기 때문에 식4.5처럼 베르누이 분포에 대한 likelihood로 나타낼 수 있음.
>
> 위 식을 log-likelihood로 바꾸면 바이너리 크로스엔트로피가 나오는데 이 함수는 베타에 대해 비선형이기 때문에 선형 회귀때 처럼 명시적인 해가 존재하지 않아서 GD같은 방법으로 해를 구해야함.



<img src = "https://py-tonic.github.io/images/islr/t4.1.png">



balance 변수를 사용하여 체납한 사람의 확률을 로지스틱 회귀 모델을 사용하여 예측한 결과에 대한 정보를 위 테이블에서 볼 수 있습니다. $\hat \beta_1 = 0.0055$ 이므로 잔액의 증가가 체납 확률의 증가와 관련이 있음을 나타냅니다.

위 테이블에 나온 것들을 보면 우리가 3장에서 다뤘던 것과 크게 다르지 않습니다.

Z-statistic을 통해 가설 검정을 할 수 있고 여기서 귀무가설은 동일하게 $H_0:\beta_1=0$입니다. 이 뜻은 $p(X) = \frac{e^{\beta_0}}{1+e^{\beta_0}}$가 되어 balance가 체납의 확률에 영향을 미치지 않는다는 의미로 해석할 수 있습니다.

물론 여기선 $p-value$가 매우 작으므로 귀무가설을 기각합니다. 테이블의 추정 절편은 일반적으론 관심이 없습니다. 주된 목적은 ㄷ이터에서 평균 피팅 확률을 비율에 맞게 조정하는 것입니다.



### 4.3.3 Making Predictions

일단 계수가 추정되면, 주어진 신용카드 잔액의 체납 확률을 계산하는 것은 간단한 일입니다.

예를 들어, 위 테이블에서 주어진 계수 추정치를 사용하여 balance=$1,000일 때 체납 확률을 계산하는 식은 다음과 같습니다.

$$\hat p(X)=\frac{e^{\hat\beta_0+\hat\beta_1X}}{1+e^{\hat\beta_0+\hat\beta_1X}}=0.00576$$

이 값은 1%도 되지 않습니다. balance=$2,000일 때 체납 확률은 0.586으로 훨씬 높은 58.6%에 해당합니다.

앞선 3장에서 다뤘던 더미 변수를 포함하는 모델을 생각해서 예측하는 문제를 생각해봅시다. 학생이면 1 학생이 아니면 0입니다. 아래 테이블은 더미 변수가 긍정일 때와 관련된 로지스틱 회귀 모델의 결과를 보여줍니다.

<img src = "https://py-tonic.github.io/images/islr/t4.2.png">



$p-value$가 크므로 이 변수는 유의미합니다. 학생인 경우에 체납 확률이 더 높은 것을 확인할 수 있습니다.

$$\hat{Pr}(Y=Yes\vert student=Yes) = \frac{e^{-3.5041+0.4049\times 1}}{1+e^{-3.5041+0.4049\times 1}} = 0.0431\\ \hat{Pr}(Y=Yes\vert student=No)=\frac{e^{-3.5041+0.4049\times 0}}{1+e^{-3.5041+0.4049\times 0}} = 0.0292$$



### 4.3.4 Multiple Logistic Regression

바이너리 반응에 대해 이제 여러개의 예측 변수를 사용하여 다중 로지스틱 회귀로 확장할 수 있습니다. 3장에서 했던 것 처럼 식 4.4를 다음과 같이 일반화 할 수 있습니다.

$$log(\frac{p(X)}{1-p(X)})=\beta_0+\beta_1X_1+\cdot\cdot\cdot+\beta_pX_p\tag{4.6}$$

$X=(X_1,...,X_p)$인 $p$개의 예측 변수 이고 식 4.6은 다음과 같이 다시 쓸 수 있습니다.

$$p(X)=\frac{e^{\beta_0+\beta_1X_1+\cdot\cdot\cdot+\beta_pX_p}}{1+e^{\beta_0+\beta_1X_1+\cdot\cdot\cdot+\beta_pX_p}}\tag{4.7}$$



섹션 4.3.2에서 했던 것 처럼 MLE를 사용하여 $\beta$를 추정할 수 있습니다.

<img src = "https://py-tonic.github.io/images/islr/t4.3.png">



위 테이블은 예측 변수 balance, income, student에 대한 체납의 확률을 예측한 로지스틱 회귀 모델의 계수 추정을 보여줍니다.

여기에 놀랄만한 사실이 존재합니다. balance와 더미 변수 student의 $p-value$가 매우 작고 이는 체납의 확률과 변수가 관련이 있다는 사실을 나타냅니다. 그러나 더미 변수의 계수는 음수이기 때문에 학생이 비 학생에 비해 체납 확률이 낮음을 나타냅니다. (반대로 더미 변수의 계수가 테이블 4.2에서는 양수였죠.)

어떻게 이게 가능할까요? 우선 아래 그림의 왼쪽 패널을 살펴봅시다.

<img src = "https://py-tonic.github.io/images/islr/4.3.png">



주황색, 파란색 실선은 신용카드 잔액에 대한 함수로 학생과 비학생 각각의 평균 체납 비율을 나타냅니다. 다중 로지스틱 회귀에서 학생 변수에 대한 계수가 음수라는 것은 고정된 balance, income에 대해 학생이 비학생에 비해 체납 가능성이 낮다는 것을 나타냅니다.

실제로, 우리는 왼쪽 패널에서 학생 체납 비율이 모든 balance에 대해 비학생 아래라는 것을 볼 수 있습니다. 그러나 모든 balance와 소득에 대해 평균을 낸 학생과 비학생의 체납 비율을 보여주는 그래프 아래의 점선은 반대 효과를 보여줍니다.

전체 학생의 체납률이 비 학생의 체납률 보다 높습니다. 따라서 테이블 4.2에 나타난 단일 변수 로지스틱 회귀 분석의 아웃풋에서 학생에 대한 계수는 양수였습니다.

오른쪽 패널이 이러한 불일치에 대한 설명을 제공합니다. 학생과 밸런스 변수는 상관관계를 가집니다. 학생들은 더 높은 수준의 부채를 갖는 경향이 있는데 이것은 더 높은 체납률과 관련이 있습니다. 즉, 학생들은 신용카드 잔액이 클 가능성이 높으며, 왼쪽 패널에서 알 수 있듯 높은 체납률과 연관되는 경향이 있습니다.

따라서 주어진 신용카드 잔액을 가진 개별 학생이 동일한 신용카드 잔액을 가진 비 학생보다 체납률이 낮은 경향이 있지만 전체 학생이 신용카드 잔액이 더 높은 경향이 있다는 사실은 전반적으로 학생들은 비 학생들보다 더 높은 비율로 체납하는 경향이 있습니다.

학생의 신용카드 잔액에 대한 정보가 없는 경우 학생은 비 학생보다 위험합니다.(체납할 확률이 높음) 그러나 그 학생은 동일한 신용카드 잔액을 가진 비학생 보다는 덜 위험합니다.

선형 회귀 설정에서와 같이 한 예측 변수를 사용하여 얻은 결과는 여러 예측 변수를 사용하여 얻은 결과와 매우 다를 수 있으며, 특히 예측 변수 사이에 상관관계가 있을 경우 더욱 그렇습니다.

일반적으로 위 그림에서 보이는 현상을 교란(confounding)이라고 합니다.



### 4.3.5 Logistic Regression for >2 Response Classes

4.2에서 다뤘던 질병 클래스 분류처럼 2 개 이상의 클래스에 대해 로지스틱 회귀를 생각해 볼 수 있습니다. 

앞의 섹션에서 논의한 바이너리 로지스틱 회귀 모델은 멀티 클래스의 확장이 가능하지만 실제로는 이 모델을 자주 사용하지 않는 경향이 있습니다. 그 이유중 하나는 다음 섹션에서 논의할 방법인 판별 분석이 멀티 클래스 분류에 인기 있기 때문입니다. 우리는 여기서 멀티 클래스 로지스틱 회귀에 대한 세부 사항을 다루지 않고 단지 그런 접근법이 가능하다는 것과 R과 같은 소프트웨어로 가능하다는 것에 주목합니다.



## 4.4 Linear Discriminant Analysis

로지스틱 회귀는 로지스틱 함수를 사용하여 $Pr(Y=k\vert X=x)$를 직접 모델링 하는 것을 포함합니다. 4.7은 두 개의 반응 클래스의 경우에 대한 식입니다. 
통계 용어로는 예측 변수 $X$를 고려하여 반응 변수 $Y$의 조건부 분포를 모델링합니다.

이제 우리는 이러한 확률을 추정하기 위한 대안적이고 덜 직접적인 접근법을 고려합니다. 이 대안적 접근법에서는 각 반응 클래스(i.e. given $Y$)에서 예측 변수 $X$의 분포를 별도로 모델링 한 다음 베이즈 정리를 사용하여 $Pr(Y=k\vert X=x)$에 대한 추정치로 전환합니다.

이러한 분포를 정규분포로 가정할 때의 모델이 로지스틱 회귀와 매우 유사한것으로 알려져있습니다.

로지스틱 회귀가 있는데 왜 다른 방법이 필요한가요? 몇 가지 이유가 존재합니다.

- 클래스가 잘 분리되어 있을때 파라미터 추정치가 불안정합니다. LDA는 이 문제를 겪지 않습니다.
- $n$이 작고 예측 변수 $X$의 분포가 각 클래스에서 근사적으로 정규 분포를 따르는 경우 LDA는 로지스틱 회귀 모델 보다 안정적입니다.
- 4.3.5에 언급 했듯 LDA가 세 개 이상의 반응 클래스를 가질 때 더 인기있습니다.



### 4.4.1 Using Bayes' Theorem for Classification

$(K\geq2)$에 대해 관측치를 $K$개의 클래스중 하나로 분류하고 싶다고 가정해봅시다.

즉, 정성적 반응 변수 $Y$는 $K$개의 구별 가능하며 순서가 없는 값을 가질 수 있습니다.

 $\pi_k$ 는 랜덤하게 선택한 관측치가 $k$번째 클래스에서 나올 전체 혹은 사전 확률을 나타냅니다.

이것은 주어진 관측치가 반응 변수 $Y$의 $k$번째 클래스와 연관될 확률입니다. $f_k(x)=Pr(X=x\vert Y=k)$는 $k$번째 클래스에서 나온 관측치에 대한 $X$의 밀도 함수를 나타냅니다. 

즉, $k$번째 클래스의 관측치가 $X\approx x$일 확률이 높으면 $f_k(x)$가 상대적으로 크며, $k$번 째 클래스의 관츠깇가 $X\approx x$일 확률이 낮으면 $f_k(x)$가 작습니다.

베이즈 이론을 통해 다음과 같이 식을 정의할 수 있습니다. 

$$Pr(Y=k\vert X=x)=\frac{\pi_kf_k(x)}{\sum_{l=1}^K\pi_lf_l(x)}\tag{4.10}$$

> 좀 복잡한데...
>
> 그냥 $p(k\vert x) = \frac{p(x\vert k)p(k)}{p(x)}$ 랑 형태는 비슷하다.

앞선 표기법에 따라 $p_k(X)=Pr(Y=k\vert X)$를 사용하겠습니다.

이는 4.3.1절과 같이 $p_k(X)$를 직접 계산하는 대신에 $\pi_k$(prior)와 $f_k(X)$(likelihood)의 추정치를 4.10에 간단히 연결할 수 있음을 시사합니다.

일반적으로 $Ys$의 모집단에서 랜덤 샘플링한 경우 $\pi_k$를 추정하는 것은 쉽습니다. 우리는 단지 $k$번째 클래스에 속하는 훈련 관측치의 비율만 계산합니다. 그러나 밀도에 대해 간단한 형태를 가정하지 않는 한  $f_k(X)$를 추정하는 것이 더 어렵습니다.

$p_k(X)$를 사후확률이라고 부르며 이는 관측치 $X=x$가 $k$번째 클래스에 속할 확률입니다.

우리는 4.10에 정의된 모든 용어가 정확히 명시되어 있는 경우 베이즈 분류기가 모든 분류기 중에서 가장 낮은 오류율을 가진다는 것을 2장에서 살펴봤습니다. 따라서 $f_k(x)$를 추정할 수 있는 방법을 찾을 수 있다면 베이즈 분류기에 근접한 분류기를 개발할 수 있습니다.



### 4.4.2 Linear Discriminant Analysis for p=1

p=1이라는 것은 오직 하나의 예측 변수만 존재한다고 가정하는 것입니다.

우리는 사후확률을 추정하기 위해서 $f_k(x)$에 대한 추정을 얻고 싶습니다. 그리고 관측치를 사후확률이 가장 큰 클래스로 분류합니다. 가능도를 추정하기 위해서 우리는 이 형태에 대해 가정합니다.

가능도가 정규분포를 따른다고 가정합시다.

$$f_k(x) = \frac1{\sqrt{2\pi}\sigma_k}exp(-\frac1{2\sigma^2_k}(x-\mu_k)^2)\tag{4.11}$$

$K$개의 클래스에 대해 분산 항을 공유한다고 가정해봅시다. $\sigma_1^2 =...=\sigma_K^2$ 식 4.11을 4.10에 연결하면 다음과 같습니다.

$p_k(x) = \frac{\pi_k\frac1{\sqrt{2\pi}\sigma}exp(-\frac1{2\sigma^2}(x-\mu_k)^2)}{\sum_{l=1}^K\pi_l\frac1{\sqrt{2\pi}\sigma}exp(-\frac1{2\sigma^2}(x-\mu_l)^2)} \tag{4.12}$

위 식의 양변에 로그를 씌워 봅시다.

$$\delta_k(x) = x\cdot\frac{\mu_k}{\sigma^2} - \frac{\mu_k^2}{2\sigma^2}+log(\pi_k) \tag{4.13}$$

즉, 정규분포와 등분산을 가정햇을 때 공통 분산 $\sigma^2$과 각 클래스에 대한 파라미터$\mu_k$를 알고 있다면 정확한 베이즈 분류기를 구할 수 있습니다. LDA는 이 두 파라미터를 추정하여(정규분포니까 두 개) 베이즈 부류기에 근사하는 것입니다.

파라미터는 다음과 같이 추정합니다.

$$\hat\mu_k = \frac1{n_k}\sum\limits_{i:y_i=k}x_i \\ \hat\sigma^2 = \frac1{n-K}\sum\limits_{k=1}^K\sum\limits_{i:y_i=k}(x_i-\hat\mu_k)^2 \tag{4.15}$$

$\hat\mu_k$는 샘플 평균이고, 공통 분산은 편차 제곱에 자유도(K개의 $\hat\mu$)를 나눠준 형태입니다.

사전 확률인 $\pi_k$는 앞서 말했듯 간단하게 추정할 수 있습니다.

$$\hat\pi_k = n_k/n\tag{4.16}$$

$k$개의 클래스 데이터 수/ 전체 데이터 수 입니다. 이제 사전확률, 가능도를 구했으니 식4.13을 최대로 만드는 클래스 K를 찾으면 됩니다.

> Linear Discriminant라는 명칭은 최종 함수인 $\hat\delta_k(x)$(decision boundary)가 x에 대한 선형 식으로 나오기 때문입니다.



<img src = "https://py-tonic.github.io/images/islr/4.4.png">

검은색 실선이 결정 경계 입니다.

이 경우 $n_1=n_2=20$으로, $\hat\pi_1 =\hat\pi_2$입니다. 그 결과 결정 경계는 두 클래스의 표본 평균 사이의 중간 점인 $(\hat\mu_1+\hat\mu_2)/2$에 해당합니다.

LDA 결정 경계가 최적의 베이즈 결정 경계에 약간 왼쪽에 존재합니다. 이 데이터에 대해 LDA가 얼마나 잘 수행되는지 살펴봅시다.(시뮬레이션 데이터라 가능함.)

최저 에러율은 10.6%이고 LDA의 에러율은 11.1%입니다. 즉 LDA분류기의 에러율은 꽤 좋은 성과를 내는 것을 볼 수 있습니다.



### 4.4.3 Linear Discriminant Analysis for p>1

이제 다중 예측 변수의 경우로 LDA를 확장해봅시다. 이를 위해 $X=(X_1,X_2,...,X_p)$는 클래스 고유 평균 벡터(클래스마다 평균이 다름)와 공통 분산이 있는 다변량 가우스 분포에서 도출된다고 가정합니다.

<img src = "https://py-tonic.github.io/images/islr/4.5.png">

다변량 가우스 분포는 4.11과 같이 각 예측 변수가 예측변수 쌍 사이에 어느정도 상관관계가 있는 1차원 정규분포를 따른다고 가정합니다.

위 그림을 한쪽 축에서 보면 정규분포 모양입니다. 왼쪽 패널은 각 축의 분산이 같고 공분산이 0인 그림입니다(종 모양). 그러나 오른쪽 패널에서 처럼 예측 변수의 상관관계가 있거나 분산이 동일하지 않은 경우 종 모양이 왜곡돼어 종 밑면이 타원형으로 나타납니다.

다변량 정규 분포는 $X\sim N(\mu,\Sigma)$ 로 나타냅니다. $E(X)=\mu$는 $X$의 평균이고 ($p$개의 클래스 벡터) $Cov(X)=\Sigma$ 는 $p\times p$공분산 행렬입니다. 식으로 적으면 다음과 같습니다.

$$f(x) = \frac1{(2\pi)^{p/2}\vert\Sigma\vert^{1/2}}exp(-\frac12(x-\mu)^T\Sigma^{-1}(x-\mu))\tag{4.18}$$

앞선 섹션에서 다룬것과 동일하다 다음 식을 최대화 하는 $k$번째 클래스를 구하는 문제입니다.

$$\delta_k(x) = x^T\Sigma^{-1}\mu_k-\frac12\mu_k^T\Sigma^{-1}\mu_k+log\ \pi_k \tag{4.19}$$

<img src = "https://py-tonic.github.io/images/islr/4.6.png">

위의 다변량 정규분포를 하늘에서 본 것입니다.(예측 변수는 2개, 클래스는 3개)

왼쪽 패널은 $k=1,2,3$인 경우 $f_k(x)=P(X=(x1,x2)\vert Y=k)$를 추정하여 경계를 그린 것입니다. 각 원들은 각 분포의 95% 신뢰 구간을 나타냅니다. 오른쪽은 시뮬레이션된 데이터에 대해 베이즈 분류기와 비교한 것입니다. 마찬가지로 0.0746, 0.0770으로 비슷한 수준의 성능을 보입니다.



책의 예시로 돌아가서 체납을 예측하는 모델을 만들 때를 생각해봅시다. 10,000개의 데이터에 대해 피팅시킨 LDA는 2.75%의 오류를 보였습니다. 그러나 모델을 피팅 시킬때 오버피팅을 항상 염두해야합니다.

다행히 여기선 2개의 파라미터에 대한 추정이기 때문에 오버피팅은 관심 대상이 아닙니다. 그렇다면 다음 질문은 어떨까요?

- 전체 트레이닝 데이터 중 체납자의 비율이 몇%인가? 아무것도 하지 않은 null classifier에 비해 성능이 얼마나 좋은가?

실제 트레이닝 데이터중 체납자의 비율은 3.33%로 매우 적습니다. 그러니까 모두가 돈을 다 잘 냈다고 해도 오류율이 3.33%밖에 되질 않습니다. 이를 적나라하게 볼 수 있는 테이블이 아래와 같습니다.

<img src = "https://py-tonic.github.io/images/islr/t4.4.png">



LDA가 체납자라고 평가한 104명중 81명이 실제 체납자이므로 체납자라 평가한 사람들에 한해서는 잘 수행되고 있습니다. 그러나 실제 체납자 333명중 81명만을 체납자라고 평가했습니다 이것은 체납자중 252/333 = 75.7%를 놓친 것입니다.

이런 문제를 해결하기 위해 베이즈 분류기를 약간 수정해봅시다. 기존의 베이즈 분류기는 해당 클래스에 속할 확률이 가장 크면 그 클래스로 분류하는 비율이 50%였습니다. 

$$P(defalut=Yes\vert X=x)>0.5\tag{4.21}$$

그러나 현재 클래스의 비율이 불균형하므로 (체납자가 매우 많으므로) 이 비율을 조금 낮춰줍시다. 즉 확률이 0.2만 되어도 체납자라고 평가하는 것입니다 이에대한 결과는 아래 테이블에서 확인할 수 있습니다.

$$P(defalut=Yes\vert X=x)>0.2\tag{4.22}$$

<img src = "https://py-tonic.github.io/images/islr/t4.5.png">

이제 138/333 = 41.4%를 놓치긴 하지만 훨씬 나아졌습니다. 오류율은 3.73%로 늘어났지만 체납자를 잘 찾아내야하는 회사의 입장에선 훨씬 좋은 모델입니다.

이후 책에 나오는 PR-ROC에 대한 내용은 아래의 제 블로그 링크로 대체합니다.

[Model Evaluation](https://py-tonic.github.io/ai/2020/07/29/model_eval-post/)



### 4.4.4 Quadratic Discriminant Analysis

QDA는 LDA와 비슷하면서 다릅니다. 다변량 정규 분포를 가정하여 베이즈 이론을 사용한다는 점은 동일하지만, 동일한 공분산을 가정하지 않고 각 클래스 $k$마다 각각의 공분산을 가정합니다.

즉, $X\sim N(\mu_k,\Sigma_k)$를 가정합니다. 최종 결정 함수에서는 한가지 텀만 추가하면 됩니다. 식으로는 다음과 같습니다.

$$\delta_k(x) = -\frac12(x-\mu_k)T\Sigma^{-1}_k(x-\mu_k)-\frac12 log\vert\Sigma_k\vert+log\ \pi_k \\ \quad\quad\ = -\frac12x^T\Sigma_k^{-1}x+x^T\Sigma_k^{-1}\mu_k -\frac12\mu_k^T\Sigma_k^{-1}\mu_k -\frac12log\vert\Sigma_k\vert+log\ \pi_k\tag{4.23}$$

위 식이 가장 크게 나오는 $k$번째 클래스로 입력을 분류해줍니다. 이번에는 위 함수가 선형이 아니고 이차식 형태이기 때문에 Quadratic이라 불리고 결정 경계 또한 비선형입니다.

<img src = "https://py-tonic.github.io/images/islr/4.9.png">



왜 굳이 LDA, QDA로 나눴을까?

바로 bias-variance tradeoff 때문이라고 합니다. $p$개의 예측 변수에 대해 그들의 공분산을 추정하기 위해서는 p(p+1)/2개의 파라미터가 필요합니다. (pC2개)

이를 k개에 클래스에 대해 다른 분산으로 추정하려면 K*p(p+1)/2개의 파라미터를 추정하는 문제가 되고 훨씬 더 유연하며 분산이 높은 모델이라는 뜻입니다. 반대로 우리는 좀 덜 유연하더라도 높은 편향을 갖는 모델이 필요할 때가 있습니다.(트레이닝 데이터가 적을 때) 이때 LDA를 사용하는게 오버피팅을 피하면서 좋은 결론을 얻을 수 있습니다.

