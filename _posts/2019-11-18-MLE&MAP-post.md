---
layout: post
title: What is MLE?
author: Jaeheon Kwon
categories: Mathematics
tags: [statistics]
---

# Maximum Likelihood Estimation & Maximum a Posterior Estimation

동전 던지기 게임을 하려고합니다.<br>
(통상적으로 우리는 동전의 앞,뒷면이 나올 확률이 1/2임을 알고 있지만 모른다고 가정해봅시다. ex 여기는 현실과 다른 이세계라고 생각해봅시다!)<br>

총 5번을 던집니다.<br>
앞면이 3번, 뒷면이 2번나왔습니다.<br>
우리는 다음 동전을 던졌을 때 앞면이 나올 확률을 어떻게 예상할까요?<br>
위의 데이터만으로 추론했을 때 직관적으로 3/5라고 대답하셨나요?<br>
정답입니다. 그런데 왜 그런지 생각해본적이있으신가요?<br>
그 해답을 MLE로 해결할 수 있습니다.<br>

P(H) =  $θ$ (앞면이 나올 확률)<br>
P(T) = $1-θ$ (뒷면이 나올 확률)<br>
P(HHTHT) =  $θθ(1-θ)θ(1-θ) = θ^3 (1-θ)^2$ <br>

이런 식을 유도할 수 있습니다.<br>
D as Data = H,H,T,H,T 이고<br>
$n=5$,<br>
$k=a_H=3$ (앞면 나온 횟수)<br>
$p=θ$<br>

$θ$를 앞면이 나올 확률로 가정했을 때(Given) D가 관측될 확률은 다음과 같이 표현될 수 있습니다.<br>

$P(D|θ) = θ^{a_H}(1-θ)^{a_T}$ <br>
여기서 우리의 가설인 Data가 $θ$를 따른다 라는 것을 어떻게 해야 강력하게 만들 수 있을까요?<br>
$θ$를 최적화 하는게 그 해답입니다. 바로 MLE of $θ$가 됩니다.<br>
말로 풀어서하면 관측된 데이터들의 등장할 확률을 최대화 하는 $θ$를 찾아내는 것입니다.<br>

<br>
$\hat{θ} = argmax_θP(D|θ)$ <br>
파라미터를 구하기 위해 위에서 정의했던 식을 대입해보면<br>
$\hat{θ}$<br>
<br>
$ = argmax_θP(D|θ)$<br>
<br>
$= argmax_θθ^{a_H}(1-θ)^{a_T}$<br>

$= argmax_θln[θ^{a_H}(1-θ)^{a_T}]<br>$

$= argmax_θ[{a_H}lnθ + {a_T}ln(1-θ)]$<br>
이런식으로 나오는 것을 알 수 있습니다.<br>
이제 이 식을 미분하여 극값을 찾아보면,<br>
$d/dθ({a_H}lnθ + {a_T}ln(1-θ))$<br>

$a_H/θ - a_T/(1-θ) = 0$이 되고,<br>
$θ = a_H/a_H+a_T$ 임을 알 수 있습니다.<br>
즉 $θ$는 앞면이 나온 사건/ 전체사건 임을 알 수 있습니다.<br>
위에서 저희가 직관을 이용하여 추론한 결과와 같죠?<br>

MAP도 MLE와 크게 다르지 않습니다.<br>
이세계에서 동전의 앞면이 나올 확률을 MLE를 이용하여 통계적으로 잘 추론해서 3/5라는 결과값을 얻었습니다.<br>

그런데 우리는 현실세계에서 동전의 앞면이 나올확률이 1/2이고, <br>
단순히 표본이 작기때문에 3/5이지 않을까라는 의구심을 품을 수 있습니다.<br>
우리가 동전을 5번 던져서 앞면이 3번 나오는동안<br>
옆에있던 외계인이 50번던져서 앞면이 30번 나왔다고 합시다.<br>
그렇다면 여전히 3/5라는 확률이 맞을까요?<br>

우리가 의구심을품은 1/2이라는 확률을 사전정보로하여 파라미터를 추정할 순 없을까요?<br>
이러한 질문에서 시작한것이 MAP입니다.<br>

Bayes theorem에 의해<br>
$P(θ|D) = P(D|θ)P(θ)/P(D)$이고,<br>

Posterior = Likelihood x Prior Knowledge / Normalizing Constant 입니다.<br>
<br>
우리는 이미 $P(D|θ) = θ^{a_H}(1-θ)^{a_T}$ 임을 알고 있습니다.<br>
그럼 우리의 의구심을 Prior Knowledge로 이용하여 문제를 해결해봅시다.<br>
P(D)는 θ에 영향을 받지 않으므로 Constant 취급하겠습니다. 그럼 위의 Likelihood를 아래와 같이 변경할 수 있습니다.<br>
<br>
$P(θ|D) \propto P(D|θ)P(θ)$ <br>
<br>
$P(D|θ) = θ^{a_H}(1-θ)^{a_T}$<br>
<br>
$P(θ)=???$<br>

잠깐만요 $P(θ)=1/2$로 사용하면 안되나요? <br>
아쉽지만 그렇게 쉽게 사용할 수는 없습니다. 위의 Likelihood처럼 어떤 분포에 의존하여 표현해야만 하기 때문입니다.<br>
여기서는 동전 던지기이므로 Beta 분포를 사용하겠습니다.<br>
따라서,<br>

 $P(θ) = θ^{α-1}(1-θ) ^{β-1}/B(α,β)$<br>
$B(α,β) = \Gamma(α)\Gamma(β)/\Gamma(α+β)$<br>
$\Gamma(n)=(n-1)!$<br>

위처럼 감마함수와 베타분포를 이용하여 표현할 수 있습니다.<br>
위의 비례식에 우리의 베타분포를 대입해보면<br>
<br>
$P(θ|D)$
<br>
$\propto P(D|θ)P(θ)$<br>

$\proptoθ^{aH}(1-θ)^{aT}θ^{α-1}(1-θ)^{β-1}$<br>

$= θ^{a_H+α-1}(1-θ)^{a_T+β-1}$<br>

어라 뭔가 MLE와 비슷하지않나요?<br>
MLE에서 했던 것 처럼 미분을 이용해서 정리해보면<br>

$\hat{θ}= a_H+α-1/a_H+a_T+α+β-2$<br>위의 식처럼 나오는 것을 알 수 있습니다.<br>

정리해보면<br>
MLE에서는  we found $θ$ from $\hat{θ} = argmax_θP(D|θ)$ 이고,<br>
$\hat{θ} = a_H/a_H+a_T$ 이 됩니다.<br>

MAP에서는 we find $θ$ from $\hat{θ} = argmax_θP(θ|D)$ 이고,<br>
$\hat{θ} = a_H+α-1/a_H+a_T+α+β-2$ 이 되는것을 볼 수 있습니다.<br>

즉 MAP에서는 Prior Knowledge를 이용하여 $α,β$를 바꿔가며 파라미터를 바꿀 수 있습니다.<br>

사실 시행 횟수가 많이 늘어나면 위의 파라미터$α,β$ 에대한 사전 정보의 값이 점점 fade away하여서  ${a_H,a_T}$의 값이 Dominant해집니다.(당연한 얘기죠..?) 
즉 MLE = MAP가 됩니다.<br>

