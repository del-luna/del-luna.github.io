---
layout: post
title: ISLR chapter.3
author: Jaeheon Kwon
categories: Ai
tags: [islr]
---



## 3.1 Simple Linear Regression

단순 선형 회귀는 이름에 걸맞게 단일 예측 변수 $X$를 통해 정량적 반응 $Y$를 예측하는 매우 간단한 접근 방법입니다. $X$와 $Y$사이에는 선형적 관계가 있다고 가정합니다.

수학적으로 선형적 관계는 다음과 같이 나타낼 수 있습니다.

$$Y\approx \beta_0+\beta_1X\tag{3.1}$$



위 식에서 $\beta_0, \beta_1$은 선형 모델의 절편 및 기울기를 나타내는 두 개의 알 수 없는 상수입니다.(계수 혹은 파라미터 라고도 불림)

훈련 데이터를 사용하여 모델 계수에 대한 추정치 $\beta_0,\beta_1$을 생성하면 다음 식을 계산하여 TV특정 값을 계산 기반으로 미래 매출을 예측할 수 있습니다.

$$\hat y = \hat\beta_0+\hat\beta_1x\tag{3.2}$$

$\hat y$는 $X=x$를 기반으로한 $Y$에 대한 예측값입니다. ^ 기호는 알 수 없는 파라미터 혹은 계수에 대한 추정 값이거나, 반응 변수에 대한 예측 값을 나타내는 기호입니다.



### 3.1.1 Estimating the Coefficients

베타 값들은 알 수 없으니 식 3.1을 사용하여 예측하기 전에 데이터를 사용해서 계수를 추정해야 합니다.

$$(x_1,y_1),(x_2,y_2),...,(x_n,y_n)$$

$X,Y$에 대한 관측치로 구성된 $n$개의 페어가 있습니다.

광고의 예시에서 이 데이터 셋은 티비 광고 비용과 $n=200$개의 서로다른 시장에서의 제품 판매로 구성됩니다.

우리의 목표는 선형 모델 식 3.1이 데이터에 잘 맞도록  계수에 대한 추정 $\beta_0,\beta_1$을 얻는 것입니다. 즉, $i=1,...,n$에 대하여 $y_i \approx \hat\beta_0+\hat\beta_1x_i$가 됩니다.

다시말해서 200개의 데이터 포인트에 최대한 가까운 직선을 만들기 위해 편향과 기울기를 찾고 싶습니다.

거리를 측정하는 많은 방법이 존재하지만, 이 장에서는 최소 제곱을 기준으로 사용합니다.



$e_i=y_i-\hat y_i$는 $i$번째 관측된 반응과 반응 값에 대한 우리 선형 모델의 예측의 잔차입니다.

우리는 잔차 제곱 합 RSS를 다음과 같이 정의합니다.

$$RSS = e_1^2+e_2^2+\cdot\cdot\cdot+e_n^2$$

결국 최소 제곱법은 $RSS$를 최소화 하기 위해 $\beta_0,\beta_1$을 선택합니다.

$$\hat\beta_1 = \frac{\sum\limits_{i=1}^n(x_i-\bar x)(y_i-\bar y)}{\sum\limits_{i=1}^n(x_i-\bar x)^2} \\\hat\beta_0=\bar y-\hat\beta_1\bar x \tag{3.4}$$

$\bar x, \bar y$는 각각 샘플의 평균입니다.

다시말해서, 위 식은 선형 회귀를 통한 최소 제곱 계수 추정을 정의합니다.



<img src = "https://py-tonic.github.io/images/islr/3.1.png">



위 그림은  $\hat\beta_0=7.03, \hat\beta_1=0.0475$일 때 광고 데이터에 대한 단순 선형 회귀를 적용한 것을 보여줍니다.

위 근사치를 통해 TV광고에 1000달러를 추가했을 때 약 47.5개의 제품을 추가로 판매할 수 있다는 것을 알 수 있습니다.



<img src = "https://py-tonic.github.io/images/islr/3.2.png">

위 그림은 여러 가지 $\beta_0,\beta_1$에 대해 RSS값을 계산한 것을 보여줍니다.

빨간 점이 $RSS$를 최소화 하는 최적의 페어입니다.



### 3.1.2 Assesing the Accuracy of the Coefficient Estimates

우리는 2장에서 $X,Y$의 관계를 알 수 없는 함수 $f$와 평균이 0 인 $\epsilon$으로 정의했습니다.

$f$는 근사시킨 선형 함수이고, 다음과 같은 관계를 나타낼 수 있습니다.

$$Y = \beta_0+\beta_1X+\epsilon \tag{3.5}$$

에러 텀은 이 단순한 모델에서 놓친 모든 것들을 포괄합니다. 실제 관계는 선형이 아닐 수 있습니다. $Y$를 설명하기 위한 추가적인 변수가 필요할 수도 있고, 측정에서 오류가 있었을 수 있습니다.

 우리는 일반적으로 에러 텀을 $X$와 독립이라고 가정합니다.

위 식에 의해 주어진 모델은 $X,Y$사이의 실제 관계에 대한 모집단 회귀 선을 정의합니다.

최소 제곱 회귀 계수 추정은 최소 제곱 선을 특징화 합니다.



<img src = "https://py-tonic.github.io/images/islr/3.3.png">

위 왼쪽 그림(Fig 3.3)은 단순한 시뮬레이팅 예시에 대한 두 직선입니다.

100개의 $X$ 와 $Y$를 모델로 부터 생성했습니다.

$$Y = 2+3X+\epsilon\tag{3.6}$$

왼쪽 그림의 붉은 직선은 $f(X)=2+3X$인 진짜 관계식을 나타내는 반면 파란 직선은 관측된 데이터를 바탕으로 최소 제곱 추정을 통한 직선을 나타낸 것입니다.

실제 데이터의 진짜 관계식은 일반적으로 알려져있지 않지만, 최소 제곱 직선은 항상 식 3.4를 통해 계수 추정을 계산할 수 있습니다.

오른쪽 그림은 식 3.6을 통해 생성한 다른 데이터셋에 대한 최소 제곱 직선을 그린 것입니다.

동일한 실제 모델에서 생성된 다른 데이터 셋은 약간 다른 최소 제곱 직선을 생성하지만 관찰되지 않은 모집단 회귀선은 변경되지 않습니다.

동일한 데이터셋에서 나온 서로 다른 두개의 직선이 입력변수와 반응 변수의 관계를 설명한다는 것을 무엇을 의미할까요?

이 개념은 표본의 정보를 사용하여 대규모 모집단의 특성을 추정하는 표준 통계 접근 방식의 자연스러운 확장입니다.

예를 들어, 임의 변수 $Y$의 모집단 평균 $\mu$를 알고 싶다고 가정합시다.

불행하게도 $\mu$는 알수 없습니다. 하지만 우리는 $Y$로 부터 $n$개의 관측치 $y_1,y_2,...,y_n$에 접근할 수 있고 이를 통해서 $\mu$를 추정할 수 있습니다.

합리적인 추정은 $\hat y = \bar y$이며, 여기서 $\frac1n\sum\limits_{i=1}^ny_i$는 표본 평균입니다.

물론 표본평균과 모집단 평균은 다르지만 일반적으로 표본 평균은 모집단 평균의 좋은 추정치를 제공합니다.

같은 방식으로, 선형 회귀에서 알려지지 않은 계수 $\beta_0,\beta_1$은 모집단 회귀선을 정의합니다. 우리는 식 3.4를 통해 $\hat\beta_0 , \hat\beta_1$을 사용하여 이러한 알려지지 않은 계수를 추정합니다. 이러한 계수 추정 값은 최소 제곱 선을 정의합니다.

선형 회귀와 확률 변수의 추정 사이의 유추는 편향의 개념을 기반으로합니다.

$\mu$를 추정하기 위해 표본 평균 $\hat \mu$를 사용하는 경우, 이 추정치는 평균적으로 $\mu$가 $\hat \mu$와 같을 것으로 기대한다는 의미에서 편향되지는 않습니다(unbiased).

이 뜻은, 특정 관측 세트$y_1,...,y_n,\hat \mu$가 $\mu$를 과대 추정할 수 있고, 다른 관측치를 기반으로 $\hat \mu$가 $\mu$를 과소 추정 할 수 있음을 의미합니다.

그러나 많은 수의 관측치에서 $\mu$의 추정치에 대한 평균을 구할 수 있다면 그 평균은 정확히 $\mu$와 같습니다.

따라서, 편향되지 않은 추정치는 실제 모수를 체계적으로 과대 추정, 과소 추정 하지 않습니다. unbiasedness에 대한 속성은 식 3.4에 주어진 최소 제곱 계수 추정에도 적용됩니다.

특정 데이터를 기반으로 $\beta_0,\beta_1$을 추정하면 추정 값이 $\beta_0,\beta_1$과 정확히 일치하지 않습니다. 하지만 많은 수의 데이터 셋에서 얻은 추정치를 평균화 할 수 있다면 이러한 추정치의 평균은 일치합니다.

실제로 Fig 3.3의 오른쪽 패널에서 각각 별도의 데이터 셋에서 추정된 많은 최소 제곱 선이 실제 모집단 회귀 선에 매우 가깝다는 것을 알 수 있습니다.

이제 자연스러운 질문이 발생할 수 있습니다. $\mu$의 추정치로서 표본 평균 $\hat \mu$는 얼마나 정확합니까? 우리는 많은 데이터 셋에 대한 추정치의 평균은 모집단의 평균과 매우 근사하지만 단일 추정치는 모집단의 평균을 과대 추정하거나 과소 추정 할 수 있음을 확인했습니다.

표본 평균의 단일 추정치는 얼마나 멀리 떨어져 있을까요? 일반적으로 우리는 $SE(\hat \mu)$로 표기된 $\hat \mu$의 standard error를 계산하여 이 질문에 대답할 수 있습니다.

$$Var(\hat\mu) = SE(\hat\mu)^2 = \frac{\sigma^2}{n}\tag{3.7}$$

러프하게 말하자면 표준 오차는 이 추정치가 모집단의 평균과 얼마나 다른지를 알려줍니다.

또한 위 식은 관측값이 많을 수록 표준 오차가 작아지는 이유를 말해줍니다.

비슷한 맥락으로 $\hat \beta_0,\hat\beta_1$이 $\beta_0,\beta_1$과 얼마나 가까운지를 다음 식을 통해 알 수 있습니다.

$$SE(\hat\beta_0)^2 = \sigma^2[\frac1n+\frac{\bar x^2}{\sum_{i=1}^n(x_i-\bar x)^2}],\quad SE(\hat\beta_1)^2=\frac{\sigma^2}{\sum_{i=1}^n(x_i-\bar x)^2}\tag{3.8}$$

여기서 $\sigma^2=Var(\epsilon)$입니다.

이 공식이 엄밀하게 유효하려면, 각 관측치의 오차 $\epsilon_i$가 공통 분산 $\sigma^2$와 상관이 없다고 가정해야 합니다.

이 것은 Fig 3.1에서 명백한 사실은 아니지만, 공식은 여전히 좋은 근사치로 밝혀졌습니다.

공식에 따르면 $SE(\hat\beta_1)$은 $x_i$가 더 넓어지면 작아집니다. 직관적으로 이 경우 기울기를 추정하는데 더 많은 레버리지가 존재하게됩니다.

또한 $\bar x$가 0이면 $SE(\hat \beta_0)$는 $SE(\hat \mu)$와 동일합니다.(이 경우 $\hat \beta_0$=$\bar y$)

일반적으로 $\sigma^2$은 알 수 없지만 데이터에서 추정할 수 있습니다. $\sigma$의 추정치는 residual standard error로 알려져 있으며 식으로는 다음과 같습니다. $RSE = \sqrt{RSS/(n-2)}$

표준 오차를 사용하여 신뢰 구간을 계산할 수 있습니다.

95%의 신뢰 구간은 95% 확률로 매개변수의 실제 알 수 없는 값이 포함되는 값의 범위로 정의됩니다.

범위는 데이터 샘플에서 계산된 하한 및 상한으로 정의됩니다.

선형 회귀에서 $\beta_1$에 대한 95%신뢰 구간은 대략 다음과 같은 형식을 취합니다.

$$\hat\beta_1\pm 2\cdot SE(\hat \beta_1) \tag{3.9}$$

95%확률에 대한 구간의 범위는 다음과 같습니다.

$$[\hat\beta_1 - 2\cdot SE(\hat\beta_1), \hat\beta_1 + 2\cdot SE(\hat\beta_1)]  \tag{3.10}$$

유사하게 $\beta_0$에 대한 신뢰 구간은 다음과 같은 형식을 취합니다.

$$\hat\beta_0\pm 2\cdot SE(\hat \beta_0) \tag{3.11}$$



광고 데이터의 예시로 돌아가봅시다. $\beta_0$에 대한 95%신뢰구간은 [6.130, 7.935]이며, $\beta_1$에 대한 신뢰 구간은 [0.042, 0.053]입니다. 그러므로 우리는 광고가 없을 때 판매량이 평균적으로 6,130 ~ 7940단위 사이에 떨어질 것이라는 결론을 내릴 수 있습니다. 또한 TV광고가 $1000 증가할 때마다 평균 판매량은 42~53대 증가할 것 입니다.



표준 오차를 사용하여 계수에 대한 가설 검정을 수행할 수 있습니다. 가장 일반적인 가설 검정은 다음의 귀무 가설을 검정하는 것입니다.

- $H_0$: There is no relationship between X and Y
- $H_1$: There is some relationship between X and Y



수학적으로는 다음과 같습니다.

- $H_0:\beta_1 = 0$
- $H_1:\beta_1\neq0$

$\beta_1 = 0$이면 모델 3.5가 $Y=\beta_0+\epsilon$으로 바뀌고 $X$는 $Y$와 관련이 없습니다.

귀무 가설을 테스트 하려면 $\beta_1$에 대한 추정치인 $\beta_1$이 0에서 충분히 멀리 떨어져 $\beta_1$이 0이 아니라고 확신할 수 있는지 여부를 확인해야 합니다.

얼마나 멀어야 할까요? 물론 이것은 $\hat\beta_1$의 정확도에 따라 달라집니다. 즉, $SE(\hat\beta_1)$에 따라 달라집니다. 만약 $SE(\hat\beta_1)$이 작다면, 상대적으로 작은 값의 $\beta_1$이라도 $\beta_1 \neq0$이라는 강력한 증거가 될 수 있고, 이 것은 $X,Y$에 관계가 있음을 보여줍니다.

반대로 $SE(\hat\beta_1)$가 크면 귀무 가설을 기각하기 위해서는 $\hat\beta_1$의 절대 값이 커야 합니다. 실제로 우리는 다음과 같은 t-statistic을 계산합니다.

$$t = \frac{\hat\beta_1 -0}{SE(\hat\beta_1)} \tag{3.14}$$

> 검정 통계량 t는
>
> $\hat\beta_1$ 데이터로 부터 추정한 기울기 - (귀무가설에서 설정한 기울기 즉, 0)을, standard error로 스케일링함. t가 크면 데이터랑 내가 주장하는 바가 다르므로 귀무가설을 기각.



$\beta_1$이 0에서 벗어난 표준 편차의 수를 측정합니다. $X,Y$사이에 관계가 없다면 $n-2$ 의 자유도를 갖는 t-분포를 가질 것으로 예상합니다. (t-분포는 종 모양이고 n>30인 경우에 대해 정규 분포와 매우 유사합니다.)

결과적으로 $\vert t\vert$와 같은 숫자를 관찰할 확류을 계산하는 것은 간단한 문제입니다. $\beta_1=0$이라고 가정하면 절 대 값이 더 큽니다. 우리는 이 확률을 $p-value$라고 부릅니다. (tip. 각 $\beta$에 대한 $p-value$값이 큰게 중요한 변수)

$p-value$를 다음과 같이 해석합니다. 작은 $p-value$는 예측 변수와 반응 사이에 실질적인 연관성이 있다는 것을 의미합니다.(즉, 우리의 기울기가 귀무가설에서 세운 0과 차이가 크다면 t값은 커지게 되고 이는 p-value가 매우 작아지는 것을 의미합니다. P(Y>t) 인데 여기서 t가 크다는 얘기이니까요.. 이렇게 되면 실제로 일어날 확률이 매우 작다는 뜻이고 이는 귀무 가설이 잘못되어서 기각한다는 결론으로 이어집니다.)

일반적으로 귀무가설을 기각하기위한 $p-value$의 값은 n=30일 때 1% 혹은 5%입니다.



<img src = "https://py-tonic.github.io/images/islr/t3.1.png">

위 표는 회귀 분석에 대한 최소 제곱 모델의 세부 정보를 제공합니다. 광고 데이터의 TV광고 예산으로 판매된 단위 수입니다.

계수 $\beta_0,\beta_1$은 표준 오차와 매우 큰 상관관계가 있고, 검정 통계량 또한 큽니다. $H_0$가 정답일 확률은 매우 낮습니다. 그러므로 $\beta_0, \beta_1 \neq 0$이라고 결론 지을 수 있습니다.



### 3.1.3 Assessing the Accuracy of the Model

귀무 가설을 기각하고 대립 가설을 채택하면 모델이 데이터에 얼마나 적합한지 정량화 하는 것 또한 당연합니다.

선형 회귀의 적합도를 일반적으로 RSE(residual standard error)와 $R^2$통계량을 이용하여 평가합니다.

<img src = "https://py-tonic.github.io/images/islr/t3.2.png">

위 표(table 3.2)는 TV광고 예산으로 판매된 선형 회귀에 대한 RSE, $R^2$통계 및 F-통계를 표시하고 있습니다.



**Residual Standard Error**

각 관측치와 관련이 있는 모델 3.5에서 에러 텀을 떠올려 봅시다.

이러한 에러 텀으로 인해 진짜 회귀선 ($\beta_0,\beta_1$을 알아도) 우리는 $X$를 통해 $Y$를 완벽하게 예측할 수 없습니다. RSE는 에러 텀에 대한 표준 편차를 추정하는 것입니다.

러프하게 말하면, 반응 변수와 진짜 회귀 직선 사이 편차의 평균입니다. 식으로는 다음과 같이 나타냅니다.

$$RSE = \sqrt{\frac{1}{n-2}RSS} = \sqrt{\frac{1}{n-2}\sum\limits_{i=1}^n(y_i-\hat y_i)^2}\tag{3.15}$$

$RSS$는 섹션 3.1.1에서 정의합니다. 식은 다음과 같습니다. (여기서 자유도로 나누면 MSE)

$$RSS = \sum\limits_{i=1}^n(y_i-\hat y_i)^2\tag{3.16}$$

광고 데이터의 케이스에서 우리는 RSE가 3.26인 것을 테이블 3.2를 통해 볼 수 있습니다.

즉, 각 시장은 평균적으로 약 3,260단위 만큼 실제 회귀선에서 벗어납니다. 이 말은 실제 모델을 알아도 TV광고를 기반으로한 판매 예측은 여전히 평균 약 3,260단위 만큼 떨어진다는 의미입니다. 위 단위가 허용 가능한 예측 오류인지에 대한 여부는 문제에 따라 다릅니다.

광고 데이터 셋에서 판매에 대한 평균 값은 대략 14,000 유닛 입니다. 에러의 비율은 3,260/14,000 = 23% 정도 입니다.

만약 모델을 사용하여 실제 결과 값과 매우 가까운 예측 값을 얻는다면, RSE가 매우 작은 것이고 모델이 데이터에 잘 맞는다고 할 수 있습니다.

즉, $\hat y_i$가 $y_i$와 굉장히 멀다면 RSE는 매우 클 것이고, 모델이 데이터에 적합하지 않다는 것을 의미합니다.



**$R^2$ Statistic**

RSE는 데이터와 모델의 차이에 대한 메저를 제공했습니다. 그러나 $Y$의 단위로 측정되기 때문에 무엇이 좋은 RSE를 구성하는지  항상 명확하지는 않습니다. $R^2$ 통계량은 다른 적합도를 제공합니다. 이 메저는 비율을 사용하여 분산을 설명합니다. (비율이기 때문에 항상 0~1사이 값을 가집니다.) 또한 Y의 스케일에 독립적입니다.

$R^2$은 다음과 같이 계산합니다.

$$R^2 = \frac{TSS-RSS}{TSS} = 1-\frac{RSS}{TSS} \tag{3.17}$$

$TSS= \sum(y_i-\bar y)^2$ 입니다. TSS는 반응 변수 $Y$에 대한 변동성을 나타냅니다.(여기서 자유도로 나누면 분산이지만 뭐...편의상 분산이라고 언급해도 되지 않을까요...?)

위에서 다룬 $RSS$는 회귀 분석 수행 후 설명되지 않은 상태로 남아 있는 변동성(에러 텀)의 양을 측정합니다.

> 설명이 좀 ... 애매하긴한데 저는 이렇게 이해했습니다.
>
> TSS가 러프하게 따지면 전체 Y에 대한 분산이면
>
> RSS는 회귀분석 수행 후 남은 에러 텀에 대한 분산을 나타내겠죠?(입력 변수로 Y에대한 관계를 모두 설명 한경우 에러텀에 대한 분산만 남겠죠?)
>
> 그래서 TSS-RSS는 이 모델이 에러텀을 제외하고 얼마나 많은 분산을 표현(혹은 설명) 했는지 라고 해석했습니다.

$R^2$가 1에 가까울수록 회귀 모델이 반응 변수에대한 설명을 잘 하고 있는 것으로 볼 수 있습니다.

반대로 0에 가까울수록 모델이 너무 좋지 않거나 에러 텀의 분산이 매우 큰 것으로 볼 수 있습니다.

$R^2$는 RSE와 달리 0~1사이 값으로 표현되기 때문에 해석하는데 이점이 존재합니다.



선형 모델은 생물학, 심리학, 마케팅 및 기타 영역에서 복잡한 데이터에 대한 근사치정도로 밖에 쓰이지 못하고, 측정되지 않은 요인으로 인한 잔류 오차는 매우 큽니다.

이러한 환경에서 예측 변수에 의해 설명되는 반응의 분산은 극히 작은 비율만 예상되며 0.1보다 낮은 R2 값이 더 현실적입니다.

$R^2$통계량은 $X$와 $Y$의 선형 관계에 대한 척도임을 기억합시다. 상관 관계는 다음과 같이 정의됩니다.

$$Cor(X,Y) = \frac{\sum_{i=1}^n(x_i-\bar x)(y_i-\bar y)}{\sqrt{\sum_{i=1}^n(x_i-\bar x)^2}\sqrt{\sum_{i=1}^n(y_i-\bar y)^2}}\tag{3.18}$$

위 식 또한 $X, Y$에 대한 선형 관계를 측정할 수 있습니다.

이는 선형 모델의 적합성을 평가하기 위해 $R^2$대신 $r=Cor(X,Y)$를 사용할 수 있음을 보여줍니다.

실제로 단순 선형 회귀 설정에서 $R^2=r^2$입니다.

하지만 다음 섹션에서 다룰 다중 선형 회귀는 여러 예측 변수를 사용하여 반응 변수를 예측합니다.

상관 관계는 단일 변수 쌍 간의 관계를 정량화하므로, 예측 변수와 반응 변수 사이의 상관 관계 개념은 자동으로 다중 회귀에서 확장되지는 않습니다. $R^2$가 이 역할을 할 수 있습니다.



## 3.2 Multiple Linear Regression