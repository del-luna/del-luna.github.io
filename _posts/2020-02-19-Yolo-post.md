---
layout: post
title: Yolo
author: Jaeheon Kwon
categories: Paper
tags: [detection]
---

#  You Only Look Once: Unified, Real-Time Object Detection 

## Abstract

기존의 방식인 Classifiers를 재활용 하는 방식에 덧 붙여 공간적으로 분리된 b-box와 관련 클래스의 확률에 대한 **regression**문제로 접근합니다.<br>

Yolo는 **단일 신경망**으로 구성되고 한번의 평가만 이루어 집니다.(can optimize end-to-end & fast)<br>

다른 SOTA 모델들에 비해 Localization error는 많지만 빠르고, Background에서 false predict가 잘 나오지 않습니다.<br>

또한 이전의 대표 모델인 DPM과 R-CNN에 비해 일반화 성능이 좋습니다.<br>

## Introduction

기술의 발전으로 Detection을 통해 다양한 것들을 할 수 있습니다.(ex: 자율주행)<br>

현재의 Detection system은 단순히 classifier를 목적에 맞게 재구성 하는게 대부분입니다.<br>

Sliding window 방식의 DPM, Region Proposals 방식의 R-CNN등 복잡한 Pipeline은 개별 구성 요소를 독립적으로 교육해야 하므로 속도가 느리고 최적화하기 어렵습니다.<br>

Yolo는 image detection → b-box coordinate → class probabilities에 이르기 까지 Detection 모델을 single regression 문제로 재구성 했습니다.<br>

<img src = "https://py-tonic.github.io/images/yolo/1.PNG">

**Yolo의 이점**

1. 빠르다(detection을 regression으로 구성, complex pipeline 필요없음) real-time streaming video가능 & 현재 나온 다른 real-time 모델들에 비해 두 배 이상 정확함
2. 전체 이미지를 input으로 사용하여 이미지에 대한 globally한 추론이 가능하다.(Sliding, region proposals와 달리 전체 이미지를 확인하므로 정보를 implicitly encode) 아마도 전체 이미지를 사용해서 위에서 말한 background error도 적지 않을까 싶음.<br>
3. 일반화가 잘 된다.

## Unified Detection

Object Detection의 개별 구성 요소를 Single Network로 통합.<br>

전체 이미지 기능을 사용하여 각 b-box를 예측합니다.<br>

이를 통해 Yolo는 높은 정확도를 유지하면서 end-to-end 학습과 real-time 처리가 가능합니다.<br>

**학습 과정**

<img src = "https://py-tonic.github.io/images/yolo/2.PNG">

input image를 SxS grid로 나누고 객체의 중심이 cell에 들어가면 해당 cell로 객체를 감지합니다.<br>

each grid cell은 B개의 b-box와 해당 box에 대한 Confidence Score를 예측합니다.<br>

Confidence Score는 두 가지를 반영하는데,<br>

1. the model is that the box contains an object(box에 객체가 있는지)
2. how accurate it thinks the box is that it predicts(box가 예측한 정확도가 얼마나 정확한지)

b-box는 x,y,w,h,confidence의 5가지 predict로 구성됩니다.<br>

여기서 confidence score 는 GT와 Predict 사이의 IOU입니다.(IOU(Intersection of Union): predict box와 truth box가 얼마나 겹치는지)<br>

그러므로 Confidence = Pr(object) * $IOU^{truth}_{pred}$ 입니다.(해당 cell에 객체가 없다면 Confidence Score = 0)<br>

 each grid cell은 C개의 조건부 클래스 확률을 예측합니다.<br>

$Pr(Class_i｜Object)$<br>

상자수 B와 관계 없이 grid cell당 하나의 Class 확률만 예측합니다.<br>

Test time에선 조건부 클래스 확률과 개별 상자 신뢰도 예측을 곱합니다.<br>

$Pr(Class_i｜Object)*Pr(Object)*IOU^{truth}_{pred} = Pr(class_i)*IOU^{truth}_{pred}$ <br>

<br>

각 상자마다 클래스 별 Confidence Score를 제공합니다.<br>

이 점수는 해당 클래스가 상자에 나타날 확률과 예측된 상자가 객체에 얼마나 잘 맞는지  둘 다를 인코딩 합니다.<br>

## Network Design

CNN으로 구현했고, PASCAL dataset으로 평가했습니다.<br>

초기 Conv layer는 image에서 feature를 추출, fcn은 output확률과 좌표를 예측합니다.<br>

GoogLeNet에서 영감을 받았습니다.(24Conv, 2fcn), Inception대신에 1x1 reduction layer와 3x3 conv layer를 사용합니다.<br>

<img src = "https://py-tonic.github.io/images/yolo/3.PNG">

논문에서 조건을 S=7, B=2, C=20으로 걸어둬서,<br>

Final output = S x S x (B x 5 + C) = 7 x 7 x 30 tensor 입니다.<br>

## Training

ImageNet으로 pre-train한 모델 사용, pre-train에는 20개의 Conv layer, average-pool, fcn을 사용했습니다.<br>

Darknet framework를 사용해서 구현했고,<br>

Object Detection networks on convolutional featuremap 이라는 논문에서 pre-train model에 conv 및 fcn을 추가하면 성능이 향상되는 걸 따라서 무작위 초기화된 4개의 conv layer와 2개의 fcn을 추가하였습니다.<br>

input 해상도를 224x224 → 448x448로 증가시켰습니다.<br>

마지막 레이어는 클래스 확률과 b-box 좌표를 모두 예측합니다.<br>

b-box w,h를 image w,h로 정규화 합니다.(0~1)<br>

Leaky Relu를 사용합니다.<br>

<img src = "https://py-tonic.github.io/images/yolo/4.PNG">

SSE(Sum-Square-Error)로 Optimize합니다.<br>

최적화 하기 쉽고 빠르지만 정확도는 조금 떨어집니다.<br>

Localization error & Classification error의 가중치를 동일하게 적용합니다.<br>

모든 이미지에서 대부분의 grid cell에는 객체가 포함되어 있지 않습니다.(대부분이 배경)<br>

해당 cell의 신뢰도 점수가 '0'으로 채워지면 객체를 포함하는 cell의 gradient를 압도하여 모델의 불안정과 학습 조기종료의 원인이 될 수 있습니다.<br>

이를 해결하기 위해 b-box로 예측으로 인한 손실을 늘리고, 객체가 포함되지 않은 상자에 대한 신뢰도 예측으로 인한 손실을 줄입니다.<br>

 $ λ_{coord}, λ_{noobj}$ 두가지 파라미터를 사용하고<br>

$ λ_{coord} = 5, λ_{noobj} = 0.5$로 설정합니다.<br>

SSE는 큰 상자와 작은 상자 error에 동일하게 가중치를 적용합니다.<br>

오차 측정법은 큰 상자에서 small deviation이 작은 상자에서 보다 중요한 것을 반영해야 합니다.<br>

이를 해결하기 위해 직접적인 w,h대신 box의 w,h의 제곱근을 예측합니다.(보정이라고 보면 됨)<br>

Yolo는 grid cell당 여러 b-box를 예측합니다.<br>

training time에 우리는 오직 하나의 b-box predictor가 각 Object를 책임지길 원합니다.<br>

어떤 predictor가 GT와 현재 IOU가 가장 높은지를 기반으로 객체를 예측하기 위해 하나의 predictor를 'responsible'로 지정합니다.(이는 b-box predictor의 specialization으로 이어집니다)<br>

각 predictor는 특정 크기, 종횡비, 객체 클래스를 더 잘 예측하여 전반적인 recall을 향상시킵니다.<br>

<img src = "https://py-tonic.github.io/images/yolo/5.PNG">

출처 : [TAEU]( https://taeu.github.io/paper/deeplearning-paper-yolo1-02/ )

식을 보면 객체 탐지를 하지 못한경우에 대한 손실을 생각하지 않는 것을 볼 수 있습니다.<br>

위에서 부터 각 식에대한 설명을 해보면,<br>

1. 객체가 존재하는 grid cell i의 j번 째 b-box의 (x,y)에 대한 loss
2. 객체가 존재하는 grid cell i의 j번 째 b-box의 (w,h)에 대한 loss
3. 객체가 존재하는 grid cell i의 j번째 b-box의 Confidence Score에 대한 loss
4. 객체가 존재하지 않는 grid cell i의 j번째 b-box의 Confidence Score에 대한 loss
5. 객체가 존재하는 grid cell i의 조건부 클래스 확률에 대한 loss

로 볼 수 있습니다.<br>

(2)번을 보면 우리가 위에서 말했듯 큰 박스의 small deviation에 대한 보정으로 볼 수 있겠죠?<br>

또한 1, 2, 4번식에도 위에서 말했듯 람다 파라미터를 이용하여 b-box의 size, location에 대한 loss를 늘려주고, 객체가 탐지되지 않은 경우의 Confidence score에 대한 loss를 낮춰주는 모습입니다.<br>

왜 4번에 파라미터가 추가되어서 loss를 낮춰줘야 할까요?<br>

바로 Yolo가 background error에 강한 부분이라고 생각하는데, 앞서 말했듯 대부분의 이미지에는 객체가 포함되지 않은 부분이 많고 저런 과정을 거치지 않을경우 배경을 객체로 인식하게 되어 학습하는 일이 발생한다고 합니다.<br>

## Limitation of Yolo

박스 두 개로 예측하지만 하나의 클래스만 예측하기 때문에 b-box 예측에 강력한 '**공간적 제약**'을 부과합니다.<br>

여기서 **공간적 제약**이란 모델이 예측할 수 있는 주변 수를 제한하는 것으로 예를 들어 조류 무리등의 그룹을 예측하기 어렵다는 것입니다.<br>

모델이 데이터로 부터 박스 예측을 학습하기 때문에 새롭거나 특이한 종횡비로 구성된 객체를 일반화 하는데 어려움을 겪습니다.<br>

Loss function이 학습하는 동안 큰 박스와 작은 박스의 error를 동일하게 처리합니다.<br>

작은 상자의 small error는 IOU에 훨씬 큰 영향을 미칩니다.<br>

주요 error의 원인은 잘못된 'Localization'입니다.<br>

## Experiments

<img src = "https://py-tonic.github.io/images/yolo/6.PNG">

<img src = "https://py-tonic.github.io/images/yolo/7.PNG">

<img src = "https://py-tonic.github.io/images/yolo/8.PNG">

<br>

## Reference

- [Paper]( https://arxiv.org/pdf/1506.02640.pdf )
- [어쩐지 오늘은]( https://zzsza.github.io/data/2018/05/02/YOLO-You-only-look-once-review/ )
- [TAEU]( https://taeu.github.io/paper/deeplearning-paper-yolo1-02/ )
